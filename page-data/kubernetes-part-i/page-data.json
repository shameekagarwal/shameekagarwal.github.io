{"componentChunkName":"component---src-templates-blog-js","path":"/kubernetes-part-i/","result":{"data":{"markdownRemark":{"fields":{"slug":"/kubernetes-part-i/"},"id":"2c7b9d71-182c-506f-b74f-75a5715793f5","html":"<h1 id=\"about\" style=\"position:relative;\"><a href=\"#about\" aria-label=\"about permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>About</h1>\n<ul>\n<li>kubernetes is the most widely used container scheduler</li>\n<li>modern infrastructure is created using immutable images, and an upgrade is performed by replacing the older images with newer ones using rolling updates</li>\n<li>we specify how many resources to run and kubernetes maintains that number</li>\n<li>it ensures that the resources run within the specified memory and cpu constraints</li>\n<li>kubernetes is cloud-agnostic and can also be run on-prem</li>\n<li>it has features like service discovery, load balancing, secret and configuration management, etc</li>\n</ul>\n<h1 id=\"minikube-and-kubectl\" style=\"position:relative;\"><a href=\"#minikube-and-kubectl\" aria-label=\"minikube and kubectl permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Minikube and Kubectl</h1>\n<ul>\n<li>minikube allows us set up a single node cluster on our local workstation</li>\n<li>minikube is useful for development purpose</li>\n<li>kubectl is the kubernetes command line tool which allows to manage a kubernetes cluster</li>\n<li>configuring autocomplete for kubectl (restart terminal after running the command) -\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-bash line-numbers\"><code class=\"language-bash\"><span class=\"token builtin class-name\">echo</span> <span class=\"token string\">'source &lt;(kubectl completion bash)'</span> <span class=\"token operator\">>></span> ~/.bashrc</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span></span></pre></div>\n</li>\n<li>minikube can be deployed as a vm or as a container (i am trying as a container for now)</li>\n<li>configuring minikube -\n<ul>\n<li><code class=\"language-text\">minikube config set driver docker</code></li>\n<li><code class=\"language-text\">minikube config set memory 8192</code></li>\n<li><code class=\"language-text\">minikube config set cpu 4</code></li>\n</ul>\n</li>\n<li>view config using <code class=\"language-text\">minikube config view</code> or <code class=\"language-text\">cat ~/.minikube/config/config.json</code></li>\n<li>start minikube -\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-bash line-numbers\"><code class=\"language-bash\">minikibe start\nminikube status</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span></span></pre></div>\n</li>\n<li>pointing docker client installed locally to minikube’s docker daemon -\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-bash line-numbers\"><code class=\"language-bash\"><span class=\"token function\">docker</span> container <span class=\"token function\">ls</span>\nminikube docker-env\n<span class=\"token builtin class-name\">eval</span> <span class=\"token variable\"><span class=\"token variable\">$(</span>minikube <span class=\"token parameter variable\">-p</span> minikube docker-env<span class=\"token variable\">)</span></span>\n<span class=\"token function\">docker</span> container <span class=\"token function\">ls</span></code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span></span></pre></div>\n</li>\n<li>to ssh into minikube - <code class=\"language-text\">minikube ssh</code>. now also we can run commands like <code class=\"language-text\">docker container ls</code> etc</li>\n<li>to get all running components, we can use <code class=\"language-text\">kubectl get all --all-namespaces</code></li>\n<li>to shut down minikube, use <code class=\"language-text\">minikube stop</code>. it preserves the state</li>\n<li>to start minikube again, <code class=\"language-text\">minikube start</code></li>\n<li>to delete the cluster, <code class=\"language-text\">minikube delete</code></li>\n<li>can format output e.g. <code class=\"language-text\">kubectl version --output=yaml</code>. output format can be json as well</li>\n<li><code class=\"language-text\">minikube ip</code> to get the ip address of minikube cluster</li>\n<li>an issue on my laptop - minikube cannot pull docker images at times. temporary fix is to pull manually using <code class=\"language-text\">docker image pull</code> after pointing docker client to minikube’s docker daemon</li>\n</ul>\n<h1 id=\"cluster-architecture\" style=\"position:relative;\"><a href=\"#cluster-architecture\" aria-label=\"cluster architecture permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Cluster Architecture</h1>\n<ul>\n<li>the cluster has master nodes and worker nodes. note: there can be multiple masters in the cluster</li>\n<li>the master nodes schedule and monitor the containers assigned to it on the worker nodes</li>\n<li>different methods of viewing information related to the different components e.g. etcd\n<ul>\n<li><code class=\"language-text\">ps aux | grep etcd</code></li>\n<li><code class=\"language-text\">sudo cat /etc/kubernetes/manifests/etcd.yaml</code></li>\n<li><code class=\"language-text\">docker container ls | grep etcd</code></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"etcd\" style=\"position:relative;\"><a href=\"#etcd\" aria-label=\"etcd permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Etcd</h3>\n<ul>\n<li>a distributed key-value store that allows for fast storage and retrieval</li>\n<li>it runs on the port 2379</li>\n<li>etcdctl is the etcd control client which helps communicate with etcd</li>\n<li>it is used for storing and retrieving information about all kubernetes resources</li>\n<li>the etcd clusters can either be present on the master nodes or be entirely decoupled from them</li>\n<li>kubeadm runs etcd as a static pod on the master nodes</li>\n<li>we specify its ip address and port on the api server</li>\n<li>an example of using etcdctl api version 3 -\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-shell line-numbers\"><code class=\"language-shell\">kubectl <span class=\"token builtin class-name\">exec</span> etcd-minikube <span class=\"token parameter variable\">--namespace</span><span class=\"token operator\">=</span>kube-system -- <span class=\"token function\">sh</span> <span class=\"token parameter variable\">-c</span> <span class=\"token punctuation\">\\</span>\n  <span class=\"token string\">\"ETCDCTL_API=3 etcdctl get / \\\n  --prefix --keys-only --limit=100 \\\n  --cacert /var/lib/minikube/certs/etcd/ca.crt \\\n  --cert /var/lib/minikube/certs/etcd/server.crt \\\n  --key /var/lib/minikube/certs/etcd/server.key\"</span></code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\nto get the location of the certs, use <code class=\"language-text\">kubectl get pod etcd-minikube --namespace=kube-system --output=yaml</code></li>\n<li>peer to peer communication in etcd clusters when there are multiple master nodes happens through 2380</li>\n<li>etcd is distributed i.e. we can read from any of the instances, while all writes go to the master in the etcd cluster which syncs the data on the other replicas</li>\n<li>in case of inconsistencies, the quorum determines if the update is valid. it is the minimum number of nodes in the etcd cluster which should have processed the update, which is floor(n / 2) + 1. the value of fault tolerance is total instances - quorum. so, it is recommended to have an odd number of etcd instances / master nodes depending on the configuration, since fault tolerance is the same for n and n - 1 nodes where n is even</li>\n</ul>\n<h3 id=\"api-server\" style=\"position:relative;\"><a href=\"#api-server\" aria-label=\"api server permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Api Server</h3>\n<ul>\n<li>it runs on the master node</li>\n<li>external clients like kubectl communicate changes to the cluster via the api server</li>\n<li>schedulers, controllers, kubelets, etc. monitor the api server for new resources</li>\n<li>they also send updates to the api server which then updates it on the etcd cluster</li>\n<li>so, api server is the only component that directly interacts with the etcd cluster</li>\n<li>the api server on the multiple master nodes can run concurrently i.e. all api servers on all the master nodes can be active at once. however, in case of controller manager and scheduler, to avoid duplication and inconsistencies, they are in the active state on the master node which is elected as the leader while they are in standby mode on the other master nodes</li>\n<li>in case of multiple masters, clients like kubectl interact with a load balancer, where the load balancer routes requests to the multiple api servers</li>\n</ul>\n<h3 id=\"controllers\" style=\"position:relative;\"><a href=\"#controllers\" aria-label=\"controllers permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Controllers</h3>\n<ul>\n<li>also called controller manager</li>\n<li>different kinds of controllers run on the master node</li>\n<li>for instance, the master node expects heartbeats from the worker nodes. the node controller monitors them and if the heartbeats do not reach the master nodes for a certain time period, the pods on it are evicted</li>\n<li>similarly, we have replication controller to maintain the number of pods of the same type</li>\n<li>the controller manager package installs all the different controllers. to view the different controllers, use -\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-shell line-numbers\"><code class=\"language-shell\">kubectl get pod kube-controller-manager-minikube <span class=\"token punctuation\">\\</span>\n  <span class=\"token parameter variable\">--namespace</span><span class=\"token operator\">=</span>kube-system <span class=\"token parameter variable\">--output</span><span class=\"token operator\">=</span>yaml <span class=\"token operator\">|</span> <span class=\"token function\">grep</span> controllers</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span></span></pre></div>\n</li>\n</ul>\n<h3 id=\"scheduler\" style=\"position:relative;\"><a href=\"#scheduler\" aria-label=\"scheduler permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Scheduler</h3>\n<ul>\n<li>runs on the master node</li>\n<li>it assigns pods to a specific node</li>\n<li>it does this based on available resources like cpu and memory and filters out nodes which cannot run the pod</li>\n<li>it then based on a priority function ranks the remaining nodes</li>\n<li>the pod then gets scheduled on one of the remaining nodes</li>\n<li>we might need a custom scheduler in some cases. below, we simulate running multiple schedulers -\n<ul>\n<li>create the static pod for the new scheduler. to do this, copy the file at /etc/kubernetes/manifests/kube-scheduler.yaml in the same directory with a different name</li>\n<li>add a custom name using <code class=\"language-text\">--scheduler-name</code></li>\n<li>change to argument <code class=\"language-text\">--leader-elect=true</code> to the command of both (i see it as false in my minikube)</li>\n<li>add <code class=\"language-text\">--lock-object-name</code> for the new scheduler. my understanding - (<a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/\">kube scheduler args</a>) since multiple schedulers cannot run at a time, a leader is needed. however, right now we are running two different types of schedulers (or simulating running different types of schedulers). so, they have different leaders</li>\n<li>now, we can specify the scheduler to use manually using <code class=\"language-text\">schedulerName</code> in pods</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"kubelet\" style=\"position:relative;\"><a href=\"#kubelet\" aria-label=\"kubelet permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Kubelet</h3>\n<ul>\n<li>it runs on all worker nodes (and optionally on the master node)</li>\n<li>it registers the nodes with the cluster</li>\n<li>picks up the pods from the api server to run on the node and then runs it</li>\n<li>it then sends updates of the status of the pod to the api server</li>\n<li>unlike the rest, kubelet does not run as a static pod, daemon set etc</li>\n<li>it runs via a binary installed on the vms</li>\n<li>so, to view information, use <code class=\"language-text\">ps aux | grep kubelet</code></li>\n<li>this will show the files locations, so, for e.g., use - <code class=\"language-text\">cat /var/lib/kubelet/config.yaml</code></li>\n</ul>\n<h3 id=\"kube-proxy\" style=\"position:relative;\"><a href=\"#kube-proxy\" aria-label=\"kube proxy permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Kube Proxy</h3>\n<ul>\n<li>it runs on all nodes, since it runs as a daemon set</li>\n<li>pods in a node can reach pods on other nodes as well because of this</li>\n<li>the kube proxy assigns an ip to the service</li>\n<li>to view the ip range from which services are assigned ip addresses, we can use <code class=\"language-text\">kubectl get pod kube-apiserver-minikube --namespace=kube-system --output=yaml | grep service-cluster-ip-range</code></li>\n<li>it configures ip tables, which maps the ip address of services to endpoints</li>\n<li>an endpoint = the ip address of the pod + port of the pod. this port can belong to any one of the containers, set by target port field in the service definition</li>\n<li>if we have multiple pods sitting behind a service, an algorithm similar to round robbin is used</li>\n<li>the proxy mode used by it can be userspace, iptables, ipvs. iptables is the most common one. to view the proxy mode configured, i had to view the logs of the kube proxy pod</li>\n</ul>\n<h3 id=\"kube-dns\" style=\"position:relative;\"><a href=\"#kube-dns\" aria-label=\"kube dns permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Kube Dns</h3>\n<ul>\n<li>it maps the service name to the service ip address</li>\n<li>so, it configures the dns server</li>\n<li>kubernetes uses coredns for achieving this functionality</li>\n<li>on running <code class=\"language-text\">kubectl get deployment coredns --namespace=kube-system --output=yaml</code>, we can see that a config map is mounted as a volume on it</li>\n<li>we get the contents of it using <code class=\"language-text\">kubectl get configmap coredns --namespace=kube-system --output=yaml</code>. it shows the plugins being used by coredns</li>\n<li>there is also a service associated with kube dns, which we can get using <code class=\"language-text\">kubectl get service kube-dns --namespace=kube-system --output=yaml | grep clusterIP</code>. the pods point to this ip, which can be confirmed by inspecting the pod using <code class=\"language-text\">kubectl exec any_pod_name -- cat /etc/resolv.conf</code></li>\n</ul>\n<h1 id=\"pods\" style=\"position:relative;\"><a href=\"#pods\" aria-label=\"pods permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Pods</h1>\n<ul>\n<li>the smallest unit in kubernetes</li>\n<li>represents a single running process</li>\n<li>a pod encapsulates one or more containers, but usually we run only one container in a pod</li>\n<li>sidecar pattern - helper containers can be spun alongside the application container in the same pod</li>\n<li>to create a pod in an imperative way using commands, use <code class=\"language-text\">kubectl run db --image=mongo</code></li>\n<li>to get all running pods, use <code class=\"language-text\">kubectl get pods</code>\n<ul>\n<li>to get more information, we can use <code class=\"language-text\">--output=wide</code>, <code class=\"language-text\">--output=yaml</code> or <code class=\"language-text\">--output=json</code></li>\n</ul>\n</li>\n<li>we can do a dry run and get the yaml, e.g. <code class=\"language-text\">kubectl run db --image=mongo --dry-run=client --output=yaml</code></li>\n<li>to see the list of events that occurred, use <code class=\"language-text\">kubectl describe pod db</code></li>\n<li><a href=\"https://gist.github.com/shameekagarwal/8ca1f31a5d76b00c20a5a8a6da3b183b\">yaml file to create a pod declaratively</a></li>\n<li>now, we can run <code class=\"language-text\">kubectl apply -f file_name.yml</code></li>\n<li>we can specify the file name in commands instead of the resource name - <code class=\"language-text\">kubectl describe -f file_name.yml</code>, <code class=\"language-text\">kubectl delete -f file_name.yml</code>, <code class=\"language-text\">kubectl get -f file_name.yml</code></li>\n<li>suppose we want to execute a command against a container. one way would be to issue commands using docker, e.g. <code class=\"language-text\">docker container exec container_name command</code>, this is just like spawning off another process in an already running container. however, we have to issue this command on a particular node of the cluster. this may not matter for minikube since everything is on our local but would matter for a production cluster. another way is to run <code class=\"language-text\">kubectl exec pod_name -- command</code>. this would by default execute the command on the first container of the pod. we can also specify the container using <code class=\"language-text\">--container</code> flag</li>\n<li>to view logs, use <code class=\"language-text\">kubectl logs pod_name</code>. like in <code class=\"language-text\">exec</code>, the container can be specified explicitly</li>\n<li>containers of the same pod run on the same node. they can talk via localhost i.e. if a container is running in a pod on port 8080, the other container can make requests to localhost:8080. they also share the same volumes</li>\n<li>if we stop the container using <code class=\"language-text\">docker container stop container_id</code>, the pod will restart the container</li>\n<li>to stop a pod, use <code class=\"language-text\">kubectl delete pod db</code></li>\n<li>when a pod is deleted\n<ul>\n<li>it first sends <code class=\"language-text\">TERM</code> (terminate) signal to all processes of all containers of the pod</li>\n<li>if it does not stop within the <code class=\"language-text\">gracePeriod</code>, <code class=\"language-text\">KILL</code> signal is sent for a forceful shutdown</li>\n</ul>\n</li>\n<li>all containers part of the same pod coexist in the same node i.e. they cannot be distributed across the nodes</li>\n<li>all of them can also access the same volume</li>\n</ul>\n<h3 id=\"process-of-creation\" style=\"position:relative;\"><a href=\"#process-of-creation\" aria-label=\"process of creation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Process of Creation</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 561px; margin: 5px 0 5px 0 !important;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/f8db3434569ee49748ca9f9bbb2854dd/410f3/pod-creation.drawio.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 71.51898734177216%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsTAAALEwEAmpwYAAABY0lEQVQ4y6WT2Y6CUAyGef8388qERC9AJYJg3BBZ3BBrviYlJyTMOJkmJ2dp+/fvcrzNZiPL5VL2+7283285HA6yWCwkSRK53W5SVZXegyCQsizl8XhIlmUShqGcz2dp21Z9wODuXa9XYTVNo4q6rhWIRYDX69UDc+66Tu/YAcb9+Xyq/n6/i4cTgLvdTnzflzRNpSgKBcOBnQWIiYEi6BDuvHtEBZ0onNmhnue5zOdziaKoj26CHSRcQO68e5aSq4QhjA2Y3WX4K+BQifN2u9VCr1YrbYYxtLr+GTCOY5lMJlrX0+n0P4akSdrH41HTpaPYuN21ADTiK4ZmaALwbDZT1owXzXPlR0DYwYQhZhGAN2oJMF3njbryDjh2o4A48IYhC+fL5dLPJDoC8mOm06kytsH/KmXOsFmv1/rlDBBfm13q3g/2EJCukpp9S1K1eeTPwwbAIYnRwTalOybDsRnz+QCKNz8zM1dYDgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"pod creation\"\n        title=\"pod creation\"\n        src=\"/static/f8db3434569ee49748ca9f9bbb2854dd/410f3/pod-creation.drawio.png\"\n        srcset=\"/static/f8db3434569ee49748ca9f9bbb2854dd/c26ae/pod-creation.drawio.png 158w,\n/static/f8db3434569ee49748ca9f9bbb2854dd/6bdcf/pod-creation.drawio.png 315w,\n/static/f8db3434569ee49748ca9f9bbb2854dd/410f3/pod-creation.drawio.png 561w\"\n        sizes=\"(max-width: 561px) 100vw, 561px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h1 id=\"replicasets\" style=\"position:relative;\"><a href=\"#replicasets\" aria-label=\"replicasets permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>ReplicaSets</h1>\n<ul>\n<li>it is a type of controller i.e. it tries to maintain a specified number of pods</li>\n<li>this provides high fault tolerance, high availability, self-healing mechanism, etc.</li>\n<li>replica sets are the newer version of replication controllers, since replication controllers are deprecated</li>\n<li>note: when setting image using <code class=\"language-text\">kubectl set image...</code> in replica sets, i had to delete the older pods</li>\n<li><a href=\"https://gist.github.com/shameekagarwal/ec1a7d3c31814c789eae2d0e1c1ae569\">yaml example</a></li>\n<li>look how in yml syntax, <code class=\"language-text\">spec.template</code> is the exact same as that of the contents of a pod</li>\n<li>labels defined for a pod should match the labels defined for its replica set i.e. value of <code class=\"language-text\">spec.selector.matchLabels</code> should match <code class=\"language-text\">spec.template.metadata.labels</code> else kubernetes gives an error</li>\n<li>however, the replicaset can manage pods not defined in the <code class=\"language-text\">spec.template</code> section as well. in this case, the labels of pods should match the selector of the replica set</li>\n<li><code class=\"language-text\">spec.replicas</code> defines the number of pods to run</li>\n<li>use <code class=\"language-text\">kubectl get replicasets</code> and <code class=\"language-text\">kubectl get pods</code> to verify</li>\n<li>verifying the self-healing feature - if we try to delete a pod using <code class=\"language-text\">kubectl delete pod pod_name</code>, we will see that the replica set will automatically spin up a new pod</li>\n<li>deleting the replica set will delete the pods it spun up as well</li>\n<li><code class=\"language-text\">kubectl delete -f replica-set.yml --cascade=orphan</code>. this will delete the replica set but not the pods. so, in general, to prevent removal of downstream objects, use the <code class=\"language-text\">cascade</code> flag</li>\n</ul>\n<h3 id=\"process-of-creation-1\" style=\"position:relative;\"><a href=\"#process-of-creation-1\" aria-label=\"process of creation 1 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Process of Creation</h3>\n<p>only the first part has been described here, the remaining parts are similar to that of a pod</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 471px; margin: 5px 0 5px 0 !important;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/c4688f7cdd9f48573c55c65790700a37/1f09d/replica-set-creation.drawio.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 59.49367088607595%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAABOElEQVQoz5WTwY6CQBBE+f8/40ZivKESEFREEBRQQGrzJpkNi5i4kzQDQ3dRXV04x+NRvu/rdDrpdrupbVtxttvtFEWRyrJU13VKkkSu62q1Wulyuej1eok1jqOmyymK4heMQhK5T9NUeZ6rrmv1fS/yNpuNgiAw7z8CcmmaxhQfDgedz2cDNi2yu/2gBZqDGUAOYQEg7azXa4VhqOv1aphNF3nP5/MPs0WGFMOS5MfjYSKOY5N8v9+NpvZ8DvjGkAtFdtlEWgZgu93K8zzt93uTR9tfAVpNbCKTRAZAmTxBJ18znGuCXlgF62AhBkVYXf8NSMtVVRlA7JJl2dtQluIjIC1jIVoFiKHhxfnkFxkOw/AGiN+mX+YZYMJqOtWXc3KcqRZL3rL3MIMxraMlVuKZnT8NFyDHD/sOpirf6oYOAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"replica set creation\"\n        title=\"replica set creation\"\n        src=\"/static/c4688f7cdd9f48573c55c65790700a37/1f09d/replica-set-creation.drawio.png\"\n        srcset=\"/static/c4688f7cdd9f48573c55c65790700a37/c26ae/replica-set-creation.drawio.png 158w,\n/static/c4688f7cdd9f48573c55c65790700a37/6bdcf/replica-set-creation.drawio.png 315w,\n/static/c4688f7cdd9f48573c55c65790700a37/1f09d/replica-set-creation.drawio.png 471w\"\n        sizes=\"(max-width: 471px) 100vw, 471px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h1 id=\"services\" style=\"position:relative;\"><a href=\"#services\" aria-label=\"services permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Services</h1>\n<ul>\n<li>pods are short-lived, so using addresses of pods for inter-pod communication is not reliable</li>\n<li>services can be used to expose pods, replication controllers, replication sets, etc</li>\n<li>service can be of different types\n<ul>\n<li><code class=\"language-text\">NodePort</code> - target port on every node is exposed to the outside world. if we have multiple worker nodes, to hit a particular set of pods, we would have to use <code class=\"language-text\">worker_ip:node_port</code>. this also indicates that the services span multiple nodes without us having to configure anything</li>\n<li><code class=\"language-text\">ClusterIP</code> - this is the default. exposes the port only inside and not from outside the cluster</li>\n<li><code class=\"language-text\">LoadBalancer</code> - useful when deploying to cloud</li>\n<li><code class=\"language-text\">ExternalName</code> - map a service to an external address like a url</li>\n</ul>\n</li>\n<li>imperative command - <code class=\"language-text\">kubectl expose replicaset rs --name=svc --target-port=28017 --type=NodePort</code></li>\n<li>note: <a href=\"https://github.com/kubernetes/kubernetes/issues/25478\">the node port cannot be specified</a> when using <code class=\"language-text\">kubectl expose</code></li>\n<li>if we run <code class=\"language-text\">kubectl describe service svc</code>, we see that it has inherited all the labels of the replica set. recall how replica set is associated to the pods using labels, services are associated to the pods in the same way</li>\n<li>when describing a service, it also shows all endpoints aka pods it directs traffic to</li>\n<li>the three ports involved in node port are -\n<ul>\n<li>node port - how to access from outside the cluster. hit <code class=\"language-text\">http://minikube_ip:NodePort</code>. if not specified, a free port is chosen at random for its value</li>\n<li>port - incoming traffic i.e. traffic from other pods or outside the cluster hit this port of the service</li>\n<li>target port - port of the pod to which the service should forward traffic. if not specified, it takes the same value as port. so, in yml, usually only the port is specified</li>\n</ul>\n</li>\n<li>we can run <code class=\"language-text\">kubectl get endpoints</code> to get a list of all the endpoint objects. we can also get more information about a specific endpoint using <code class=\"language-text\">kubectl get endpoints endpoint_name --output=yaml</code></li>\n<li>we can run <code class=\"language-text\">kubectl exec pod_name env</code> - here, we will get environment variables like <code class=\"language-text\">&lt;&lt;SERVICE_NAME>>_SERVICE_HOST</code>, <code class=\"language-text\">&lt;&lt;SERVICE_NAME>>_SERVICE_PORT</code>. this will have the ip address and port of the different services respectively</li>\n<li>communication - till now, we were using <code class=\"language-text\">&lt;&lt;service-name>></code> for communication. it can be expanded to <code class=\"language-text\">&lt;&lt;service-name>>.&lt;&lt;namespace-name>></code>. if we don’t specify the namespace-name, it defaults to the namespace in which the resource initiating the request is</li>\n<li>communication to services can be further expanded to <code class=\"language-text\">&lt;&lt;service-name>>.&lt;&lt;namespace-name>>.svc</code> or <code class=\"language-text\">&lt;&lt;service-name>>.&lt;&lt;namespace-name>>.svc.cluster.local</code>. this bit can be confirmed using <code class=\"language-text\">kubectl exec any_pod_name -- cat /etc/resolv.conf</code> under the search field</li>\n<li>by default, direct communication to pods is not enabled. if we enable it, we can use <code class=\"language-text\">&lt;&lt;modified-pod-ip>>.&lt;&lt;namespace-name>>.pod.cluster.local</code>. here, the modified-pod-ip is constructed by replacing <code class=\"language-text\">.</code> with <code class=\"language-text\">-</code> i.e. 10.244.2.5 becomes 10-244-2-5. again, because of the search field in /etc/resolv.conf, we can skip cluster.local (but not <code class=\"language-text\">pod</code> since <code class=\"language-text\">svc</code> is the default). my doubt - at this point, i might as well use the pod’s ip address directly?</li>\n</ul>\n<h3 id=\"process-of-creation-2\" style=\"position:relative;\"><a href=\"#process-of-creation-2\" aria-label=\"process of creation 2 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Process of Creation</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; margin: 5px 0 5px 0 !important;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/19b8abfa18c7977af4880e861abca8cb/c7dcc/service-creation.drawio.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 68.9873417721519%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsTAAALEwEAmpwYAAABcklEQVQ4y4WT646CQAyFef8n4wH4YQKiBAQUlZviBaSbr0k3swbXSSbQmfb09LTjpWkqWZbJ9XqVruskjmPJ81wej4ecTifxfV82m41M0yS73U6SJJGyLOX1esk8z8I6n896FwSBeBhN0+gFTsfjUdq2VYDb7SZFUWgigvlWVaXJsQ3wfr9LXdey3+/FA6Tvew08HA767y6YLtkGZl8IsD0cYARlQGHAmTkMw/AbRHJsA3JZPp9PjVNAl8XlclGN0A9wk4NAAJFhieEfQAxb6AHQdrtV1rB3lwG6LP8FpCTTFHZWMmc0A0Cm4l3rj4A4Ui5MAQCUzq9WKwnDUNkzHnQUjcdx/M4QFjiyScA9+mEzOjAkSRRFqvlXDd31bpsExDBmNpP4LQKSkUZwjhMlw5h/vtgue14VeiMBZ4slE2xzSAIcTS9sXgVJYYfNNmkWS0avT2OC/3q91vfNvNIk9OQJLwICAJt3zQwQGxD0c+OsKT9Y3j97h9yO5wAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"service creation\"\n        title=\"service creation\"\n        src=\"/static/19b8abfa18c7977af4880e861abca8cb/f058b/service-creation.drawio.png\"\n        srcset=\"/static/19b8abfa18c7977af4880e861abca8cb/c26ae/service-creation.drawio.png 158w,\n/static/19b8abfa18c7977af4880e861abca8cb/6bdcf/service-creation.drawio.png 315w,\n/static/19b8abfa18c7977af4880e861abca8cb/f058b/service-creation.drawio.png 630w,\n/static/19b8abfa18c7977af4880e861abca8cb/c7dcc/service-creation.drawio.png 641w\"\n        sizes=\"(max-width: 630px) 100vw, 630px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h1 id=\"liveliness-probe\" style=\"position:relative;\"><a href=\"#liveliness-probe\" aria-label=\"liveliness probe permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Liveliness Probe</h1>\n<ul>\n<li>used for configuring health checks, done at a container level</li>\n<li>if the health check fails, it applies the restart policy which defaults to always</li>\n<li>the restart policy is specified at the pod level and applies to all containers</li>\n<li><code class=\"language-text\">initialDelaySeconds</code> - when should the probe start</li>\n<li><code class=\"language-text\">timeoutSeconds</code> - after waiting for how many seconds should the probe fail</li>\n<li><code class=\"language-text\">periodSeconds</code> - after how many seconds should the probe be repeated</li>\n<li><code class=\"language-text\">failureThreshold</code> - how many consecutive health checks are allowed to fail</li>\n<li>code example\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-yaml line-numbers\"><code class=\"language-yaml\"><span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> api\n  <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> user<span class=\"token punctuation\">-</span>service\n  <span class=\"token key atrule\">livenessProbe</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">httpGet</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">path</span><span class=\"token punctuation\">:</span> /actuator/health\n      <span class=\"token key atrule\">port</span><span class=\"token punctuation\">:</span> <span class=\"token number\">8080</span>\n    <span class=\"token key atrule\">initialDelaySeconds</span><span class=\"token punctuation\">:</span> <span class=\"token number\">20</span>\n    <span class=\"token key atrule\">timeoutSeconds</span><span class=\"token punctuation\">:</span> <span class=\"token number\">5</span>\n    <span class=\"token key atrule\">periodSeconds</span><span class=\"token punctuation\">:</span> <span class=\"token number\">5</span>\n    <span class=\"token key atrule\">failureThreshold</span><span class=\"token punctuation\">:</span> <span class=\"token number\">3</span></code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n</li>\n</ul>\n<h1 id=\"readiness-probe\" style=\"position:relative;\"><a href=\"#readiness-probe\" aria-label=\"readiness probe permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Readiness Probe</h1>\n<ul>\n<li>it is used to determine whether a pod is ready to serve requests</li>\n<li>it has the same configuration as liveliness probe</li>\n<li>ip addresses of unhealthy pods are removed from ip tables, so that the future requests do not make it to them</li>\n</ul>\n<h1 id=\"an-example\" style=\"position:relative;\"><a href=\"#an-example\" aria-label=\"an example permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>An Example</h1>\n<ul>\n<li><a href=\"https://gist.github.com/shameekagarwal/1883a95d8be0a74030b77966d80196a0\">a complete example</a> of\n<ul>\n<li>a database and exposing it using cluster ip</li>\n<li>backend service which talks to db, exposing it using node port, configuring health checks</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"deployments\" style=\"position:relative;\"><a href=\"#deployments\" aria-label=\"deployments permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Deployments</h1>\n<ul>\n<li>helps us achieve zero downtime when we deploy services</li>\n<li>we should not create pods or even replica sets directly</li>\n<li>deployments create replica sets behind the scenes</li>\n<li>when we make an update to for e.g. the image version, the deployment will first create a new replica set with the desired number of pods, and once that replica set has successfully scaled the pods, the deployment would mark the desired replicas of the older replica set as 0. a part of <code class=\"language-text\">kubectl describe deployment db</code> -\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">Type    Reason             Age    Message\n----    ------             ----   -------\nNormal  ScalingReplicaSet  12m    Scaled up replica set db-5cc56bf6fb to 1\nNormal  ScalingReplicaSet  4m22s  Scaled up replica set db-76774bbdf to 1\nNormal  ScalingReplicaSet  92s    Scaled down replica set db-5cc56bf6fb to 0</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n</li>\n<li>a side note - the random characters that we see are actually the hash value of the pod template</li>\n<li>to create a deployment imperatively, use <code class=\"language-text\">kubectl create deployment nginx --image=nginx --replicas=2</code>\n<ul>\n<li>we can also add flags <code class=\"language-text\">--dry-run=client --output=yaml</code> to generate the yaml</li>\n</ul>\n</li>\n<li>deployment strategy can be rolling update (default) or recreate</li>\n<li>in recreate, the old pods are stopped and new ones are created in its place. this leads to some downtime. use recreate when the coexistence of two versions of the applications can cause inconsistencies e.g. db migrations</li>\n<li>in rolling deployments, the new replica set is scaled up and the old replica set is scaled down simultaneously gradually. they can be tweaked using <code class=\"language-text\">maxSurge</code> and <code class=\"language-text\">maxUnavailable</code> fields. at any given time, we can have a maximum of desired + <code class=\"language-text\">maxSurge</code> or a minimum of desired - <code class=\"language-text\">maxUnavailable</code> pods running. both can be absolute numbers or % and both default to 25%. since both versions of applications run in parallel, the response can be returned from either of the versions at random during deployment</li>\n<li>e.g. of rolling deployment - by using the following code, the deployment order is 3 old ➝ 3 old, 1 new ➝ 2 old, 1 new ➝ 2 old, 2 new ➝ 1 old, 2 new ➝ 1 old, 3 new ➝ 3 new\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-yaml line-numbers\"><code class=\"language-yaml\"><span class=\"token key atrule\">replicas</span><span class=\"token punctuation\">:</span> <span class=\"token number\">3</span>\n\n<span class=\"token key atrule\">strategy</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">rollingUpdate</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">maxSurge</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span>\n    <span class=\"token key atrule\">maxUnavailable</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span></code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n</li>\n<li>everytime we deploy in kubernetes, a rollout takes place and a revision is created</li>\n<li>we can monitor the status of the update to deployment using <code class=\"language-text\">kubectl rollout status -f deployment.yml</code></li>\n<li>we can view the history of updates using <code class=\"language-text\">kubectl rollout history -f deployment.yml</code></li>\n<li>we can also create a rollback using <code class=\"language-text\">kubectl rollout undo -f deployment.yml</code>\n<ul>\n<li>if we want to go back to a much older version and not just the previous one, we can use <code class=\"language-text\">kubectl rollout undo -f deployment.yml --to-revision=2</code></li>\n</ul>\n</li>\n<li>side note: rollbacks might not always be possible e.g. if we had database migrations. so, we may need to roll forward in some cases i.e. implement a hot fix and redeploy the new changes</li>\n<li>using labels -\n<ul>\n<li><code class=\"language-text\">kubectl get all --show-labels</code> - show the resources with their labels</li>\n<li><code class=\"language-text\">kubectl get all --selector=name=db,app=demo</code> - filter the resources using their labels</li>\n<li>e.g. to count the total number of resources in dev environment, use <code class=\"language-text\">kubectl get all --selector=env=dev --no-headers | wc -l</code></li>\n</ul>\n</li>\n<li>we can set image of a deployment using <code class=\"language-text\">kubectl set image deployment db db=mongo:3.3</code>, where the first db is the deployment name and the second db is the container name, since we can have multi container pod</li>\n<li>to add the default change cause to the <code class=\"language-text\">kubectl rollout history ...</code> output, append commands with <code class=\"language-text\">--record</code>, e.g. <code class=\"language-text\">kubectl apply -f infra --record</code>. this flag is deprecated but i cannot find its replacement</li>\n<li>to scale deployments imperatively, use <code class=\"language-text\">kubectl scale deployment api --replicas=2</code></li>\n<li>both in deployments and in services, any one of the labels on pod need to be present in <code class=\"language-text\">spec.selector</code></li>\n</ul>\n<h3 id=\"process-of-creation-3\" style=\"position:relative;\"><a href=\"#process-of-creation-3\" aria-label=\"process of creation 3 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Process of Creation</h3>\n<ul>\n<li>a deployment controller will watch for new deployment creation requests</li>\n<li>it will then create replica set definitions on api server</li>\n<li>after this, the process of replica set creation is continued</li>\n</ul>","frontmatter":{"title":"Kubernetes - Part I"}}},"pageContext":{"id":"2c7b9d71-182c-506f-b74f-75a5715793f5"}},"staticQueryHashes":["1037383464","1617985380"]}