{"componentChunkName":"component---src-templates-blog-js","path":"/aws-part-vi/","result":{"data":{"markdownRemark":{"fields":{"slug":"/aws-part-vi/"},"id":"b7cfb081-4911-53ea-bb72-d1e18328977f","html":"<h1 id=\"kms\" style=\"position:relative;\"><a href=\"#kms\" aria-label=\"kms permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>KMS</h1>\n<ul>\n<li>a region resilient and public service</li>\n<li>manage keys for encryption and decryption, both symmetric and asymmetric</li>\n<li>kms can perform cryptographic operations like encryption and decryption as well</li>\n<li>keys never leave kms. they can be imported into kms or created inside kms, but kms keys never leave kms</li>\n<li>security standard used by kms is fips 140-2 (level 2 or l2)</li>\n<li>kms keys are actually backed by physical material bts</li>\n<li>kms keys can be used to encrypt or decrypt upto 4kb of data</li>\n<li>for encryption, the key needs to be specified but for decryption, we do not need to specify the key to use as the information for the key to use is a part of the cipher text - this means if we want to use key x, we need to specify key x when making the encrypt api call to kms, but for the decrypt api call, not providing the key will do</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-shell line-numbers\"><code class=\"language-shell\">aws kms encrypt <span class=\"token punctuation\">\\</span>\n  --key-id alias/CatRobot <span class=\"token punctuation\">\\</span>\n  <span class=\"token parameter variable\">--plaintext</span> fileb://BattlePlans.txt <span class=\"token punctuation\">\\</span>\n  <span class=\"token parameter variable\">--output</span> text <span class=\"token punctuation\">\\</span>\n  <span class=\"token parameter variable\">--query</span> CiphertextBlob <span class=\"token punctuation\">\\</span>\n  <span class=\"token operator\">|</span> base64 <span class=\"token parameter variable\">--decode</span> <span class=\"token operator\">></span> BattlePlans.enc\n\n<span class=\"token comment\"># notice how key id is not provided for decrypt</span>\naws kms decrypt <span class=\"token punctuation\">\\</span>\n  --ciphertext-blob fileb://BattlePlans.enc <span class=\"token punctuation\">\\</span>\n  <span class=\"token parameter variable\">--output</span> text <span class=\"token punctuation\">\\</span>\n  <span class=\"token parameter variable\">--query</span> Plaintext <span class=\"token punctuation\">\\</span>\n  <span class=\"token operator\">|</span> base64 <span class=\"token parameter variable\">--decode</span> <span class=\"token operator\">></span> BattlePlansDecrypted.txt</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n<ul>\n<li>dek - data encryption keys. kms keys can be used to generate dek using generate data key api</li>\n<li>dek can encrypt data > 4kb in size unlike kms keys</li>\n<li>deks are not stored inside kms, and it is the clients job to perform encryption or decryption</li>\n<li>when we generate a dek, kms provides us with the plaintext version and the ciphertext version of the dek</li>\n<li>the idea is that we would encrypt our data using the plaintext dek, discard it, and store the encrypted dek alongside the generated cipher text of our data</li>\n<li>now, to decrypt data, we pass the encrypted dek to get back the decrypted plaintext dek, which can be used again for decrypting our encrypted data</li>\n<li>kms keys are isolated to a region</li>\n<li>kms supports multi region keys - done by replicating them across regions bts</li>\n<li>kms keys can be -\n<ul>\n<li>aws owned - bts stuff</li>\n<li>customer owned\n<ul>\n<li>aws managed - created automatically by aws services like s3. rotates once per year</li>\n<li>customer managed - created explicitly by us for usage by a service or application. these are more customizable than aws managed, e.g. rotation, resource policy etc. can be modified</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>when rotating, kms stores the previous versions of backing material so that old data can still be decrypted</li>\n<li>aliases - refer keys easily, e.g. my-app</li>\n<li>permissions on kms keys - most services trust the aws account they are contained within, so configuring the identity policy is enough. however, in case of kms, this trust does not exist by default. so, my understanding - we need to even add the account inside which the key is being created to the trust policy of the resource policy of this kms key being created (if we want identity policies of this account to work)</li>\n<li>role separation - kms has different permissions for managing keys vs using keys</li>\n</ul>\n<h1 id=\"cloud-hsm\" style=\"position:relative;\"><a href=\"#cloud-hsm\" aria-label=\"cloud hsm permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Cloud HSM</h1>\n<ul>\n<li>similar to kms</li>\n<li>but, kms as a service is shared between all customers with different accounts</li>\n<li>in cloud hsm (hardware security module), aws dedicates one hsm for us</li>\n<li>cloud hsm is fips 140-2 level 3 (kms is overall 140-2 l2, some in kms are l3)</li>\n<li>“kms custom key store” - kms can use cloud hsm as a key store</li>\n<li>hsm is az resilient, so we need to deploy a hsm cluster in multiple az for high resilience. replication of keys etc. is handled underneath, and interface are injected into the subnets</li>\n<li>cloud hsm cannot be used for server side encryption e.g. like sse s3 kms, but can be used for client side encryption, where for e.g. the ec2 basically gains access to the key and uses it for encryption</li>\n</ul>\n<h1 id=\"s3\" style=\"position:relative;\"><a href=\"#s3\" aria-label=\"s3 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>S3</h1>\n<ul>\n<li>simple storage service or s3</li>\n<li>it is an aws public service</li>\n<li>it is region resilient i.e. data is replicated across azs for resilience</li>\n<li>it is an object storage, so it stores objects inside containers called buckets</li>\n<li>an object has a key, like filename, and a value, which is the actual content of the object</li>\n<li>size of an object can be zero bytes to 5TB</li>\n<li>we can have unlimited objects inside a bucket</li>\n<li>bucket names should be globally unique across all regions and all accounts</li>\n<li>s3 buckets do not have a complex structure - it is flat unlike a file system which has nested folders. if the keys of our objects have /a/abc.png, then s3 presents them inside a folder called a. the part of the key upto before the last part which looks like a folder structure is called the prefix</li>\n<li>number of buckets have a soft limit of 100 and a hard limit of 1000 per account</li>\n<li>bucket owners pay for s3 storage and data transfer costs associated with their bucket. however, we can configure a bucket to be a requester pays bucket. with requester pays buckets, the requester instead of the bucket owner pays the cost of the request and the data download from the bucket. the bucket owner still pays the cost of storing data</li>\n</ul>\n<h1 id=\"s3-security\" style=\"position:relative;\"><a href=\"#s3-security\" aria-label=\"s3 security permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>S3 Security</h1>\n<ul>\n<li>bucket policy - a resource policy</li>\n<li>my understanding - identity policy controls what an identity can do, but can only be attached to an identity. also, identity policy can only control access to resources within the same account, since the aws account trusts its own iam. for allowing access to users in another account, we use resource policies, since they can reference identities for same or other accounts</li>\n<li>resource policy can also handle anonymous principals, unlike identity policies which can only be attached to a identity that is valid</li>\n<li>so, examples where resource policy can be used -\n<ul>\n<li>other aws accounts</li>\n<li>anonymous access</li>\n</ul>\n</li>\n<li>acl (access control lists) - deprecated. also, acls can only be used for a bucket or an object, they cannot be used for a group of objects, so they are much more inflexible. they have 5 kinds of permissions -\n<ul>\n<li>read\n<ul>\n<li>bucket - list objects in a bucket</li>\n<li>object - read a specific object</li>\n</ul>\n</li>\n<li>write\n<ul>\n<li>bucket - crud objects</li>\n<li>objects - not applicable</li>\n</ul>\n</li>\n<li>read acp - read acp, again can be bucket or object level</li>\n<li>write acp - write acp, again can be bucket or object level</li>\n<li>full control -\n<ul>\n<li>bucket - read, write, read acp, write acp</li>\n<li>object - read, read acp, write acp</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>block public access - added later on after incorrectly configured buckets lead to data leaks. my understanding - only applies to anonymous access and not aws identities?</li>\n</ul>\n<h1 id=\"s3-static-web-hosting\" style=\"position:relative;\"><a href=\"#s3-static-web-hosting\" aria-label=\"s3 static web hosting permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>S3 Static Web Hosting</h1>\n<ul>\n<li>s3 can be accessed via http</li>\n<li>we have to set an index and error html for this</li>\n<li>the endpoint created is a combination of region and bucket name</li>\n<li>we can use custom domains, the limitation is that bucket name has to be the same as the fqdn</li>\n<li>we create a record with policy as simple routing policy, choose the target as alias to s3 website endpoint (other options are, for instance, ip address, alias to cdn, alias to nlb, etc.), chose the region of the bucket, and then the ui automatically allows selecting of our bucket</li>\n<li>note - no need for hosted zone and s3 bucket to be in the same region</li>\n<li>offloading - delivery of static assets can be done by s3 and not compute. this makes our solution cheaper</li>\n<li>out of band pages - maintenance page when servers are being upgraded. we cannot use the same servers to serve such pages since they are down</li>\n<li>to use this feature\n<ul>\n<li>uncheck block all public access</li>\n<li>enable static website hosting</li>\n<li>add a resource policy like this</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"json\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-json line-numbers\"><code class=\"language-json\"><span class=\"token punctuation\">{</span>\n  <span class=\"token property\">\"Version\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"2012-10-17\"</span><span class=\"token punctuation\">,</span>\n  <span class=\"token property\">\"Statement\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span>\n    <span class=\"token punctuation\">{</span>\n      <span class=\"token property\">\"Principal\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"*\"</span><span class=\"token punctuation\">,</span>\n      <span class=\"token property\">\"Effect\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Allow\"</span><span class=\"token punctuation\">,</span>\n      <span class=\"token property\">\"Action\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span> <span class=\"token string\">\"s3:GetObject\"</span> <span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n      <span class=\"token property\">\"Resource\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span> <span class=\"token string\">\"arn:aws:s3:::bytecode.org/*\"</span> <span class=\"token punctuation\">]</span>\n    <span class=\"token punctuation\">}</span>\n  <span class=\"token punctuation\">]</span>\n<span class=\"token punctuation\">}</span></code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n</li>\n</ul>\n<h1 id=\"s3-object-versioning\" style=\"position:relative;\"><a href=\"#s3-object-versioning\" aria-label=\"s3 object versioning permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>S3 Object Versioning</h1>\n<ul>\n<li>starts off disabled</li>\n<li>we can enable it if we want</li>\n<li>we cannot disable versioning once it has been enabled</li>\n<li>we can suspend it though, and then enable it again</li>\n<li>what this actually means - for e.g. for storage, we will be billed for existing versions even if we suspend versioning. suspending versioning does not delete the old versions</li>\n<li>without versioning, each object is identified by its key. updating an object replaces the old one</li>\n<li>with versioning, a version is associated to each object, and on updating it, a new version is added</li>\n<li>version id of an object is null if versioning is not enabled</li>\n<li>we interact with the latest version if we do not specify one</li>\n<li>when deleting an object, a delete marker is added, which is just like a new version of the object</li>\n<li>we can delete versions as well, which is called a permanent delete</li>\n<li>deleting the delete marker undeletes the object</li>\n<li>mfa delete - mfa is required to -\n<ul>\n<li>change versioning state (suspend or enable)</li>\n<li>permanently delete a version</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"s3-performance\" style=\"position:relative;\"><a href=\"#s3-performance\" aria-label=\"s3 performance permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>S3 Performance</h1>\n<ul>\n<li>by default, an upload happens as a single stream</li>\n<li>if a stream fails, the whole upload fails, and we will have to go through the entire upload process again</li>\n<li>anyway, we can only use single stream upload for a maximum of 5gb</li>\n<li>so, solution is to use multipart uploads</li>\n<li>minimum size for a multipart upload is 100mb</li>\n<li>we can have a maximum of 10000 parts</li>\n<li>a part can be 5mb to 5gb (since the last part is leftover, it can be &#x3C; 5mb)</li>\n<li>parts can fail and be restarted individually for uploads</li>\n<li>transfer acceleration - when transferring objects to s3, the traffic normally goes through public internet and then reaches the region where the bucket is. with transfer acceleration, the traffic goes to edge locations and then uses aws global network to get to the bucket, which is way faster than public internet</li>\n<li>byte range fetch - only specified part is transferred. we specify the byte range of the object we want to fetch. we can also make concurrent such calls for better performance when compared to a single call. so, it feels kind of like multipart download but for downloading</li>\n</ul>\n<h1 id=\"s3-encryption\" style=\"position:relative;\"><a href=\"#s3-encryption\" aria-label=\"s3 encryption permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>S3 Encryption</h1>\n<ul>\n<li>buckets cannot be encrypted, objects can</li>\n<li>encryption is at object level i.e. each object inside a bucket can have different encryption strategies</li>\n<li>the encryption being talked about above is encryption at rest, encryption in transit can be achieved using tls</li>\n<li>two methods of encryption at rest -\n<ul>\n<li>client side encryption - client sends ciphertext to s3. implication - we control the management of keys, the compute for encryption and decryption, etc</li>\n<li>server side encryption - client sends plaintext, s3 encrypts it\n<ul>\n<li>sse-c - server side encryption with customer provided keys. s3 manages encryption and decryption. we offload the compute for expensive cryptographic operations to s3. the client has to however manage keys. when putting an object, we provide the plaintext object and key. s3 hashes the key, encrypts the object, discards the key and stores them together. when getting an object, s3 verifies that the key we provide matches the hashed key, and then decrypts the object, discards the key and then sends us the object</li>\n<li>sse-s3 - server side encryption with s3 managed keys. aws handles key management and cryptographic operations. clients only have to send the object. my understanding - s3 uses a root key, which generates one dek per object bts. the functionality of storing the encrypted dek alongside the object, decrypting it using the root key, etc. is the same as kms. the algorithm used is aes 256. disadvantage - no control over keys, no role separation, no key rotation, etc</li>\n<li>sse-kms - server side encryption with kms keys. the functionality is similar to sse-s3, but there is role separation here. deks are generated and used. can be both aws managed or customer managed. use customer managed in regulatory environments for more control. <u>so, when using sse-kms, for e.g. we need to have permissions on kms as well so that we can view objects</u></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>encryption actually uses the x-amz-server-side-encryption header in api calls. its values can be aws:kms or aes256 for sse-kms and sse-s3 respectively</li>\n<li>we can set a bucket level default for this header, which gets overridden if we provide this header again when putting the object to the bucket</li>\n</ul>\n<h1 id=\"s3-storage-classes\" style=\"position:relative;\"><a href=\"#s3-storage-classes\" aria-label=\"s3 storage classes permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>S3 Storage Classes</h1>\n<ul>\n<li>standard - the default, data is replicated across at least 3 azs. 11 9s or 99.999999999% durable. costs for storage, data out of s3 (into s3 is free), and for number of api operations done. crcs are used to ensure data integrity. first byte latency i.e. object is available within milliseconds. objects can be made publicly available. <u>we can only transition objects to standard ia / one zone ia after 30 days, but there is no such limit when transitioning to glacier?</u></li>\n<li>standard ia - similar to standard in durability and availability. cost model has the same components but is cheaper. it also has a new component for retrieval of objects, in addition to the transfer out fee. also, there is a minimum charge for a duration of 30 days, and we are charged at a minimum for 128kb for each object irrespective of the size of object</li>\n<li>one zone ia - similar to standard ia in cost model. only stored in one az, making it cheaper and less available. durability is still the same. use case - intermediate copy of data that is reproducible</li>\n<li>glacier instant - similar to standard ia in availability etc. minimum of 90days, higher costs of retrievals</li>\n<li>glacier flexible - initially called just glacier. same as glacier instant. however, objects stored inside glacier flexible (and glacier deep) are cold objects. a retrieval process needs to take place to read them, and for this, they are stored in standard ia in the interim. retrieval has 3 types -\n<ul>\n<li>expedited - 1-5mins - sometimes, in case of high demands, expedited retrievals can be rejected. we can however use provisioned capacity to combat this and ensure retrieval is available when we need it</li>\n<li>standard - 3-5hrs</li>\n<li>bulk - 5-12hrs</li>\n</ul>\nso, first byte latency is of minutes or hours. also, objects cannot be made publicly accessible</li>\n<li>glacier deep - similar to glacier flexible, the two retrieval types are -\n<ul>\n<li>standard - 12hrs</li>\n<li>bulk - 48hrs</li>\n</ul>\n</li>\n<li>aws prep question - when the word “archive” comes up, think glacier, not infrequently accessed</li>\n</ul>\n<h1 id=\"s3-intelligent-tiering\" style=\"position:relative;\"><a href=\"#s3-intelligent-tiering\" aria-label=\"s3 intelligent tiering permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>S3 Intelligent Tiering</h1>\n<ul>\n<li>has its own storage classes -\n<ul>\n<li>frequent</li>\n<li>infrequent</li>\n<li>archive instant</li>\n<li>archive</li>\n<li>archive deep</li>\n</ul>\n</li>\n<li>the intelligent tiering system automatically does the transitioning of objects for us based on how frequently they are accessed</li>\n<li>we can configure based on bucket prefixes, can opt out of some tiers, etc</li>\n</ul>\n<h1 id=\"s3-lifecycle-configuration\" style=\"position:relative;\"><a href=\"#s3-lifecycle-configuration\" aria-label=\"s3 lifecycle configuration permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>S3 Lifecycle Configuration</h1>\n<ul>\n<li>automatically transition or expire objects</li>\n<li>has a set of rules, which have actions</li>\n<li><u>rules can apply to the whole bucket, a prefix or objects with some tags</u></li>\n<li>actions can be -\n<ul>\n<li>transition actions - change the storage class</li>\n<li>expiration actions - delete objects</li>\n</ul>\n</li>\n<li><u>both actions can work on buckets with or without versioning</u> - note how this behavior is different from, for e.g. s3 replication or object locks where versioning is essential?</li>\n<li>the rules here are not based on how frequently objects are accessed, that is intelligent tiering. we use this feature when we are sure about when to transition or expire our data</li>\n<li>all storage classes and intelligent tiering are supported. we can only go downward i.e. from a higher storage class to a lower storage class (intelligent tiering is between standard ia and one zone ia)</li>\n<li>we can go from any higher storage class to any lower storage class except one zone ia to glacier instant</li>\n<li>refer the diagram below for the two points above</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; margin: 5px 0 5px 0 !important;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/55ab58bb429c16cf745294f3956279ff/21b4d/lifecycle-transitions.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.32911392405063%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAA20lEQVQoz5XQ124DMAgF0Pz/R3ZIXak3xngCkVu1ah6acYUs++EIzEH/RpjXnKMLL901v+rnIqznOZy9eM6KOfrViCtKJ11jn7PpbDKqqvyPVWVpryO4UKkNpAq1Z2xYK9QGqMyX8OZLsDRjg/cJc8sBECjHAg54XcE7i4VGBypQSi4YARMUY8Jc6zreXjkX8NEhJWtNiv7l45X2t2/Ae32rNkolh16Bknt/e2oj34pVdU62Lh6N+zT+8eHZhuPeishNeM+/OMYUU7TGFqI7On/3YRFm/m14B76QE0uxiMztYcIdAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"lifecycle transitions\"\n        title=\"lifecycle transitions\"\n        src=\"/static/55ab58bb429c16cf745294f3956279ff/f058b/lifecycle-transitions.png\"\n        srcset=\"/static/55ab58bb429c16cf745294f3956279ff/c26ae/lifecycle-transitions.png 158w,\n/static/55ab58bb429c16cf745294f3956279ff/6bdcf/lifecycle-transitions.png 315w,\n/static/55ab58bb429c16cf745294f3956279ff/f058b/lifecycle-transitions.png 630w,\n/static/55ab58bb429c16cf745294f3956279ff/40601/lifecycle-transitions.png 945w,\n/static/55ab58bb429c16cf745294f3956279ff/78612/lifecycle-transitions.png 1260w,\n/static/55ab58bb429c16cf745294f3956279ff/21b4d/lifecycle-transitions.png 1280w\"\n        sizes=\"(max-width: 630px) 100vw, 630px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h1 id=\"s3-replication\" style=\"position:relative;\"><a href=\"#s3-replication\" aria-label=\"s3 replication permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>S3 Replication</h1>\n<ul>\n<li>replication can be cross region (crr) or same region (srr)</li>\n<li>we configure a source bucket to specify the target bucket to replicate to and the iam role to use</li>\n<li>if in the same aws account, s3 buckets trust the iam of the account they are present in</li>\n<li>when configuring replication across accounts, we need to allow the role inside the bucket policy as well</li>\n<li>we can filter what objects to replicate based on prefix or tags (this is a repeating theme, this feature is available in lifecycle configuration as well)</li>\n<li>ownership - by default, the owner of the replicated objects is the source bucket. this can make accessing the objects difficult if the destination bucket is in a different account. we can configure the owner to be the account of the destination bucket</li>\n<li>rtc - replication time control - adds a 15 minute sla. without this, replication is a best efforts</li>\n<li>replication is not retroactive - existing objects inside the source bucket are not replicated, only objects which are newly added get replicated. when configuring this in the console however, i did get a popup that can perform a batch operation to achieve this</li>\n<li>versioning needs to be enabled on both source and target bucket for replication</li>\n<li>replication is one directional - objects added to destination bucket do not go to source bucket</li>\n<li>we can also choose a storage class to copy to. the default is to use the same storage class, but we can, for e.g., use one zone ia to save costs since this is a secondary copy</li>\n<li>glacier flexible or glacier deep cannot be replicated</li>\n<li>deletes markers are not replicated by default, but this can be enabled</li>\n<li>some extra configuration might be needed for replicating encrypted objects</li>\n</ul>\n<h1 id=\"s3-pre-signed-urls\" style=\"position:relative;\"><a href=\"#s3-pre-signed-urls\" aria-label=\"s3 pre signed urls permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>S3 Pre-signed Urls</h1>\n<ul>\n<li>only authenticated users can access objects inside an s3 bucket</li>\n<li>use case - a scenario where we cannot have iam users for our users, and we cannot make our objects public</li>\n<li>we can generate pre-signed urls for an object</li>\n<li>they will have an expiration time</li>\n<li>pre-signed urls <u>can be used for both put and get object calls</u></li>\n<li>stupidity - we can generate pre-signed urls for objects we do not have access to - and people using the pre-signed url will not have access to the object as well</li>\n<li>the person using the pre-signed url has the same permissions as the person who generated it <u>at that time</u>. e.g. when generated, the generator had put permission but later on, it was removed. people using the pre-signed url can no longer use put as well</li>\n<li>do not use iam roles to generate pre-signed urls in general - pre-signed urls have a much longer possible expiration than iam role credentials, and once the temporary credentials of the iam role expire, the pre-signed url can no longer be used</li>\n</ul>\n<h1 id=\"s3-select-and-glacier-select\" style=\"position:relative;\"><a href=\"#s3-select-and-glacier-select\" aria-label=\"s3 select and glacier select permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>S3 Select and Glacier Select</h1>\n<ul>\n<li>recall how data out of s3 has cost but into s3 is free</li>\n<li>retrieve parts of objects instead of the whole to save cost, latency, etc</li>\n<li>this is server side filtering, not client side filtering</li>\n<li>we can use sql like statements to retrieve parts of objects</li>\n<li>works for objects of types csv, json, parquet, etc</li>\n</ul>\n<h1 id=\"s3-events\" style=\"position:relative;\"><a href=\"#s3-events\" aria-label=\"s3 events permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>S3 Events</h1>\n<ul>\n<li>notification is generated when certain events occur</li>\n<li>events can be put, multipart upload complete, etc</li>\n<li><u>notification destinations can be sns, sqs, lambda</u></li>\n<li>the resource policies of the destinations should allow interaction from s3</li>\n<li>event bridge is the preferred alternative to s3 events</li>\n</ul>\n<h1 id=\"s3-access-logs\" style=\"position:relative;\"><a href=\"#s3-access-logs\" aria-label=\"s3 access logs permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>S3 Access Logs</h1>\n<ul>\n<li>the types of access that occurs on the source bucket can be logged on the target bucket</li>\n<li>best efforts - can take a few hours to see in the destination bucket</li>\n<li>the s3 log delivery group is the agent which writes to the destination bucket, so the destination bucket’s resource policy needs to be adjusted accordingly. e.g. the principal is <code class=\"language-text\">logging.s3.amazonaws.com</code></li>\n<li>log files are put into the destination bucket, where each access is in a new line and contains information like status code, date and time, error code, requestor, etc</li>\n<li><u>a single target bucket can be used for multiple source buckets</u>. for this, each source bucket can use a different prefix inside the target bucket</li>\n</ul>\n<h1 id=\"s3-object-locks\" style=\"position:relative;\"><a href=\"#s3-object-locks\" aria-label=\"s3 object locks permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>S3 Object Locks</h1>\n<ul>\n<li>once we enable it, we cannot disable it</li>\n<li>versioning is required for this, so once object lock is enabled, versioning cannot be disabled</li>\n<li>worm (write once read many) model - so, objects once written cannot be deleted or overwritten</li>\n<li>my understanding - the statement above is for object versions - so basically updating an object is allowed?</li>\n<li>there are two ways object locks are managed -\n<ul>\n<li>retention periods - we can specify the number of days / years for which it is applied\n<ul>\n<li>compliance - the object lock settings cannot be changed by anyone (not even root account)</li>\n<li>governance - special permissions (<code class=\"language-text\">s3:BypassGovernanceRetention</code>) exist to allow changing of object lock settings. they need to pass the header <code class=\"language-text\">x-amz-bypass-governance-retenion</code> as true when making api calls. this header is set by default in the console ui if we have the bypass governance permission</li>\n</ul>\n</li>\n<li>legal holds - we can set it to on or off. when enabled, the object cannot be overwritten. a special permission of <code class=\"language-text\">s3:PutObjectLegalHold</code> is needed to override this setting</li>\n</ul>\n</li>\n<li>my understanding - with this feature, ec2 termination protection, etc. the idea is special permissions are needed for enabling or disabling a check. post disabling this check, the irreversible actions can be carried out</li>\n<li>objects can have both, either or neither of them</li>\n<li>just like encryption, they can be specified at object level with a bucket level default</li>\n</ul>\n<h1 id=\"s3-access-points\" style=\"position:relative;\"><a href=\"#s3-access-points\" aria-label=\"s3 access points permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>S3 Access Points</h1>\n<ul>\n<li>network points via which we can perform operations on s3 bucket<strong>s</strong></li>\n<li>have their own endpoint policy and network controls</li>\n<li>multi region access endpoints - use global accelerator for higher performance</li>\n</ul>\n<h1 id=\"amazon-macie\" style=\"position:relative;\"><a href=\"#amazon-macie\" aria-label=\"amazon macie permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Amazon Macie</h1>\n<ul>\n<li>discovers, classifies and protects data in s3 buckets</li>\n<li>“data identifiers” - rules against which objects are assessed. can be “automated” i.e. detect sensitive information in pii (personally identifiable information), phi (personal health information), etc. automatically or “custom” i.e. we specify regex for macie to control</li>\n<li>“discovery jobs” which run on a schedule use these data identifiers</li>\n<li>findings can then be sent to the event bridge, etc</li>\n<li>macie can have “policy findings” (issue with security etc. of bucket) or “sensitive data findings”</li>\n</ul>","frontmatter":{"title":"AWS - Part VI"}}},"pageContext":{"id":"b7cfb081-4911-53ea-bb72-d1e18328977f"}},"staticQueryHashes":["1037383464","1617985380"]}