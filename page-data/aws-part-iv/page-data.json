{"componentChunkName":"component---src-templates-blog-js","path":"/aws-part-iv/","result":{"data":{"markdownRemark":{"fields":{"slug":"/aws-part-iv/"},"id":"b5529c35-0de0-53f1-84ed-990a0d6d3394","html":"<h1 id=\"virtualization\" style=\"position:relative;\"><a href=\"#virtualization\" aria-label=\"virtualization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Virtualization</h1>\n<ul>\n<li>operating system sits on top of underlying hardware</li>\n<li>the part of os that talks to hardware is called kernel. kernel runs in privileged mode to be able to do so</li>\n<li>the other applications that run on top of the os / users using the os, make system calls to interact with the underlying hardware via the kernel</li>\n<li>emulated virtualization - each vm has its own os called the guest os. the os runs a hypervisor. this helps run multiple guest os on top of it. the guest os has mappings, and so it’s kernel thinks it is making calls to the actual hardware, when bts, these calls are transformed by the hypervisor</li>\n<li>para virtualization - in emulated virtualization, the guest os does not know about virtualization, so the calls it makes is transformed by the hypervisor. in para virtualization, the guest os is hypervisor aware and directly makes calls to the hypervisor. so, the transformation is not needed, which speeds up the vm performance</li>\n<li>hardware assisted virtualization - initial solutions, which were software virtualization, were much slower than hardware virtualization. here the hardware helps with the transformation of calls made by the guest os</li>\n<li>sr iov - single root io virtualization - the hardware itself is broken down into multiple “logical” chunks, so they are represented as separate units to each guest os. now, there is no transformation needed altogether</li>\n<li>hypervisor used by aws is called nitro</li>\n</ul>\n<h1 id=\"ec2-architecture\" style=\"position:relative;\"><a href=\"#ec2-architecture\" aria-label=\"ec2 architecture permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>EC2 Architecture</h1>\n<ul>\n<li>ec2 instances are vms</li>\n<li>ec2 instances run on ec2 hosts</li>\n<li>ec2 hosts are az resilient</li>\n<li>ec2 <u>can have network interfaces in the same az but in different subnets</u></li>\n<li>ebs volumes should be in the same az</li>\n<li>when we restart an ec2 instance, it stays on the same host</li>\n<li>an instance is stopped then started / ec2 host is brought down for e.g. for maintenance, the ec2 instance will be scheduled to run on a different ec2 host</li>\n<li>ec2 hosts generally run the same type of instance, size of instance etc., because ultimately, the ec2 instances use the ec2 host’s resources</li>\n<li>when configuring ssh, if we only plan on using instance connect, in <a href=\"https://ip-ranges.amazonaws.com/ip-ranges.json\">https://ip-ranges.amazonaws.com/ip-ranges.json</a>, search for <code class=\"language-text\">EC2_INSTANCE_CONNECT</code> with the region our ec2 is in, and use the corresponding ip address in the inbound rule for ssh in the security group</li>\n<li>to connect to windows instances, we use rdp (remote desktop protocol) which runs on port 3389, while to connect to linux instances, we use ssh (secure shell) which runs on port 22</li>\n<li>we use a key pair to connect to an instance. this includes a private key and a public key. the public part is on the instance, and we can only connect to it if we have its corresponding private part. for linux, this is enough while for windows, we use the private key to exchange it for a local admin password, and then we use the local admin username and password to connect to the instance</li>\n<li>costs can be due to compute, memory, networking and storage</li>\n<li>an instance can be in a few states - running, stopped and terminated</li>\n<li>even when in stopped state, while we do not pay for compute, memory and networking, we do pay for storage</li>\n<li>my understanding - using systems manager, we can even connect to private ec2 instances which do not have any public ip. this offering is called sessions manager. right click on instance -> connect -> go to session manager tab -> connect. such instances are called “managed nodes”, and this feature is called “run command”</li>\n<li>hibernate - we can enable some instances for hibernation. if we do so, the contents of ram are saved into the root volume. when we start the instance again, the ram is loaded, and the processes which were running are resumed as well. this is why we are not billed when an instance is preparing to stop, but we are billed when an instance is preparing to hibernate\n<ul>\n<li>it is not possible to enable or disable hibernation on an instance after it has been launched. so, we will have to migrate to a new instance to change the hibernation capability</li>\n</ul>\n</li>\n<li>ena (elastic network adapter) for higher performance. efa (elastic fabric adapter) that has os bypass capabilities, but it <u>does not work with windows</u></li>\n</ul>\n<h1 id=\"ec2-instance-types\" style=\"position:relative;\"><a href=\"#ec2-instance-types\" aria-label=\"ec2 instance types permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>EC2 Instance Types</h1>\n<ul>\n<li>general purpose - default. equal resource ratios (ratio of for e.g. storage to compute)</li>\n<li>compute optimized - media processing, hpc (high performance computing), machine learning</li>\n<li>memory optimized - in memory caching</li>\n<li>accelerated computing - gpus for high scale parallel processing</li>\n<li>storage optimized - for better sequential and random access of data, e.g. warehousing, elasticsearch, etc</li>\n<li>instance type naming convention - r5dn.8xlarge -\n<ul>\n<li>r - instance family</li>\n<li>5 - instance generation</li>\n<li>dn - additional capabilities, e.g. d for nvme, n for network optimized, etc</li>\n<li>8xlarge - size of instance</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"storage\" style=\"position:relative;\"><a href=\"#storage\" aria-label=\"storage permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Storage</h1>\n<ul>\n<li>direct or local attached storage - directly connected to ec2 hosts. in aws, called instance store. issues - if the hardware fails, a network failure occurs, etc., the data is lost</li>\n<li>network attached storage (nas) - in aws, called ebs. decoupled from ec2 hosts</li>\n<li>ephemeral storage - temporary, not reliable, e.g. instance store</li>\n<li>persistent storage - lives beyond the lifetime of the instance, e.g. ebs</li>\n<li>block storage - a number of addressable blocks. a file system like ntfs / ext3 is created on top of it. it can be mountable and bootable i.e. where the os is stored</li>\n<li>file storage - ready-made file structure, so we cannot create a file system on it. mountable, but not bootable</li>\n<li>object storage - collection of objects, not mountable or bootable. e.g. s3 in aws</li>\n<li>performance of storage -\n<ul>\n<li>io (block size) - size of blocks of the storage</li>\n<li>iops - number of blocks we can read or write per second</li>\n<li>throughput - io * iops, so it can be mbps</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"ebs\" style=\"position:relative;\"><a href=\"#ebs\" aria-label=\"ebs permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>EBS</h1>\n<ul>\n<li>elastic block storage</li>\n<li>ebs is az resilient</li>\n<li>generally attached to one ec2 instances (some ebs volume types allow multi attach)</li>\n<li>ebs volumes can be detached from one ec2 instance and attached to another</li>\n<li>so, their lifecycle is decoupled from ec2 instances</li>\n<li>delete on termination - what happens to ebs volumes when ec2 is terminated. by default, it is set to true for root ebs volume and off for volumes attached additionally</li>\n<li>supports live configuration changes while in production which means that we can modify the volume type, volume size, and iops capacity without service interruptions</li>\n</ul>\n<h3 id=\"general-purpose\" style=\"position:relative;\"><a href=\"#general-purpose\" aria-label=\"general purpose permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>General Purpose</h3>\n<ul>\n<li>ssd storage</li>\n<li>gp2 (older) and gp3</li>\n<li>gp2 - can be 1gb - 16tb</li>\n<li>gp2 has an io credit allocation</li>\n<li>one credit = 16kb, so for all numbers below, multiply by 16kb for value in bytes</li>\n<li>for understanding performance of gp2, think of an io credit bucket. the bucket fills at a certain rate, and we can consume from it at a certain rate. we can only consume from it until the bucket becomes empty, so in general, we shouldn’t consume at a rate higher than the fill rate</li>\n<li>the maximum capacity of the gp2 bucket is 5.4 million io credits</li>\n<li>baseline performance = rate at which bucket is filled / rate at which we can consume</li>\n<li>however, sometimes we can burst i.e. consume at a rate which is higher than the baseline performance</li>\n<li>the bucket fills at a baseline performance of 3 io credits per second per gb. so, for a volume of 100gb, the bucket fills at 300 io credits per second</li>\n<li>for volumes &#x3C;= 33.33gb, the baseline performance is 100 io credits per second</li>\n<li>for volumes >= 5334gb, the baseline performance is 16000 io credits per second</li>\n<li>gp2 can burst (i.e. consume) upto 3000 io credits per second. my understanding - ofcourse this is for volumes &#x3C; 1000gb. when we burst, our buckets starts depleting, since our consumption rate is more than the fill rate. hence, i think for volumes having size greater than 1000gb, there is no concept of bursting, i.e. our max consumption rate can be the baseline performance</li>\n<li>this means if we have periodic heavy workloads, e.g. backups or some sale-season, we are not limited in our consumption rate by the baseline performance</li>\n<li>the bucket starts of at full i.e. 5.4 million io credits filled fully</li>\n<li>so, when an ebs is spun up, assuming the fill rate is 0 (it never is), we can burst at 3000 iops upto 5.4 million / 3000 i.e. for 30 minutes</li>\n<li>gp3 is the newer version of general purpose ssd</li>\n<li>it does not have the credit bucket architecture, and we can always burst at 3000 iops</li>\n<li>we can also pay extra for upto 16000 iops in gp3</li>\n<li>note - in this section, i have always considered in terms of iops, but apparently, there are limits of throughput as well i.e. mbps instead of iops (and this is not io * iops)</li>\n<li>after attaching an ebs volume to an instance, we have to create a file system on it (<code class=\"language-text\">sudo mkfs -t xfs /dev/xvdf</code>) and then mount it to some directory (<code class=\"language-text\">sudo mount /dev/xvdf /ebstest</code>). my understanding - for the mounting to sustain reboots, configure mounting inside the file <code class=\"language-text\">/etc/fstab</code> instead of using <code class=\"language-text\">mount</code></li>\n</ul>\n<h3 id=\"provisioned-iops\" style=\"position:relative;\"><a href=\"#provisioned-iops\" aria-label=\"provisioned iops permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Provisioned IOPS</h3>\n<ul>\n<li>io1 (older), io2 (newer) and io block express</li>\n<li>for io1 and io2 - 64000 iops, which is 4 times the 16000 iops of gp2 and gp3</li>\n<li>for io2 block express - 256000 iops</li>\n<li>we select the iops we want in provisioned iops</li>\n<li>we are restricted to selecting a maximum of 50 iops per gb for io1, 500 iops per gb for io2 and finally 1000 iops per gb for io2 block express</li>\n<li>we are also limited by the size and type of ec2 around throughput. these limitations are calculated after combining all storages attached to that instance. e.g. maximum of an instance for io1 is 260000 iops, so we need 4 ebs io1 attached to the instance to hit this limit</li>\n</ul>\n<h3 id=\"hdd-based\" style=\"position:relative;\"><a href=\"#hdd-based\" aria-label=\"hdd based permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>HDD Based</h3>\n<ul>\n<li>physical moving bits, making random access tougher, and meant for sequential reads</li>\n<li>also, bad for io since it is hdd, use ssd for io</li>\n<li>sc1 (cold hdd) and st1 (throughput optimized)</li>\n<li><u>cannot be used as boot volumes despite being block storages</u></li>\n</ul>\n<h1 id=\"instance-stores\" style=\"position:relative;\"><a href=\"#instance-stores\" aria-label=\"instance stores permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Instance Stores</h1>\n<ul>\n<li>block storage</li>\n<li>local instead of network based - so it has much higher performance than ec2</li>\n<li>each ec2 host has its own instance store which the ec2 instances can use</li>\n<li>so, it is ephemeral, i.e. when the ec2 instances move between hosts, they are lost</li>\n<li>we have to attach them at launch time, not later on</li>\n<li>the price of instance store is included in the price of ec2, no extra cost</li>\n<li>use if performance needs are higher than 256000 (io2 block express) or 260000 (multiple io2)</li>\n</ul>\n<h1 id=\"ebs-snapshots\" style=\"position:relative;\"><a href=\"#ebs-snapshots\" aria-label=\"ebs snapshots permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>EBS Snapshots</h1>\n<ul>\n<li>recall ebs is az resilient</li>\n<li>we can create snapshots (backups) that uses s3 bts. thus, ebs snapshots are regionally resilient</li>\n<li>we can also copy snapshots across regions, thus making our data globally resilient</li>\n<li>snapshots are incremental - so, the first snapshot contains a full copy of the used data i.e. if we use a 40gb volume which contains 10gb of data, the snapshot is of 10gb. all future snapshots are incremental, thus making them much quicker</li>\n<li>volumes can be restored from snapshots - so, we can use snapshots to move data between azs / regions</li>\n<li>snapshots are restored lazily i.e. after we restore a volume from a snapshot, if we request for data from this volume, it would first fetch it from s3 and then return the data. this can cause throughput bottlenecks, since reading directly from ebs is much faster. to avoid this, we can use tools like dd (disk duplicator) on linux, which forces a read on every block. a new aws option called fsr (fast snapshot restore) is available on a snapshot to achieve something similar. <u>it is available for a per snapshot az combination</u></li>\n<li>amazon data lifecycle manager (dlm) to automate creation, retention and deletion of outdated snapshots</li>\n</ul>\n<h1 id=\"ebs-encryption\" style=\"position:relative;\"><a href=\"#ebs-encryption\" aria-label=\"ebs encryption permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>EBS Encryption</h1>\n<ul>\n<li>aws managed key (aws/ebs) or customer managed key can be used</li>\n<li>a dek is generated and its encrypted form is stored alongside the volume</li>\n<li>encryption / decryption is handled by ec2 host - the host calls kms to get the decrypted key and encrypt / decrypt data before reading from / writing to ebs\n<ul>\n<li>this way, both data at rest and data in transit between instance to volume is secure</li>\n</ul>\n</li>\n<li>aws accounts can be configured to encrypt ebs volumes by default (<u>note - account level default</u>)</li>\n<li>a unique dek is used for every volume</li>\n<li>the same dek is used for volumes and snapshots created from each other</li>\n<li><u>we cannot remove the encryption from a volume or a snapshot - the only way might be to clone the data by using the volume via an ec2, since the ec2 only sees the unencrypted version as encryption / decryption is handled by the ec2 host</u></li>\n<li>snapshots of encrypted volumes are encrypted / volumes created from encrypted snapshots are encrypted</li>\n</ul>\n<h1 id=\"ec2-networking\" style=\"position:relative;\"><a href=\"#ec2-networking\" aria-label=\"ec2 networking permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>EC2 Networking</h1>\n<ul>\n<li>ec2s have a primary eni (network interface)</li>\n<li>we can however attach multiple secondary interfaces. these secondary enis can be in different subnets, however they have to be on the same az</li>\n<li>network interfaces have -\n<ul>\n<li>mac address - use case - software licensing</li>\n<li>primary ipv4 ip address, from the cidr range of the subnet it is in. a corresponding dns of ip-w-x-y-z.ec2.internal can be used inside the vpc. the ip does not change on stopping and starting the instance</li>\n<li>0 or more secondary private ipv4 addresses</li>\n<li>0 or 1 public ipv4 ip address - can change when stopped and started (not when restarted). similarly, a public dns of the format ec2-w-x-y-z.compute-1.amazonaws.com can be used. this dns resolves to the primary ipv4 inside the vpc, public ipv4 outside vpc (recall how public ip address is actually on the igw)</li>\n<li>1 elastic ip per private ipv4 address (is this because of igw nat works?!). elastic ip = public ip, but non changing, since it is allocated to our account. note how it is not limited to just one like normal public ip4 addresses. when we attach it to the primary eni, the non-elastic public ip is replaced by this public ip</li>\n<li>0 or more ipv6 (no concept of public or private here)</li>\n<li>security groups - so, the traffic to ip addresses of an eni are affected by the security group on it</li>\n<li>source / destination checks can be enabled and disabled - recall how this is needed when using self-managed nat instance instead of aws-managed nat gateway</li>\n</ul>\n</li>\n<li>secondary vs primary eni - secondary enis can be attached / detached</li>\n<li>aws prep question - if we have the requirement that a dns points to a private ip, and we have a single instance running, for failover, we can use a secondary eni and use its private ip for the dns pointing. this way, in a failover, the secondary eni can be attached to a new instance, and no dns changes would be needed</li>\n<li>an ec2 instance can have multiple enis on different subnets, e.g. one for data and one for management. while we can have multiple ip addresses on a single eni as an alternative, the advantage of multiple eni is that each eni can have its own security group</li>\n</ul>\n<h1 id=\"ami\" style=\"position:relative;\"><a href=\"#ami\" aria-label=\"ami permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>AMI</h1>\n<ul>\n<li>amazon machine images</li>\n<li>used to launch ec2 instances</li>\n<li>they can be aws provided, community provided (e.g. centos, ubuntu, etc.), or even marketplace (commercial software for licensing etc.)</li>\n<li>amis are scoped to a region</li>\n<li>permissions of amis - only our account can use it (default), public (any account) or specific accounts</li>\n<li>we can also do the reverse i.e. create an ami from an ec2</li>\n<li>when we create an ami, ebs snapshots are taken and referenced inside the ami using the block device mapping (e.g. /dev/xvda etc.). i think this is for both boot and data volumes</li>\n<li>when we launch an instance from an ami (can be in a different az but should be in the same region), the snapshots and block device mappings are used to create new volumes</li>\n<li>ami baking - after configuring an ec2, creating an ami out of it to reuse it easily</li>\n<li>amis cannot be edited - launch an instance from it configure it and create a new ami</li>\n<li>amis can be copied across regions (snapshots are copied as well in this process)</li>\n<li>when copying ami from one region to another, non-encrypted snapshots can be encrypted / <u>encrypted snapshots will have to be re-encrypted</u> since kms keys are specific to a region in general</li>\n</ul>\n<h1 id=\"ec2-purchase-options--ec2-launch-types\" style=\"position:relative;\"><a href=\"#ec2-purchase-options--ec2-launch-types\" aria-label=\"ec2 purchase options  ec2 launch types permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>EC2 Purchase Options / EC2 Launch Types</h1>\n<ul>\n<li>on demand - the default. instances of different size from different customers, run on the same hardware. instances are charged while in running state, while storage can cost regardless. there is no capacity reservations, i.e. if there is an aws outage, aws will give preference to uptime of reserved instances over our on demand instances, so consider this for critical applications. so it is good for short term unknown loads. there are no upfront costs therefore no discounts</li>\n<li>spot - ec2 hosts have some spare capacity left after scheduling on demand and reserved instances. aws sells that as well at a discounted rate. customers set a maximum price they are wiling to pay, but they only pay the current spot price. when the spot price goes above the maximum price they are willing to pay, they lose their compute. so, these are good for running non-critical workloads\n<ul>\n<li>aws prep question - use this option for workloads that can <u>tolerate interruption</u></li>\n</ul>\n</li>\n<li>reserved - reduces costs by a lot. we still pay for unused reservation, so choose wisely. priority used by aws - reserved > on demand > spot. reservations can have a partial effect as well - we use on demand or something else for the remaining bits. there are two options - 1 year vs 3 years. there are also different payment methods - no upfront, partial upfront or all upfront. it has some types like scheduled - for certain fixed time windows, e.g. analysis runs every friday. reserved instances can be in an az or region and not across multiple regions. it can be of two types -\n<ul>\n<li>standard reserved instance - cheaper, unused can be sold in marketplace <u>unlike convertible</u></li>\n<li>convertible reserved instance - can be exchanged with another convertible reserved instance of a different instance type, etc. more expensive and unused cannot be sold in the marketplace</li>\n</ul>\n</li>\n<li>dedicated hosts - an ec2 host allocated to us entirely. ec2 hosts are for a particular type of instance. <u>we pay for ec2 hosts, and not for instances</u>. use case - software licensing based on hardware. host affinity - certain ec2 instances are only scheduled on certain ec2 hosts. only our instances can run on our hosts. an important feature called <u>ram</u> (resource access manager). the idea is that multiple accounts can schedule their instances on the same dedicated host, and they only gain visibility into the instance they own</li>\n<li>dedicated instances - the host is dedicated to us and instances from other accounts do not share this host, but we do not have low level access to the ec2 host. use case - compliance requires not sharing hardware</li>\n</ul>\n<h1 id=\"placement-groups\" style=\"position:relative;\"><a href=\"#placement-groups\" aria-label=\"placement groups permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Placement Groups</h1>\n<ul>\n<li>ec2 hosts used by our ec2 instances is determined by aws by default</li>\n<li>we can influence this using placement groups</li>\n<li>i think the idea is to launch instances inside the placement groups</li>\n<li>it is of three types - cluster, spread and partition</li>\n<li>cluster - close to each other. it has the highest performance for intercommunication between the instances. idea is each of these instances are placed on the same rack, and sometimes on the same ec2 host as well. <u>it cannot span across multiple azs</u>. vpc peering is supported but can slow down performance. it is recommended to launch all instances at the same time and launch instances of the same type so that their proximity is as close to each other as possible</li>\n<li>spread - separated i.e. span multiple azs. located in separated isolated racks. this helps with high resiliency. there is a <u>hard limit of 7 instances per az since they need to be on different racks (not just hosts)</u></li>\n<li>partition - just like spread, but think in terms of partitions instead of instances. so, can span multiple azs, and a max of 7 partitions per az. each partition has multiple ec2 instances, so we are now not limited by the 7 instance limit in case of spread. e.g. we have 14 instances per az, each partition can house 2 instances</li>\n</ul>\n<h1 id=\"ec2-status-checks\" style=\"position:relative;\"><a href=\"#ec2-status-checks\" aria-label=\"ec2 status checks permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>EC2 Status Checks</h1>\n<ul>\n<li>two status checks - system status checks and instance status checks</li>\n<li>auto recovery - moves instance to a new ec2 host if status checks fail</li>\n</ul>\n<h1 id=\"ec2-termination-protection\" style=\"position:relative;\"><a href=\"#ec2-termination-protection\" aria-label=\"ec2 termination protection permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>EC2 Termination Protection</h1>\n<ul>\n<li>we can enable termination protection on an instance</li>\n<li>this prevents accidental terminations</li>\n<li>role separation - there are now two separate permissions for terminating an instance vs disabling this check on the instance to allow it to be terminated</li>\n</ul>\n<h1 id=\"ec2-shutdown\" style=\"position:relative;\"><a href=\"#ec2-shutdown\" aria-label=\"ec2 shutdown permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>EC2 Shutdown</h1>\n<ul>\n<li>there are ways of initiating a shutdown from inside the instances, e.g. using <code class=\"language-text\">shutdown</code> in linux</li>\n<li>this will stop the instance by default</li>\n<li>we can configure aws to either stop or terminate the instance when this command is issued</li>\n</ul>\n<h1 id=\"ec2-instance-metadata\" style=\"position:relative;\"><a href=\"#ec2-instance-metadata\" aria-label=\"ec2 instance metadata permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>EC2 Instance Metadata</h1>\n<ul>\n<li>general data about the instance available from inside the instance</li>\n<li>otherwise, the other option would have been to use iam roles and grant permissions on get api calls</li>\n<li>endpoint - <a href=\"http://169.254.169.254\">http://169.254.169.254</a></li>\n<li>it has information like public ipv4 (recall how bts, it is on igw), authentication information for temporary credentials, access to user data script, etc</li>\n<li>it has no authentication by default - e.g. anyone with ssh access can call this endpoint</li>\n<li>my understanding of implications - e.g. when we run <code class=\"language-text\">ifconfig</code>, we can see the private ip address and ipv6 address, since they are configured on the eni. however, the public ipv4 is not visible. this feature gives us easy access to the public ipv4 from inside the instance as well\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-shell line-numbers\"><code class=\"language-shell\"><span class=\"token function\">curl</span> http://169.254.169.254/latest/meta-data/public-ipv4</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span></span></pre></div>\n</li>\n</ul>\n<h1 id=\"user-data\" style=\"position:relative;\"><a href=\"#user-data\" aria-label=\"user data permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>User Data</h1>\n<ul>\n<li>bootstrapping - perform some configuration</li>\n<li>allow some build automation to configure and install software etc</li>\n<li>accessed by ec2 via metadata api at <a href=\"http://169.254.169.254/latest/user-data\">http://169.254.169.254/latest/user-data</a></li>\n<li>executed only once at launch time, not on restart etc</li>\n<li>so, we can change the user data script, and the change would even be reflected on the instance on stopping and restarting it, but the script itself would not be re-executed</li>\n<li>it is run as the root user</li>\n<li>ec2 does not validate this script i.e. if the script fails, any status checks etc. would not fail (unless our script really screwed up something)</li>\n<li>so, user data script is not a secure feature by itself</li>\n<li>user data is not secure for credentials</li>\n<li>limit is 16kb, so do a download inside the user data if our script larger than that</li>\n<li>we should reduce bootstrapping time using ami baking where possible</li>\n<li>advantage is bootstrapping is dynamic unlike ami baking</li>\n<li>note - user data needs to be base 64 encoded - cloudformation has a function for this. ui has a checkbox “script is already base64 encoded” which we can leave unchecked</li>\n</ul>\n<h1 id=\"ec2-instance-roles-and-profile\" style=\"position:relative;\"><a href=\"#ec2-instance-roles-and-profile\" aria-label=\"ec2 instance roles and profile permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>EC2 Instance Roles and Profile</h1>\n<ul>\n<li>we grant aws services permissions to other aws services using roles</li>\n<li>instance profile - a wrapper around the iam role which allows the iam role credentials to be used by the ec2</li>\n<li>we attach the instance profile (not roles) to the ec2</li>\n<li>instance metadata should be used to deliver the credentials to the app. internally, sts and ec2 communicate with each other to ensure that the credentials are always valid and automatically renewed</li>\n<li>aws sdks use instance metadata automatically\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-shell line-numbers\"><code class=\"language-shell\"><span class=\"token function\">curl</span> <span class=\"token number\">169.254</span>.169.254/latest/meta-data/iam/security-credentials/<span class=\"token operator\">&lt;&lt;</span>role-name<span class=\"token operator\">>></span></code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span></span></pre></div>\n</li>\n</ul>\n<h1 id=\"ssm-parameter-store\" style=\"position:relative;\"><a href=\"#ssm-parameter-store\" aria-label=\"ssm parameter store permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>SSM Parameter Store</h1>\n<ul>\n<li>ssm - systems manager</li>\n<li>we can create parameters which have names and values</li>\n<li>it has integrations with cloudformation</li>\n<li>supports <u>hierarchy</u> and <u>versioning</u></li>\n<li>parameters have three different types - strings, string lists and secure strings</li>\n<li>secure strings use kms underneath (recall when using kms, we need permissions for the service itself and kms)</li>\n<li>public parameters - parameters maintained by aws, e.g. ami ids</li>\n<li>e.g. retrieve all parameters under a root after decrypting using kms\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-shell line-numbers\"><code class=\"language-shell\">aws ssm get-parameters-by-path <span class=\"token parameter variable\">--path</span> /my-cat-app/ --with-decryption</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span></span></pre></div>\nremember this requires kms permissions as well</li>\n</ul>\n<h1 id=\"efs\" style=\"position:relative;\"><a href=\"#efs\" aria-label=\"efs permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>EFS</h1>\n<ul>\n<li>elastic file storage</li>\n<li>shared storage called nfs (network file system) v4</li>\n<li>they can be mounted on multiple linux ec2. note - only linux</li>\n<li>it is file storage (and not block) so not bootable</li>\n<li>efs is isolated in the vpc and used via mount targets</li>\n<li>for high availability, a mount target should be created in each az we want</li>\n<li>we <u>can access efs outside a vpc as well</u> via hybrid networks like vpn and direct connect</li>\n<li>there are <u>two performance modes</u> - general purpose and max io</li>\n<li>there are <u>two throughput modes</u> - bursting (our throughput scales with the size of the file system we use) and provisioned (we can specify the throughput independent of the size)</li>\n<li>there are <u>two storage classes</u> - standard and ia (infrequently accessed), and we can use lifecycle policies as well for automatic transition like in s3</li>\n</ul>\n<h1 id=\"aws-backup\" style=\"position:relative;\"><a href=\"#aws-backup\" aria-label=\"aws backup permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>AWS Backup</h1>\n<ul>\n<li>data backup and restore</li>\n<li>can work <u>across multiple accounts and regions - uses control tower and organizations</u></li>\n<li>so, helps manage it for multiple services like rds, dynamodb, etc. from one place</li>\n<li>backup plans - how frequently backups should occur. we can specify a cron</li>\n<li>we can also enable <u>continuous backups</u> for the products that support it, so we can use point in time restore - recall how 35 day rolling window is present in some services</li>\n<li>lifecycle for moving to cold storage</li>\n<li>backup resources - what gets backed up</li>\n<li>vault - the place where the backups are put (destination). considerations around kms come as well</li>\n<li>vault lock - to implement worm(write once read many) model, a 72-hour cool down period post which no one (not even aws) can delete anything from here</li>\n<li>on demand backups - like manual backups</li>\n</ul>\n<h1 id=\"elb\" style=\"position:relative;\"><a href=\"#elb\" aria-label=\"elb permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>ELB</h1>\n<ul>\n<li>elb - elastic load balancer</li>\n<li>load balancers - accept connections from customers and distributes it between the compute behind it, thus hiding it away from the users</li>\n<li>we pick the subnets for a load balancer, and the load balancer creates a node in each of these subnets</li>\n<li>a dns is assigned to the elb, and when a request comes to this dns, it resolves to one of these load balancer nodes present in each subnet we selected</li>\n<li>load balancers can be public or private - if we chose public, the nodes discussed above get both public and private addresses, but if we chose private, the nodes only get private addresses</li>\n<li>the load balancer nodes are configured using <u>listeners</u> - they accept traffic on a protocol and a port, and communicate with <u>targets</u> on a protocol and port</li>\n<li>the load balancer nodes consume 8 ip addresses in the subnet they are deployed to - so a minimum of /28 is needed, which gives us 16 - 5 = 11 ip addresses</li>\n<li>cross zone load balancing - e.g. subnet a has 4 instances, and subnet b <em>in a different az</em> has 1 instance. so, the load balancer node in each subnet gets 50% of the total traffic, and so the instances in subnet a get 12.5% of the traffic, while the only instance in subnet b gets 50% of the traffic, since a load balancer node sends the traffic it receives to the instances in its own az. with cross zone load balancing, the load balancer node in an az can send traffic to the instances in another az, so each of the total 5 instances get 20% of the traffic</li>\n<li>with sni (server name indication) we can have multiple ssl certificates loaded on the same alb. this way, cats.io and dogs.io can both use the same alb, and each listener can be associated with one ssl certificate and target group - so one listener with ssl certificate for cats.io and the target being an asg serving traffic for cats, and one listener with ssl certificate for dogs.io and the target being an asg serving traffic for dogs</li>\n<li>there are two versions - v1 and v2. we should avoid v1 and migrate to v2</li>\n<li>access logging - optional feature, disabled by default. can be enabled and sent to s3</li>\n<li>clb - classic load balancers\n<ul>\n<li>only elb that is v1</li>\n<li>they can load balance http, https and lower level protocols. but, they cannot load balance based on http protocol features, since they are not truly layer 7 like alb</li>\n<li>classic load balancers also only support one ssl certificate per load balancer</li>\n</ul>\n</li>\n<li>alb - application load balancer -\n<ul>\n<li>they are layer 7 devices, and works with <u>http, https and web socket</u> but not other layer 7 protocols like smtp, ssh, gaming protocols, etc. <u>it can also not work directly with lower protocols like tcp, tls or udp</u></li>\n<li><u>also works with grpc</u></li>\n<li>cross zone load balancing is enabled by default</li>\n<li>it can inspect http features like cookies, headers, etc</li>\n<li><u>unbroken ssl not supported</u> i.e. ssl terminates at the load balancer. so, for ssl, the ssl certificates should be loaded onto the load balancer</li>\n<li>slower than nlb</li>\n<li>can evaluate application health using layer 7 health checks</li>\n<li><u>rules</u> direct connections which arrive at a <u>listener</u>. they are processed in the order of priority, and there is a default catch all rule as well</li>\n<li>rules can be things like http headers, ip addresses, http methods, ips, etc</li>\n<li>rules have <u>actions</u>, so they can redirect, return fixed responses, forward traffic to <u>target groups</u></li>\n<li>rules can also authenticate using cognito etc</li>\n<li>we can use ssl certificates with listeners, and since layer 7, we can redirect traffic based on host headers. so, <a href=\"https://cats.io\">https://cats.io</a> and <a href=\"https://dogs.io\">https://dogs.io</a> can use the same alb</li>\n</ul>\n</li>\n<li>nlb - network load balancer -\n<ul>\n<li>tcp, tls (secure tcp) and udp</li>\n<li>do not understand http / https, cookies, headers, etc</li>\n<li>much faster than alb since they do not perform computations of layer 7</li>\n<li>so, they can also be used for protocols of layer 7 which are not http, https and web sockets</li>\n<li>health checks cannot be detailed, and only by icmp / tcp</li>\n<li>also allows for unbroken encryption</li>\n<li><u>uses elastic ip i.e. static ip unlike alb</u>. so, this can be whitelisted in for e.g. firewalls of companies if needed. this allows for byoip (bring your own ip) model</li>\n</ul>\n</li>\n<li>gwlb - gateway load balancer -\n<ul>\n<li>sometimes, we need security checks to be run for both inbound (before entering application) and outbound (after leaving application) traffic</li>\n<li>this helps us run 3rd party firewalls, intrusion detection systems, etc</li>\n<li>the security appliances are basically ec2 running the security software</li>\n<li>the advantage it works with horizontally scaled backend instances and security appliances</li>\n<li>it uses the geneve protocol between the gwlb and security appliances - this helps it to not change the source or destination ips, which is important for the security appliances inspection</li>\n<li>it also manages flow stickiness - one security appliance manages both inbound and outbound for the same flow, which is another important feature for security appliances</li>\n</ul>\n</li>\n<li>ssl can be handled in three ways by load balancers -\n<ul>\n<li>bridging - default of application load balancer. ssl between clients and load balancer, terminated at the load balancer. so, the certificate needs to be stored at the load balancer. now, since the traffic is decrypted, it can now make http based decisions. then, a secure connection happens between the load balancer and the instances. so, a certificate needs to be loaded on the instances as well. so, the compute performs cryptographic operations as well</li>\n<li>pass through - the load balancer just passes the connection from the client to the backend. so, the load balancer does not need the certificate. e.g. network load balancer uses this</li>\n<li>offload - the clients and load balancer use https, while load balancer and instances use http</li>\n</ul>\n</li>\n<li>stickiness - for sessions, if state is handled at an instance level. load balancer generates a cookie called <code class=\"language-text\">AWSALB</code> and sends it to the client. we can specify the duration of this cookie (between 1 second - 7 days). using this, the connection is forwarded to the same instance. note - stickiness is enabled on a target group (recall listeners have rules, which specify actions to for e.g. route traffic to target groups). also, stickiness in alb is intelligent enough to not route to unhealthy instances (recall load balancer has health checks)</li>\n</ul>\n<h1 id=\"launch-configurations-and-launch-templates\" style=\"position:relative;\"><a href=\"#launch-configurations-and-launch-templates\" aria-label=\"launch configurations and launch templates permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Launch Configurations and Launch Templates</h1>\n<ul>\n<li>allow us to provide the configuration around ec2 like networking and security group, iam role to use, instance type, storage, the key pair to use, etc</li>\n<li>launch template is the newer of the two</li>\n<li>they are not editable once created (launch template allows versions)</li>\n<li>autoscaling groups use them</li>\n<li>but, <u>launch templates can be used to launch standalone instances as well unlike launch configurations</u></li>\n</ul>\n<h1 id=\"auto-scaling-groups\" style=\"position:relative;\"><a href=\"#auto-scaling-groups\" aria-label=\"auto scaling groups permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Auto Scaling Groups</h1>\n<ul>\n<li>helps scale ec2 automatically</li>\n<li>use launch configuration or a specific version of launch template</li>\n<li>we can set the minimum, desired and maximum instances</li>\n<li>the asg tries to make the number of running instances equal to the desired</li>\n<li>generally, it tries to maintain the number of instances in an az the same</li>\n<li>scaling policies - update the desired capacity based on various factors. it has the following types -\n<ul>\n<li>scheduled scaling - adjust the desired capacity based on schedules. useful when for e.g., we already know that a sale season is coming up over the weekend</li>\n<li>dynamic scaling -\n<ul>\n<li>simple - when cpu metric is above 50%, add two instances, when metric is below 50%, remove two instances. it can use metrics like sqs queue length etc. as well. it can also act on metrics which require cloudwatch agent to be installed</li>\n<li>stepped scaling - if cpu is more than 50%, add one instance but if cpu usage is more than 80%, add two new instances. usually preferred over simple since it has more granularity around metrics</li>\n<li>target tracking - we specify desired cpu usage, and that target is maintained. my understanding - with this type, fewer metrics are supported</li>\n<li>scale based on number of messages in the sqs queue</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>cooldown period - in seconds, wait for this period before adding or removing newer instances to allow all for all the metrics to be stable i.e. before a new scaling action (launching new or terminating old instances), let the previous scaling action to be complete</li>\n<li>note - default cooldown period is 300 seconds</li>\n<li>self-healing - asg monitors the health of instances - status checks. it provisions new instances and removes the unhealthy ones automatically</li>\n<li>asg can integrate with load balancer - the target group can be an asg i.e. as the instances are added to or removed from the asg, they are added to or removed from the target group</li>\n<li>now recall how the self-healing feature uses status checks. we can configure asg to instead use the application load balancer health checks, which are http checks and are therefore much more feature rich</li>\n<li>grace period - the time the instance is given for bootstrapping before the health check is started</li>\n<li>we have configurations around suspending adding / removing of instances, putting all autoscaling actions to standby, switching off health checks, toggling rebalancing number of instances across az, etc</li>\n<li>autoscaling groups are free - only costs are for the underlying resources</li>\n<li>lifecycle hooks - custom actions that happen when instances launch / terminate, i.e. when an asg scales out or scales in, the process pauses, and either the process that is invoked calls the “complete lifecycle action” api, or the timeout occurs which can either continue or abandon the autoscaling action. this way, we can perform some processing before the scaling happens and even change the decision of the asg. we can also use lifecycle hooks to send messages to event bridge / sns</li>\n<li>the order in which asg terminates instances during a scale in - an instance present in the az with the most instances / instance that uses the oldest launch configuration / instance closer to the billing hour to save costs</li>\n<li>aws prep question - if requirement is to run minimum two instances, and we set minimum to 2 instances in an asg, and if an az outage happens, we would have only one running instance temporarily while the asg brings back its running instances to two. this temporary period of only one running instance might not be desirable</li>\n</ul>\n<h1 id=\"fsx\" style=\"position:relative;\"><a href=\"#fsx\" aria-label=\"fsx permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>FSx</h1>\n<h3 id=\"fsx-for-windows\" style=\"position:relative;\"><a href=\"#fsx-for-windows\" aria-label=\"fsx for windows permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>FSx for Windows</h3>\n<ul>\n<li>managed windows file servers</li>\n<li>integrates with directory service <u>and even on prem active directory</u></li>\n<li>can work in single or multi az mode</li>\n<li>nfs means linux i.e. efs, smb means windows i.e. fsx</li>\n<li>can be accessed with vpn, direct connect, peering connections, etc</li>\n<li>aws prep question - nothing to do with on prem like storage gateway - so, in storage gateway, we continue using on prem solution, but if the question is about migrating, we chose something like efs / fsx</li>\n</ul>\n<h3 id=\"fsx-for-lustre\" style=\"position:relative;\"><a href=\"#fsx-for-lustre\" aria-label=\"fsx for lustre permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>FSx for Lustre</h3>\n<ul>\n<li>lustre is used by hpc (high performance computing) linux clients. this is what fsx for lustre is used for</li>\n<li>for single az only (even in persistent mode discussed below) because of high performance needs</li>\n<li>two modes -\n<ul>\n<li>scratch - for very fast and short term temporary workloads</li>\n<li>persistent - longer term storage, higher availability, self-healing</li>\n</ul>\n</li>\n<li>this lustre file system can synchronize data to / from s3, but it is separate from s3</li>\n<li>backups can be manual or automatic to s3 like other products</li>\n</ul>","frontmatter":{"title":"AWS - Part IV"}}},"pageContext":{"id":"b5529c35-0de0-53f1-84ed-990a0d6d3394"}},"staticQueryHashes":["1037383464","1617985380"]}