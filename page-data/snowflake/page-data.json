{"componentChunkName":"component---src-templates-blog-js","path":"/snowflake/","result":{"data":{"markdownRemark":{"fields":{"slug":"/snowflake/"},"id":"3456f466-645c-56db-a136-b71799b35566","html":"<h1 id=\"basics\" style=\"position:relative;\"><a href=\"#basics\" aria-label=\"basics permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Basics</h1>\n<ul>\n<li>we write our queries in worksheets - it has a rich editor with autocomplete etc</li>\n<li>we can set a default database, schema, etc. in worksheet context to avoid typing the fully classified name</li>\n<li>there are two kinds of console - the classic and the newer one to toggle between</li>\n<li>we can use the ui or sql to perform provisioning of resources and to issue other commands</li>\n<li>data is stored by snowflake using hybrid columnar storage in the form of blobs for e.g. in s3 instead of rows</li>\n<li>it is a cloud based saas offering, so we do not need to care about the underlying hardware</li>\n<li>it uses virtual warehouses i.e. compute to perform queries which utilizes mpp (massive parallel processing) behind the scenes to make processing of queries efficient</li>\n<li>according to my understanding, clusters and virtual warehouses can be used interchangeably</li>\n<li>snowflake has a lot of functionality out of the box, e.g. we can easily copy the data from a csv stored in s3</li>\n<li>my understanding - when copying data from s3 to snowflake, there might be a cost associated with this transfer of data. however, it is free if the aws bucket and snowflake warehouses are in the same region</li>\n</ul>\n<h1 id=\"loading-data\" style=\"position:relative;\"><a href=\"#loading-data\" aria-label=\"loading data permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Loading Data</h1>\n<ul>\n<li>we use <strong>bulk loading</strong> i.e. we load data from stages and use the compute of our warehouses</li>\n<li><strong>stages</strong> - the place where we load data from. this can be external, where we store the data in a cloud provider based block or object storage like s3, or internal, where we source data from an on premise server\n<div class=\"gatsby-highlight\" data-language=\"snowflake\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-snowflake line-numbers\"><code class=\"language-snowflake\">create or replace stage db.schema.aws_stage\n  url=&#39;s3://snowflake_bucket&#39;\n  credentials=(aws_key_id = &#39;id&#39; aws_secret_key = &#39;key&#39;);\n\ndescribe stage db.scehma.aws_stage;\nlist @db.schema.aws_stage; -- list the files in the stage\n\ncopy into db.schema.orders\n  from @db.schema.aws_stage\n  file_format = (type = csv field_delimiter = &#39;,&#39; skip_header = 1)\n  files = (&#39;order_details.csv&#39;);</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n</li>\n<li>we can also pass multiple files, using the <code class=\"language-text\">pattern</code> argument, e.g. pattern = ‘order_details_.*.csv’</li>\n<li>we can select specific columns in the stage, e.g. <code class=\"language-text\">stage.$1</code> for the first column. this way we can only insert data from some specific columns in stage to some other specific columns in the table, and apply transformations using functions in snowflake on them\n<div class=\"gatsby-highlight\" data-language=\"snowflake\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-snowflake line-numbers\"><code class=\"language-snowflake\">select aws_s3.$1, date_from_parts(aws_s3.$4, aws_s3.$3, aws_s3.$2)\n  from @aws_s3 (\n    file_format =&gt; aws_s3,\n    pattern =&gt; &#39;OrderDetails.*.csv&#39;\n  ) aws_s3;</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n</li>\n<li>imagine some rows in the csv have errors, e.g. amount is occasionally in words instead of being a number</li>\n<li>therefore, we can also specify an <code class=\"language-text\">on_error</code> attribute, which can take values like <code class=\"language-text\">continue</code>, <code class=\"language-text\">abort_statement</code> or <code class=\"language-text\">skip_file</code>, and all are self-explanatory</li>\n<li>for skip file, we can specify additional parameters like <code class=\"language-text\">skip_file_2</code> (skip file if 2 errors are found) or <code class=\"language-text\">skip_file_2.5%</code> if 2.5% of the rows had errors in them</li>\n<li>we can also store file formats in snowflake so that we can reuse them everywhere <code class=\"language-text\">create or replace file format db.schema.my_file_format;</code></li>\n<li>to modify them, we can use <code class=\"language-text\">alter file format db.schema.my_file_format set field_delimiter = '|'</code></li>\n<li>we can then use them like this - <code class=\"language-text\">file_format = (format_name = db.schema.my_file_format)</code>. this allows us to perform some overrides like <code class=\"language-text\">file_format = (format_name = ... skip_header = 1)</code></li>\n<li>my understanding - when we use a <code class=\"language-text\">copy into</code> command to copy the data, some metadata is stored behind the scenes by snowflake. so, we should use <code class=\"language-text\">truncate table table_name</code> to clear it instead of <code class=\"language-text\">delete from</code></li>\n<li>when using the validation mode, data is not copied into the tables, we just get back the errors after the data in the stage is validated</li>\n<li>so, we can set <code class=\"language-text\">validation_mode = return_errors</code> to get all the errors that are encountered while validating. there are other options for validation mode, which can be found <a href=\"https://docs.snowflake.com/en/sql-reference/sql/copy-into-table.html#:~:text=VALIDATION_MODE%20%3D%20RETURN_n_ROWS%20%7C%20RETURN_ERRORS%20%7C%20RETURN_ALL_ERRORS\">here</a></li>\n<li>getting records which failed the copy into command with validation mode - <code class=\"language-text\">select * from table(result_scan(last_query_id()))</code></li>\n<li>getting records which failed the copy into command without the validation mode - <code class=\"language-text\">select * from table(validate(orders, job_id => '_last'))</code></li>\n<li>we can store the rejected records from here into a new table for bookkeeping purpose. the rejected records are available in the column <code class=\"language-text\">rejected_records</code> as coma separated values -\n<div class=\"gatsby-highlight\" data-language=\"snowflake\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-snowflake line-numbers\"><code class=\"language-snowflake\">create or replace table rejected_values as\n  select\n      split_part(rejected_record, &#39;,&#39;, 1) as id,\n      split_part(rejected_record, &#39;,&#39;, 2) as name\n  from table(result_scan(last_query_id()));</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n</li>\n<li>we can specify a <code class=\"language-text\">size_limit</code> to specify a limit on how many bytes of data should be loaded</li>\n<li>however, note that if we have 3 files of 20mb, and our size limit is 30mb, 2 files will be loaded completely and then the third file will be skipped</li>\n<li>if target column are strings, and we do not want the source column lengths to exceed it, we can set the <code class=\"language-text\">truncatecolumns</code> option to true to abandon the extra characters. its value is ofcourse false by default</li>\n<li>since snowflake stores metadata behind the scenes, when we try to rerun the <code class=\"language-text\">copy into</code> command, no new data from the file is added to the tables. we can change this by providing <code class=\"language-text\">force = true</code>, but this would ofcourse mean duplicated data</li>\n<li>we can use the table <code class=\"language-text\">information_schema.load_history</code> present in every database or the table <code class=\"language-text\">account_history.load_history</code> in the <code class=\"language-text\">snowflake</code> database for a more global view of the copy commands that were run. we can then apply filtering and ordering clause on them for reporting purposes</li>\n</ul>\n<h1 id=\"unstructured-data\" style=\"position:relative;\"><a href=\"#unstructured-data\" aria-label=\"unstructured data permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Unstructured Data</h1>\n<ul>\n<li>snowflake can handle unstructured / semi structured data</li>\n<li>we load data from the stages into a table which has a column of type of variant. we then use snowflake functions to flatten and load the data into the appropriate tables\n<div class=\"gatsby-highlight\" data-language=\"snowflake\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-snowflake line-numbers\"><code class=\"language-snowflake\">create or replace table db.schema.json_raw (raw variant);\n\ncopy into db.schema.json_raw\n  from @db.schema.stage\n  file_format = (type = json)\n  files = (&#39;data.json&#39;);</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\nnote - according to me, the column name of this new table doesn’t matter, i.e. even if i named it foo, snowflake will automatically load it into this column of type variant</li>\n<li>assume the json looks like this -\n<div class=\"gatsby-highlight\" data-language=\"json\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-json line-numbers\"><code class=\"language-json\"><span class=\"token punctuation\">{</span>\n  <span class=\"token property\">\"Skills\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span>\n    <span class=\"token string\">\"PS3\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"Vlookup\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"Go\"</span>\n  <span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n  <span class=\"token property\">\"age\"</span><span class=\"token operator\">:</span> <span class=\"token number\">43</span><span class=\"token punctuation\">,</span>\n  <span class=\"token property\">\"department\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Business Development\"</span><span class=\"token punctuation\">,</span>\n  <span class=\"token property\">\"first_name\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Flossy\"</span><span class=\"token punctuation\">,</span>\n  <span class=\"token property\">\"id\"</span><span class=\"token operator\">:</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span>\n  <span class=\"token property\">\"last_name\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Fasson\"</span>\n<span class=\"token punctuation\">}</span></code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\nwe can then parse it for e.g. as follows to get skills in separate columns -\n<div class=\"gatsby-highlight\" data-language=\"snowflake\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-snowflake line-numbers\"><code class=\"language-snowflake\">create or replace table json_parsed as\n  select\n      raw:first_name::string as first_name,\n      raw:last_name::string as last_name,\n      raw:Skills[0]::string as skills_1,\n      raw:Skills[1]::string as skills_2,\n      raw:age::int as age\n  from json_raw;</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\nwe can also parse it as follows to get a table where there is a new row for each skill. this table is like a resultant join of sorts between skills and the original record values\n<div class=\"gatsby-highlight\" data-language=\"snowflake\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-snowflake line-numbers\"><code class=\"language-snowflake\">select\n  raw:first_name::string as first_name,\n  skill.value::string as skill\nfrom json_raw, table(flatten(raw:Skills)) skill;</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span></span></pre></div>\n</li>\n<li>parquet is a data format used by apache hadoop, is efficient for data compressing. snowflake can handle data in the form of parquet files as well</li>\n<li>apart from using the data inside the files, we have variables available for us to use, e.g. <code class=\"language-text\">metadata$file_row_number</code>, <code class=\"language-text\">$metadata:filename</code>, etc</li>\n</ul>\n<h1 id=\"performance\" style=\"position:relative;\"><a href=\"#performance\" aria-label=\"performance permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Performance</h1>\n<ul>\n<li>sizes of virtual warehouse can go from xs to 4xl. with each increment in size, the number of servers double up i.e. xs(1), s(2), m(4)…4xl(128). this is vertical scaling. note that we can change the size of a warehouse after creating it as well</li>\n<li>we can have multiple <strong>clusters</strong> to fit our needs. we can dynamically scale clusters depending on the workload. this is horizontal scaling. <strong>scaling policies</strong> - can be economic or standard</li>\n<li>in <strong>standard</strong>, if there is any query that is queued up, an additional cluster is spun up</li>\n<li>in <strong>economic</strong>, if snowflake estimates that the query load is enough to keep the currently running clusters busy for more than 6 minutes, an additional cluster is spun up</li>\n<li>clusters can be in suspended state in case of inactivity and resume again automatically when we query them</li>\n<li>enabling auto suspend on warehouses is important to prevent running into enormous costs. but we also need to consider caching when thinking about the timeout of this suspension</li>\n<li>we can also enable auto resume in warehouses</li>\n<li>pricing in snowflake is split into compute and storage. depending on the edition of snowflake we use and our region, we get credits in exchange for money</li>\n<li>like compute has classes, storage too can have classes, and can either be <strong>on demand storage</strong> or <strong>capacity storage</strong>, the latter being cheaper</li>\n<li>we should create dedicated virtual warehouse depending on the type of job, teams in the organization, etc</li>\n<li>caching is done by snowflake automatically behind the scenes without us configuring anything. so, we should consider what warehouse to use so that the caching feature can be used by the multiple queries</li>\n<li>to verify the use of cache, we can use the <strong>query profile</strong> i.e. after a query is run, there is an option to view the query profile under the query stats</li>\n<li>snowflake automatically creates <strong>cluster keys</strong> behind the scenes for us for columns, which helps optimize the performance of our queries</li>\n<li>groups of rows are stored in a single <strong>micro partition</strong>, which helps speed up the search</li>\n<li>the columns which are frequently used in the where clause or columns used for joins should be considered for clustering. note that the number of distinct values across rows for the clustered attribute should be small, and the amount of data we store should be large to see notable speed up due to clustering</li>\n<li>we can even use expressions for cluster keys, e.g. using the <code class=\"language-text\">month</code> function</li>\n<li>to create a cluster key - <code class=\"language-text\">create table ... cluster by (&lt;expr>, &lt;col>, ...)</code> or <code class=\"language-text\">alter table ... cluster by (&lt;expr>, &lt;col>, ...)</code></li>\n<li>note that it takes time for clustering to take effect from the time of creating cluster keys i.e. we need to give snowflake time to form structures behind the scenes</li>\n<li>even the number of partitions that were scanned etc. can be viewed in the query profile, which can be used as a metric to decide how much performance benefit clustering has given us</li>\n</ul>\n<h1 id=\"s3-storage-integration-object\" style=\"position:relative;\"><a href=\"#s3-storage-integration-object\" aria-label=\"s3 storage integration object permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>S3 Storage Integration Object</h1>\n<ul>\n<li>though we can provide aws credentials when using the <code class=\"language-text\">copy into</code> command itself, it is not the best method</li>\n<li>we should instead create a storage integration object</li>\n<li>first, we create an aws iam role, with type “aws account”, and check the option “require external id” and set it to any random value for now</li>\n<li>then, we run the following in snowflake -\n<div class=\"gatsby-highlight\" data-language=\"snowflake\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-snowflake line-numbers\"><code class=\"language-snowflake\">create or replace storage integration s3_integration\n  type = external_stage\n  storage_provider = s3\n  enabled = true\n  storage_aws_role_arn = &#39;arn:aws:iam::&lt;account-id&gt;:role/role-name&#39;\n  storage_allowed_locations = (&#39;s3://&lt;bucket&gt;/&lt;path&gt;&#39;, &#39;s3://&lt;bucket&gt;/&lt;path&gt;&#39;);\n\ndescribe storage integration s3_integration;</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n</li>\n<li>the describe command will spit out two values which can be used to update the trust policy of the iam role -\n<div class=\"gatsby-highlight\" data-language=\"json\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-json line-numbers\"><code class=\"language-json\"><span class=\"token punctuation\">{</span>\n  <span class=\"token property\">\"Version\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"2012-10-17\"</span><span class=\"token punctuation\">,</span>\n  <span class=\"token property\">\"Statement\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span>\n    <span class=\"token punctuation\">{</span>\n      <span class=\"token property\">\"Effect\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Allow\"</span><span class=\"token punctuation\">,</span>\n      <span class=\"token property\">\"Principal\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span>\n        <span class=\"token property\">\"AWS\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"&lt;&lt;storage_aws_iam_user_arn>>\"</span>\n      <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n      <span class=\"token property\">\"Action\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"sts:AssumeRole\"</span><span class=\"token punctuation\">,</span>\n      <span class=\"token property\">\"Condition\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span>\n        <span class=\"token property\">\"StringEquals\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span>\n          <span class=\"token property\">\"sts:ExternalId\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"&lt;&lt;storage_aws_external_id>>\"</span>\n        <span class=\"token punctuation\">}</span>\n      <span class=\"token punctuation\">}</span>\n    <span class=\"token punctuation\">}</span>\n  <span class=\"token punctuation\">]</span>\n<span class=\"token punctuation\">}</span></code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n</li>\n<li>now, we can create the stage like usual. look at how we can default values to null, and also declare how values can be enclosed by double quotes, which can, for e.g. help us use values like “hey, how are you” in the csv\n<div class=\"gatsby-highlight\" data-language=\"snowflake\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-snowflake line-numbers\"><code class=\"language-snowflake\">create or replace file format db.file_formats.s3_csv\n  type = csv\n  field_delimiter = &#39;,&#39;\n  skip_header = 1\n  null_if = (&#39;NULL&#39;, &#39;null&#39;)\n  empty_field_as_null = true\n  field_optionally_enclosed_by = &#39;&quot;&#39;;\n\ncreate or replace stage db.file_formats.s3_csv\n  url = &#39;s3://&lt;bucket&gt;/&lt;path&gt;&#39;\n  storage_integration = s3_integration\n  file_format = db.schema.s3_csv</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n</li>\n<li>now we can create a table and copy the data into it from the stage as usual</li>\n</ul>\n<h1 id=\"snowpipe\" style=\"position:relative;\"><a href=\"#snowpipe\" aria-label=\"snowpipe permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Snowpipe</h1>\n<ul>\n<li>we use <strong>continuous loading</strong> here, since we have small volumes of data that need to be loaded immediately</li>\n<li>snowpipes are serverless i.e. snowflake does not use our warehouses</li>\n<li>whenever a new file is uploaded in for e.g. s3, a notification is sent out to snowflake which triggers the snowpipe so that it automatically refreshes our database tables with the new data</li>\n<li>we need to configure our copy command to the snowpipe</li>\n<li>a best practice - in your database, create different schemas for stages, pipes, file formats, etc\n<div class=\"gatsby-highlight\" data-language=\"snowflake\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-snowflake line-numbers\"><code class=\"language-snowflake\">create or replace pipe db.pipes.employees\n  auto_ingest = true as\n  copy into db.public.employees\n  from @db.stages.s3_csv;</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span></span></pre></div>\n</li>\n<li>when we run <code class=\"language-text\">describe pipe manage_db.pipes.employee_pipe</code>, we see an attribute <code class=\"language-text\">notification_channel</code>. so, we create an event notification in s3 with the target type set to sqs queue, and the arn set to this value</li>\n<li>to view the pipe status, we can use <code class=\"language-text\">select system$pipe_status('db.pipes.employees')</code></li>\n<li>to view errors / statuses of loads which might have occurred, we can use -\n<div class=\"gatsby-highlight\" data-language=\"snowflake\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-snowflake line-numbers\"><code class=\"language-snowflake\">select * from table(information_schema.copy_history(\n  table_name =&gt; &#39;db.public.employees&#39;,\n  start_time =&gt; dateadd(hour, -2, current_timestamp())\n));</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span></span></pre></div>\n</li>\n<li>to check the files loaded by the pipe, use <code class=\"language-text\">alter pipe db.pipes.employees refresh</code></li>\n<li>if we want to for e.g. <a href=\"https://docs.snowflake.com/en/user-guide/data-load-snowpipe-manage.html#modifying-the-copy-statement-in-a-pipe-definition\">make changes to the copy command in our pipe</a>, we should pause it first. we do this by running <code class=\"language-text\">alter pipe db.pipes.employees set pipe_execution_paused=true</code></li>\n<li>then, we use <code class=\"language-text\">create or replace pipe ...</code> to recreate the pipe</li>\n<li>due to how snowflake runs behind the scenes, this automatically gets linked to executions of the previously created pipe even though we are recreating the pipe. verify this by using <code class=\"language-text\">alter pipe ... refresh</code></li>\n<li>then, we pause the pipe again just to verify the configuration, for e.g. using <code class=\"language-text\">describe pipe ...</code> etc. then, we can resume the pipe by setting the <code class=\"language-text\">pipe_execution_paused</code> flag back to false</li>\n</ul>\n<h1 id=\"time-travel\" style=\"position:relative;\"><a href=\"#time-travel\" aria-label=\"time travel permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Time Travel</h1>\n<ul>\n<li>suppose we make an erroneous update</li>\n<li>we can go back to what the table looked like earlier, e.g. 3 minutes back using <code class=\"language-text\">select * from our_first_db.public.test at (offset => -180)</code>. upto how far we can go back depends on the edition of snowflake we are using</li>\n<li>other useful clauses are <code class=\"language-text\">before (timestamp => ...)</code> to see what the table looked like before a particular time or <code class=\"language-text\">before (statement => &lt;&lt;query-id>>)</code> to see what the table looked like before a query was executed</li>\n<li>to restore the table, <strong>do not</strong> use <code class=\"language-text\">create or replace</code>, because a new table is created in this case, and the association with the history is lost. so, instead we can run <code class=\"language-text\">truncate table employees</code> and then <code class=\"language-text\">insert into employees select * from employees at (offset => -300)</code></li>\n<li>optionally, as a best practice, instead of restoring data directly into the original table, we can get the old data into a backup table, truncate the original table and then finally get the data back into the original table from the backup table we just created</li>\n<li>we can drop tables using <code class=\"language-text\">drop table &lt;&lt;table-name>></code>, and we can undo this using <code class=\"language-text\">undrop table &lt;&lt;table-name>></code>. the same is true for schemas and databases as well</li>\n<li>when we run <code class=\"language-text\">show tables</code>, we see an attribute <code class=\"language-text\">retention_time</code> to see the number of days that the table is retained, which is 1 day by default. so, the retention value of a table can be configured while creating or altering it as well</li>\n<li>the higher we set our retention value, the more is the storage and therefore the cost associated with it</li>\n<li><strong>failsafe</strong> - once the retention period discussed above ends, the data gets moved int the failsafe zone. we cannot query it like the data within retention period. we need to contact snowflake to get this data. it is available for 7 days for permanent tables and is non-configurable</li>\n<li>some terminology - i have also seen that normal retention being referred to as <strong>time travel retention</strong> and the one discussed above as <strong>failsafe retention</strong></li>\n<li>the <code class=\"language-text\">snowflake</code> database often has most of the metadata related to all such stuff</li>\n</ul>\n<h3 id=\"types\" style=\"position:relative;\"><a href=\"#types\" aria-label=\"types permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Types</h3>\n<ul>\n<li><strong>permanent</strong> - the default type, it has the defaults of retention and failsafe. this however ends up being expensive. the retention period is configurable between 0-90 days, and the failsafe period is non-configurable and is for 7 days</li>\n<li><strong>transient</strong> - has retention but does not have failsafe protection, thus reducing the costs. the retention period is configurable between 0-1 day. the syntax is simply <code class=\"language-text\">create or replace transient table ...</code></li>\n<li><strong>temporary</strong> - just like transient, it has retention but does not have failsafe protection. however, this table is not available if we log out of the session. the retention period is configurable between 0-1 day</li>\n<li>my understanding - the above types are not only restricted to tables, but for schemas and databases as well, and then the cascading behavior applies down to the tables</li>\n<li>use case - if using for e.g. a table for the staging area, we can use transient type</li>\n</ul>\n<h1 id=\"zero-copy-cloning\" style=\"position:relative;\"><a href=\"#zero-copy-cloning\" aria-label=\"zero copy cloning permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Zero Copy Cloning</h1>\n<ul>\n<li>we clone our tables such that it continues to refer to the original source table’s metadata</li>\n<li>this has benefits like reduced costs, same history, etc</li>\n<li>syntax - <code class=\"language-text\">create table &lt;&lt;table-name>> clone &lt;&lt;source-table-name>></code></li>\n<li>optionally, we can also append time travel features to the command above, e.g. <code class=\"language-text\">before (timestamp => ...)</code></li>\n<li>we can clone only permanent and transient <strong>data storage objects</strong> (i.e. tables, databases, schemas). we can also clone file formats, stages and tasks</li>\n<li>my understanding - when we clone a table, it still refers to the old data, history, etc. but when we start modifying data in the source and clone tables, the layers of the source and the clone start diverging, just like layers in docker images</li>\n<li><strong>swapping tables</strong> - to do this, we just run the command <code class=\"language-text\">alter table &lt;&lt;table-name>> swap with &lt;&lt;target-table-name>></code>. this way, the table name for e.g. remains the same, but now it points to the swapped data and metadata. use case - bringing a development table into production</li>\n</ul>\n<h1 id=\"data-sharing\" style=\"position:relative;\"><a href=\"#data-sharing\" aria-label=\"data sharing permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Data Sharing</h1>\n<ul>\n<li>ideally / usually, the data sharing should enable a readonly access</li>\n<li>shares are of two types - <strong>inbound</strong>, where we are the consumers and <strong>outbound</strong>, where we are the producers</li>\n<li>we first need to grant usage on the database and schema, then we grant select on the table. i think that granting the usages are necessary steps</li>\n<li>basic commands -\n<div class=\"gatsby-highlight\" data-language=\"snowflake\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-snowflake line-numbers\"><code class=\"language-snowflake\">create or replace share orders_share;\n\ngrant usage on database db to share orders_share;\ngrant usage on schema db.schema to share orders_share;\n\ngrant select on table db.schema.orders to share orders_share;\ngrant select on all tables in database db to share orders_share;\ngrant select on all tables in schema schema to share orders_share;\n\nshow grants to share orders_share;</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n</li>\n<li>now, we grant access using <code class=\"language-text\">alter share orders_share add account=&lt;&lt;consumer-snowflake-account-id>></code></li>\n<li>we can also do all this using the snowflake ui as well</li>\n<li>to drop a share, use <code class=\"language-text\">drop share orders_share</code></li>\n<li>now on the consumer side, just as a prerequisite, we need to create a database from the share in order to be able to start using the tables in the share. this database need not be the same as the producer\n<div class=\"gatsby-highlight\" data-language=\"snowflake\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-snowflake line-numbers\"><code class=\"language-snowflake\">show shares;\n\ndescribe share &lt;&lt;producer-snowflake-account-id&gt;&gt;.orders_share;\n\ncreate database db from share &lt;&lt;producer-snowflake-account-id&gt;&gt;.orders_share;\n\nselect * from db.schema.orders;</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n</li>\n<li>till now, since the consumer was a snowflake account as well, sharing of data meant referring to the same stored data but using one’s own compute resources</li>\n<li>however, with non-snowflake consumers, we pay for the producer’s compute</li>\n<li>so, first the producer creates a reader account, and a share\n<div class=\"gatsby-highlight\" data-language=\"snowflake\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-snowflake line-numbers\"><code class=\"language-snowflake\">create managed account consumer_company\n  admin_name = &#39;consumer_company&#39;\n  admin_password = &#39;Consumer-123&#39;\n  type = reader;\n\nshow managed accounts;</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n</li>\n<li>then we run the steps to create a share and grant read access on it as described above</li>\n<li>when we run <code class=\"language-text\">show managed accounts</code>, we see an attribute called <code class=\"language-text\">locator</code>. this is what gets used in the command <code class=\"language-text\">alter share orders_share add account=&lt;&lt;locator>></code></li>\n<li>a common practice can also be exposing views during data sharing. we might need to grant select access on both the tables that the view uses and the view itself, using the <code class=\"language-text\">grant</code> syntax discussed above</li>\n<li>we can only share <strong>secure views</strong> and not normal views</li>\n<li>when exposing views normally, it also exposes some metadata which we may not want to ideally show. so, from a compliance standpoint, we may want to create secure views instead of normal views when exposing data. the syntax is pretty much the same, instead of <code class=\"language-text\">create view</code>, we write <code class=\"language-text\">create secure view</code>. to test this difference, we can use something like <code class=\"language-text\">show views</code>. in there, the definition to create the normal view is accessible, but to the secure view is not</li>\n</ul>\n<h1 id=\"data-sampling\" style=\"position:relative;\"><a href=\"#data-sampling\" aria-label=\"data sampling permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Data Sampling</h1>\n<ul>\n<li>we use a random dataset from our original dataset to develop a view before running it on the entire dataset</li>\n<li>this helps us prevent running into heavy costs for compute</li>\n<li>we have two different methods to achieve this</li>\n<li><strong>row</strong> or <strong>bernoulli</strong> method - there is a percentage p with which each row can be chosen. this makes our dataset a bit more random and ideal. it is recommended for smaller tables</li>\n<li><strong>block</strong> or <strong>system</strong> method - there is a percentage p with which each block (aka micro partition) can be chosen. this makes data sampling for larger datasets more efficient, but less random</li>\n<li>to reproduce the same dataset every time we run the command, we can also provide a custom seed -\n<div class=\"gatsby-highlight\" data-language=\"snowflake\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-snowflake line-numbers\"><code class=\"language-snowflake\">create or replace view address_sample as\n  select * from snowflake_sample_data.tpcds_sf10tcl.customer_address\n  sample row(1) seed(26); -- row can be replaced with system</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span></span></pre></div>\n</li>\n</ul>\n<h1 id=\"tasks\" style=\"position:relative;\"><a href=\"#tasks\" aria-label=\"tasks permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Tasks</h1>\n<ul>\n<li>we can schedule tasks, which consists of one sql statement that gets executed</li>\n<li>when we first create a task, it is in suspended state\n<div class=\"gatsby-highlight\" data-language=\"snowflake\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-snowflake line-numbers\"><code class=\"language-snowflake\">create or replace task customers\n  warehouse = compute_wh\n  schedule = &#39;using cron 0 0,12 * * * utc&#39;\n  as\n  insert into customers(create_date) values(current_timestamp);\n\nshow tasks;\n\nalter task customers resume;</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n</li>\n<li>we can specify a tree of dependencies for tasks. a parent task can have multiple child tasks\n<div class=\"gatsby-highlight\" data-language=\"snowflake\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-snowflake line-numbers\"><code class=\"language-snowflake\">create or replace task customers_two\n  warehouse = compute_wh\n  after customers\n  as\n  insert into customers_two select * from customers;</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n</li>\n<li>instead of specifying the sql to execute, we can also call a stored procedure, e.g. <code class=\"language-text\">as call procedure()</code></li>\n<li>for a history of executions, we can refer <code class=\"language-text\">information_schema.task_history</code></li>\n<li>we can also specify which determines if a task gets executed, using <code class=\"language-text\">when &lt;&lt;condition>></code>. it can be some sql containing joins. if the condition evaluates to false, the current execution of the task gets skipped</li>\n</ul>\n<h1 id=\"streams\" style=\"position:relative;\"><a href=\"#streams\" aria-label=\"streams permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Streams</h1>\n<ul>\n<li>they record the changes i.e. the sql statements executed on a table</li>\n<li>this helps us perform a delta load on the target load after etl</li>\n<li>this process is called <strong>change data capturing</strong> or <strong>cdc</strong></li>\n<li>we have three additional columns that we can use - <code class=\"language-text\">metadata$action</code>, <code class=\"language-text\">metadata$isupdate</code>, <code class=\"language-text\">metadata$row_id</code></li>\n<li>once the data goes into the target table, the stream table is cleared - recall how this implementation is exactly like we would want in warehouses, <a href=\"/data-warehouse/#data-warehouse-architecture\">a transient staging layer</a></li>\n<li>in stream objects, we only pay for the data of the three additional metadata columns, not for the actual data since that is sourced from the original source table itself</li>\n<li>command to create a stream - <code class=\"language-text\">create or replace stream sales_stream on table sales</code></li>\n<li>to view all streams, we can use <code class=\"language-text\">show streams</code></li>\n<li>to view the data in a stream, we can use <code class=\"language-text\">select * from sales_stream</code></li>\n<li>to consume data from a stream into the target table, we do it just like we would for a normal table - <code class=\"language-text\">insert into sales_final (...) select (...) from sales_stream</code></li>\n<li>my understanding - when we run a <code class=\"language-text\">select</code> on the stream along with <code class=\"language-text\">insert</code> or <code class=\"language-text\">create</code>, the stream is said to have been consumed, and the data from it is evicted</li>\n<li>suppose the values in the target table are a result of a join, we can then perform a join between some normal tables and some streams for the target table, depending on the use case</li>\n<li>if we update a row in the source table, we see two corresponding rows for it in the stream table. one with the <code class=\"language-text\">metadata$action</code> set to <code class=\"language-text\">insert</code> while the other with <code class=\"language-text\">metadata$action</code> set to <code class=\"language-text\">delete</code>. for both these rows, <code class=\"language-text\">metadata$isupdate</code> is set to <code class=\"language-text\">true</code></li>\n<li>one way to process updates (it feels like <a href=\"/data-warehouse/#slowly-changing-dimensions\">type 1 scd</a> to me) -\n<div class=\"gatsby-highlight\" data-language=\"snowflake\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-snowflake line-numbers\"><code class=\"language-snowflake\">merge into sales_final\n  using sales_stream\n    on sales_final.id = sales_stream.id\n  when matched\n    and sales_stream.metadata$action = &#39;insert&#39;\n    and sales_stream.metadata$isupdate = &#39;true&#39;\n  then update\n    set sales_final.price = sales_stream.price\n    (...)</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n</li>\n<li>similarly, to process deletes, we can use -\n<div class=\"gatsby-highlight\" data-language=\"snowflake\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-snowflake line-numbers\"><code class=\"language-snowflake\">merge into sales_final\n  using sales_stream\n    on sales_final.id = sales_stream.id\n  when matched\n    and sales_stream.metadata$action = &#39;delete&#39;\n    and sales_stream.metadata$isupdate = &#39;false&#39;\n  then delete;</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\nnote: the <code class=\"language-text\">isupdate</code> column is needed to filter out updates and just process deletes</li>\n<li>it is possible to combine all three operations of insert, update and delete into a single sql operation, but ofcourse all these things depend on our use case etc. our entire warehouse solution can now be implemented by making this sql operation be executed by a task!</li>\n<li>another way is to enable change tracking i.e. <code class=\"language-text\">alter table sales set change_tracking = true</code>. now, we can use time travel as follows to generate a table similar to the streams discussed above -\n<div class=\"gatsby-highlight\" data-language=\"snowflake\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-snowflake line-numbers\"><code class=\"language-snowflake\">select * from sales\n  changes (information =&gt; default)\n  at (offset =&gt; -60);</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span></span></pre></div>\ni think the difference is that unlike the stream, it will not become empty on consumption</li>\n</ul>\n<h1 id=\"materialized-view\" style=\"position:relative;\"><a href=\"#materialized-view\" aria-label=\"materialized view permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Materialized View</h1>\n<ul>\n<li>suppose we have a view that takes time to process. this can use up our compute, thus increasing costs</li>\n<li>materialized view - gets updated automatically using snowflake when the underlying table is updated. however, snowflake does not use our compute to achieve this</li>\n<li>so, materialized views are ideal for data that gets queried frequently and takes time to query but gets updated less often, while normal views are ideal for data that gets queried less often, is quick to query but gets updated much more frequently</li>\n<li><code class=\"language-text\">show materialized views</code> has an attribute called <code class=\"language-text\">refreshed_on</code></li>\n<li>if data changes very frequently, we can also combine tasks and streams to achieve something similar</li>\n</ul>\n<h1 id=\"dynamic-data-masking\" style=\"position:relative;\"><a href=\"#dynamic-data-masking\" aria-label=\"dynamic data masking permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Dynamic Data Masking</h1>\n<ul>\n<li>it allows us to apply column level security</li>\n<li>the command to create a masking policy -\n<div class=\"gatsby-highlight\" data-language=\"snowflake\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-snowflake line-numbers\"><code class=\"language-snowflake\">create or replace masking policy phone\n  as (val varchar) returns varchar -&gt;\n  case        \n    when current_role() in (&#39;ADMIN&#39;) then val\n    else &#39;##-###-##&#39;\n  end;</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\nside note - i was stuck on this for a while, basically i had to capitalize the role name inside the quotes</li>\n<li>to apply the masking policy on a table -\n<div class=\"gatsby-highlight\" data-language=\"snowflake\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-snowflake line-numbers\"><code class=\"language-snowflake\">alter table customers modify column phone\n  set masking policy phone;</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span></span></pre></div>\n</li>\n<li>we can also use the commands <code class=\"language-text\">describe masking policy</code> and <code class=\"language-text\">show masking policies</code></li>\n<li>to drop a masking policy, use <code class=\"language-text\">alter table customers modify column phone unset masking policy</code></li>\n<li>to modify a masking policy, we can use - <code class=\"language-text\">alter masking policy phone set body -> case(...)</code></li>\n<li>a trick - we can replace the return value with <code class=\"language-text\">sha2(val)</code> so that while we do not expose any critical information, yet we can for e.g. conclude the columns which have identical values</li>\n</ul>\n<h1 id=\"access-management\" style=\"position:relative;\"><a href=\"#access-management\" aria-label=\"access management permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Access Management</h1>\n<ul>\n<li>for access management in snowflake, we assign <strong>privileges</strong> to <strong>roles</strong> and roles to <strong>users</strong></li>\n<li>we can define a hierarchy in roles i.e. a role can inherit privileges from another role</li>\n<li><strong>discretionary access control (dac)</strong> - each object has an owner who controls access to that object</li>\n<li><strong>role based access control (rbac)</strong> - privileges are assigned to roles, roles are assigned to users</li>\n<li>snowflake uses both of the above</li>\n<li>we can grant privileges using <code class=\"language-text\">grant &lt;privilege> on &lt;object> to &lt;user></code></li>\n<li>we have a few predefined roles in snowflake. the relationship for e.g. means a user with the role account admin can do everything that a user with role either sysadmin or security admin can do</li>\n<li>since account admin is the most powerful role, it should be used with mfa. it should be used with caution just for account level setup, usage information, etc</li>\n<li>to create users with this role, use -\n<div class=\"gatsby-highlight\" data-language=\"snowflake\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-snowflake line-numbers\"><code class=\"language-snowflake\">create user adam password = &#39;...&#39;\n  default_role = accountadmin\n  must_change_password = true; -- have to change password after first login</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span></span></pre></div>\n</li>\n<li>security admins can create and manage users, roles and grant access</li>\n<li>the user admin does not have global grant privileges unlike security admin</li>\n<li>to create a hierarchy of roles, use - <code class=\"language-text\">grant role sales_admin to role sysadmin</code></li>\n<li>to grant usage on resources, use - <code class=\"language-text\">grant usage on warehouse compute to role public</code></li>\n<li>optionally, we might want to grant ownership of database to a role (that is not sysadmin), which we can do using <code class=\"language-text\">grant ownership on database sales to role sales_admin</code></li>\n<li>as a best practice, a security admin creates users and roles</li>\n<li>then the custom roles should be usually assigned to sysadmin so that sysadmin can inherit all the functionality that a custom role has. the ownership of a database is then granted to the custom admin role</li>\n<li>assume we do not extend the sales admin role to sysadmin. e.g. after granting ownership of the sales database to sales admin, sysadmin can no longer assign privileges on tables inside the sales database to other users, which is not ideal</li>\n<li>every user is automatically assigned to the public role</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 521px; margin: 5px 0 5px 0 !important;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/4d6d1f8692b4dd874ab03bde3c842415/bb9c5/roles.drawio.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 53.79746835443038%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB5UlEQVQoz22SW2/aQBSE/f9/R1VVfU+T9KEJCYrSJiEGQ0xMgi/cXMC1MTa+rNdfZVPTkPZII+1qZ8/O7BylLEsqVLVNJD+MgCt1xmXHpq0tmXg5sOc0PM3a0FJnXDzYPJoR/WmONslqKBWhISa5ZOwm9E0f9cXDmEV424K3nKrmnS69k3PMVhuv0zuC8paYC0mWFxSyRJb7fZqJet00rZBOHUJjSG5bCMcit80DlOE0pj/2UEcreuMANxC1xf0XFIwWaa1WG3sY8xRvKw4Cyv9AGTgR97rLbX/G48hj7otKSn1hExcYi4yOvuCuP0WfJqzD/YNSyqNvaEpp1AjZ2DosDq8iJY3v9y2kEIjNBhEEyDRFsVcZL7MQ3frF62LHOiwOCoswJNYHeF0Vr6eyHWiI1RIhJWmSIGTJbqgT37TZ3d4Qdx9RNCfh+9NPrtUJndeQiVdZ3qsU6zV++wr7/Azn6xle64LMMgmjCNd1CcOQpNdl8PkTnY8fiB/uUBo1meB4PN7MHULs8W586qNwg6c/sR70SVdLlCrV2XqHOd/g+gI/+htKEUcktoVvPBMYz+zMMSLwj0bon1C0Sc79KKDdm6NNBfYqP1jOV0vC6xbO6QnO2ReCy29ktnUUXt24SvxP6r8B7vtEENMaUsEAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"roles\"\n        title=\"roles\"\n        src=\"/static/4d6d1f8692b4dd874ab03bde3c842415/bb9c5/roles.drawio.png\"\n        srcset=\"/static/4d6d1f8692b4dd874ab03bde3c842415/c26ae/roles.drawio.png 158w,\n/static/4d6d1f8692b4dd874ab03bde3c842415/6bdcf/roles.drawio.png 315w,\n/static/4d6d1f8692b4dd874ab03bde3c842415/bb9c5/roles.drawio.png 521w\"\n        sizes=\"(max-width: 521px) 100vw, 521px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>","frontmatter":{"title":"Snowflake"}}},"pageContext":{"id":"3456f466-645c-56db-a136-b71799b35566"}},"staticQueryHashes":["1037383464","1617985380"]}