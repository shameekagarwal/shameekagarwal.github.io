{"componentChunkName":"component---src-templates-blog-js","path":"/microservices/","result":{"data":{"markdownRemark":{"fields":{"slug":"/microservices/"},"id":"f0466c12-830c-5c15-ae4b-56ed7d8c67f5","html":"<h1 id=\"microservices-advantage\" style=\"position:relative;\"><a href=\"#microservices-advantage\" aria-label=\"microservices advantage permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Microservices Advantage</h1>\n<ul>\n<li>reasoning about the codebase becomes easy as each service solves a specific problem</li>\n<li>each service can be developed in parallel. this helps release independent features quicker. in case of a monolith, we had to consolidate changes over a period of time and deploy them at once which increased the chances of bugs and increased time of feedback loop cycle</li>\n<li>scaling services individually - suppose different kinds of services need different resource requirements e.g. a service needs more io while another needs more cpu. we can also scale services horizontally e.g. worker services with higher loads can have more running instances. in a monolith, all services run on the same big instance, and we would have to scale the entire instance i.e. vertical scaling</li>\n<li>since each service is built separately, they can use different languages and frameworks based on requirements</li>\n<li>polyglot persistence i.e. each service has its own persistence layer</li>\n<li>improves fault isolation i.e. if a service goes down, it doesn’t affect the functionality of other services</li>\n<li>loosely coupled e.g. if the database of a service changes, it should have no effect whatsoever on other services</li>\n</ul>\n<h1 id=\"discovery-server\" style=\"position:relative;\"><a href=\"#discovery-server\" aria-label=\"discovery server permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Discovery Server</h1>\n<ul>\n<li>if multiple instances of a service (say serviceX) are running, how can another service (say serviceY) communicate to serviceX?</li>\n<li>one way could be to use server side load balancing i.e. requests from serviceY hit a load balancer first, which then directs requests to instance of serviceX using by using round robbin</li>\n<li>however, this has latency as there are now two requests instead of one as serviceY could have directly reached serviceX instances without the load balancer</li>\n<li>the solution is to use a discovery server, which maintains a list of ip address for each service</li>\n<li>now, each instance first registers itself with a discovery server, and serviceY contacts the discovery server to get the list of addresses for serviceX</li>\n<li>every instance sends a heartbeat to the discovery server regularly to notify the discovery server that it is operating normally. the discovery server can declare the instance of a service as unhealthy after it doesn’t receive a valid heartbeat status a certain number of times for a certain time period</li>\n</ul>\n<h1 id=\"client-side-load-balancing\" style=\"position:relative;\"><a href=\"#client-side-load-balancing\" aria-label=\"client side load balancing permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Client Side Load Balancing</h1>\n<ul>\n<li>but, serviceY is still making two requests - one to the discovery server and the next to make a request to one of the ip addresses of serviceX</li>\n<li>this is why we use client side load balancing</li>\n<li>we cache the results from discovery server in instances of serviceY so that we don’t have to make requests to the discovery server repeatedly</li>\n<li>an advantage of server side load balancing was that it could distribute traffic equally using round robbin, but serviceY cannot achieve this itself</li>\n<li>serviceY doesn’t know how many requests are being made to an instance of serviceX</li>\n<li>serviceY does this by using weighted round robbin based on response time from an instance of serviceX</li>\n<li>the instances can refresh their cache regularly to account for the continuous changes in discovery server like stopping of instances, starting of new instances, etc</li>\n</ul>\n<h1 id=\"retry\" style=\"position:relative;\"><a href=\"#retry\" aria-label=\"retry permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Retry</h1>\n<ul>\n<li>assume serviceY is making requests to serviceX and serviceX is down</li>\n<li>what if a service is down or a few instances of a service is down?</li>\n<li>it takes time for changes in the discovery server to propagate to the instances which initiate the request, as discovery server needs three heartbeat failures and client side load balancing cache refreshes every 30s</li>\n<li>retry mechanisms are used e.g. serviceY can retry 3 times to make requests to serviceX</li>\n</ul>\n<h1 id=\"fallback\" style=\"position:relative;\"><a href=\"#fallback\" aria-label=\"fallback permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Fallback</h1>\n<p>if all retries fail as well, a “fallback” method can be run, which returns a default response e.g. an empty list in case of an e-commerce application which returns a list of products</p>\n<h1 id=\"rate-limiting\" style=\"position:relative;\"><a href=\"#rate-limiting\" aria-label=\"rate limiting permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Rate Limiting</h1>\n<ul>\n<li>we should limit the rate at which calls are allowed to service</li>\n<li>this way a service can process requests from all services instead of all its threads getting blocked due to serving requests by one service only</li>\n<li>it can also protect against attacks like denial of service</li>\n</ul>\n<h1 id=\"bulkhead\" style=\"position:relative;\"><a href=\"#bulkhead\" aria-label=\"bulkhead permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Bulkhead</h1>\n<ul>\n<li>assume one of the endpoints of serviceY is making requests to serviceZ and that serviceZ is very slow</li>\n<li>what if serviceY has a thread pool of 100 threads, and all requests require it to talk to serviceZ?</li>\n<li>requests which do not require serviceY to talk to serviceZ will be kept waiting unnecessarily</li>\n<li>to combat this issue, a certain number of threads are reserved for requests to each service e.g. 70 threads to reach out serviceZ, 30 threads for serviceX, and so on</li>\n<li>note: this would not have been a problem had we used reactive programming</li>\n</ul>\n<h1 id=\"circuit-breaker\" style=\"position:relative;\"><a href=\"#circuit-breaker\" aria-label=\"circuit breaker permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Circuit Breaker</h1>\n<ul>\n<li>assume serviceY is making requests to serviceX and serviceX is down</li>\n<li>serviceY made a request to serviceX and since serviceX is down, it used the retry and fallback mechanism</li>\n<li>but requesting so many times to serviceX just to receive an error results in latency</li>\n<li>serviceY can cache the fact that serviceX will result in an error, and for a certain threshold instead of actually initiating the request, jump to the conclusion that the request will fail to reduce latency</li>\n</ul>\n<h1 id=\"api-gateway\" style=\"position:relative;\"><a href=\"#api-gateway\" aria-label=\"api gateway permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Api Gateway</h1>\n<ul>\n<li>cleints communicate with api gateway instead of talking to the different services directly</li>\n<li>the api gateway acts as a reverse proxy</li>\n<li>it can also act as a load balancer</li>\n<li>it can also handle pre-processing on requests and post-processing on response</li>\n</ul>\n<h1 id=\"config-server\" style=\"position:relative;\"><a href=\"#config-server\" aria-label=\"config server permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Config Server</h1>\n<ul>\n<li>configuration of an application shouldn’t be tied to the application</li>\n<li>we shouldn’t have to redeploy the application manually if the configuration changes</li>\n<li>we can have a lot of environments like dev, staging, prod, etc. and managing them can be tough</li>\n<li>we can also have sensitive information like database passwords</li>\n<li>a lot of configuration is there for every microservice, much of which is common</li>\n<li>so, we need a central place to manage all this configuration, which we can achieve using a config server</li>\n<li>each microservice on bootstrap pulls its configuration from this config server</li>\n<li>config server can have various sources like git online repository, vault, local file system, etc</li>\n<li>there are two approaches - <strong>config first</strong> and <strong>discovery first</strong></li>\n<li>in config first, the services fetch all details, including discovery server details from the config server. many config servers can sit behind a load balancer, this way the config servers are highly available. config servers run separately from discovery server, microservices, etc., and behave like an endpoint for microservices so that they can fetch their config</li>\n<li>in discovery first, the services connect to the discovery server first, treat config server like just another service and so eventually retrieve configuration after establishing connection to the discovery server</li>\n<li>i like config first because this would mean only config server url and bare minimum configuration needs to be mentioned in the microservices, rest of the configuration can be fetched from the config server, where configuration for different services can be managed easily</li>\n</ul>\n<h1 id=\"asynchronous-messaging\" style=\"position:relative;\"><a href=\"#asynchronous-messaging\" aria-label=\"asynchronous messaging permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Asynchronous Messaging</h1>\n<ul>\n<li>assume a client makes requests to serviceY which requires serviceY to make requests to serviceX</li>\n<li>the client and serviceY will have to wait for serviceX to respond - this results in coupling of services</li>\n<li>what if the processing in serviceX doesn’t affect the response e.g. it only needs to update database entries?</li>\n<li>what if serviceX is down?</li>\n<li>so, we can asynchronously deliver messages and not expect a synchronous response</li>\n<li>serviceX will process the message eventually whenever it has running instances</li>\n</ul>\n<h1 id=\"rabbitmq\" style=\"position:relative;\"><a href=\"#rabbitmq\" aria-label=\"rabbitmq permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>RabbitMQ</h1>\n<ul>\n<li>implements AMQP (advanced message queuing protocol)</li>\n<li>producer sends messages to exchange</li>\n<li>consumer receives messages from queue</li>\n<li>exchanges connect to queues via binding keys</li>\n<li>messages have routing keys</li>\n<li>exchanges decide which queues to send the messages by comparing routing keys and binding keys</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; margin: 5px 0 5px 0 !important;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/2e76ecc796f10df3e787de06f355368b/c7dcc/rabbitmq.drawio.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 46.835443037974684%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA+ElEQVQoz32ShwrDMAxE/f//GEIgCZlk76HyBApuaWsQtmXp7iTLiYjc9y3Hcajt+y7XdYn5/YW/6zq1oihknmfp+17KspQsy9TnLLBtWwnDUNI0lWVZHhBISBrHUaZpkmEY9IwBiIBt254YZ0qSJJEgCJTtExBmVJnyf8uRDNO6rk/Z3GHjjP88zzcCVNnOm+WA5fI81/pRgHT2OI7VUF3XtRCDUW5VVao4iiJpmkZbZe+YQwHovgqASaZE3lHySyEx5FqVzq//81cNgApQ45P+7CEgZgbq343dek1LUA8Bv47P7pTv/rF9m0PGA6O3kPB5gDFuTMgLqEW8/LXpfFcAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"rabbitmq\"\n        title=\"rabbitmq\"\n        src=\"/static/2e76ecc796f10df3e787de06f355368b/f058b/rabbitmq.drawio.png\"\n        srcset=\"/static/2e76ecc796f10df3e787de06f355368b/c26ae/rabbitmq.drawio.png 158w,\n/static/2e76ecc796f10df3e787de06f355368b/6bdcf/rabbitmq.drawio.png 315w,\n/static/2e76ecc796f10df3e787de06f355368b/f058b/rabbitmq.drawio.png 630w,\n/static/2e76ecc796f10df3e787de06f355368b/c7dcc/rabbitmq.drawio.png 641w\"\n        sizes=\"(max-width: 630px) 100vw, 630px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>types of rabbitmq exchange -</p>\n<ul>\n<li>fanout - ignore the routing key and send the message to all queues</li>\n<li>direct - forward messages to queue where the routing key = binding key</li>\n<li>topic - forward messages to queue where the routing key = binding key but the binding key is like a regex</li>\n<li>header - uses the message header instead of the routing key</li>\n<li>default / nameless - match the routing key with the queue name</li>\n</ul>\n<h3 id=\"how-i-use-rabbitmq\" style=\"position:relative;\"><a href=\"#how-i-use-rabbitmq\" aria-label=\"how i use rabbitmq permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>How I Use RabbitMQ</h3>\n<p>doubt: this is how i use rabbitmq for microservices, please let me know if there is a better way</p>\n<ul>\n<li>i use a direct exchange, and all queues are connected to the same direct exchange</li>\n<li>all instances of the same microservice listen to the same queue to process events only once in round robbin</li>\n<li>for each event, there is a separate queue</li>\n<li>optimistic concurrency control: events can be processed out of order specially because of multiple instances, so i maintain a version column in database tables. if a microservice receives an update for a column with version > stored version + 1, i throw an exception. this will cause no acknowledgements to happen and event goes back to rabbitmq, which will again dispatch the event to the queue. <a href=\"https://gitlab.com/shameekagarwal/online-judge/-/blob/main/backend/shared/src/main/java/com/onlinejudge/shared/util/EventProcessor.java\">my implementation for spring boot</a></li>\n<li>i have not tried implementing this yet, but maybe at this point after certain number of times of throwing the event back to rabbitmq, it should go to a dead letter queue where admins can decide what to do with it</li>\n<li>the service name is added to the queue name so that two different services don’t use the same queue</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; margin: 5px 0 5px 0 !important;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/49fdcd061e6a93368c08f307869b15d7/49217/microservices-rabbitmq.drawio.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 45.56962025316456%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAABMElEQVQoz5VRa4+DMAzr//+Nm4S28R6MFlroAzw5t+6m6b5cJCu0pI7jKOcciGmasG0bvPdgHMchyDGME9ruLmjaHnpeUFUViqJA3/e4Xq8oyxKqrmu5PJ/PUmCtfRMy2GBZFljrsPmAzXusm4cPEcZoIeObcRzxeDyg1nWVRwQVppQQQsC+75LbtpViNqYCNqUaEnGqELzUaK3lrcoj8cAgSbaAYCM25b334ZW91FMA/2XwrLJXMUYhogJKb5rmPf5/Qn0a/5fC3DmlHTEmxJQk7/sB5yyMMZjnWcBv9bnRTJ49pGr6MwyDbJnbJerXlukrl0mPL5cLbrfbL2GOb8Uk5ZanScOYHyVaG7h1fW3fyhTMnEx9EmWyb8UMquBm732PruvEZ2aqP51Ooo4WPQGyB7jEzBOPPwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"microservices-rabbitmq\"\n        title=\"microservices-rabbitmq\"\n        src=\"/static/49fdcd061e6a93368c08f307869b15d7/f058b/microservices-rabbitmq.drawio.png\"\n        srcset=\"/static/49fdcd061e6a93368c08f307869b15d7/c26ae/microservices-rabbitmq.drawio.png 158w,\n/static/49fdcd061e6a93368c08f307869b15d7/6bdcf/microservices-rabbitmq.drawio.png 315w,\n/static/49fdcd061e6a93368c08f307869b15d7/f058b/microservices-rabbitmq.drawio.png 630w,\n/static/49fdcd061e6a93368c08f307869b15d7/49217/microservices-rabbitmq.drawio.png 701w\"\n        sizes=\"(max-width: 630px) 100vw, 630px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>","frontmatter":{"title":"Microservices"}}},"pageContext":{"id":"f0466c12-830c-5c15-ae4b-56ed7d8c67f5"}},"staticQueryHashes":["1037383464","1617985380"]}