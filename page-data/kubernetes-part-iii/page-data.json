{"componentChunkName":"component---src-templates-blog-js","path":"/kubernetes-part-iii/","result":{"data":{"markdownRemark":{"fields":{"slug":"/kubernetes-part-iii/"},"id":"de14570f-d795-55ed-b17b-2ef3cf90fb53","html":"<h1 id=\"nodename\" style=\"position:relative;\"><a href=\"#nodename\" aria-label=\"nodename permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>NodeName</h1>\n<ul>\n<li>the scheduler schedules a pod on any node</li>\n<li>if we run <code class=\"language-text\">kubectl get pod pod_name --output=yaml</code>, we can see the node it was scheduled on under <code class=\"language-text\">nodeName</code></li>\n<li>behind the scenes, a binding object is created which binds the pod to a node</li>\n<li>we can manually specify the node a pod should be scheduled on using the <code class=\"language-text\">nodeName</code> property</li>\n<li>we can use this if we didnâ€™t have a scheduler, and this would schedule the pod on the specified node</li>\n</ul>\n<h1 id=\"taint-and-toleration\" style=\"position:relative;\"><a href=\"#taint-and-toleration\" aria-label=\"taint and toleration permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Taint and Toleration</h1>\n<ul>\n<li>taint is set on nodes which prevent any random pod from being scheduled on it</li>\n<li>toleration is set on pods which allows them to be scheduled on a node with taint</li>\n<li>by default, the pods have no toleration</li>\n<li>use case - a worker node has resources to enable running of a certain type of pod</li>\n<li>it means that only pods with toleration <strong>can be</strong> scheduled on the node with that taint</li>\n<li>however, the pods with this toleration can be scheduled on other nodes as well</li>\n<li>this feature is used by kubernetes as well to help ensure that normal pods are not scheduled on the master and only the management pods scheduled by kubernetes itself are</li>\n<li>to taint nodes, use <code class=\"language-text\">kubectl taint node node_name key=value:taint-effect</code></li>\n<li>similarly, to remove the taint, use <code class=\"language-text\">kubectl taint node node_name key=value:taint-effect-</code>, i.e. suffix the prior command with a <code class=\"language-text\">-</code> symbol</li>\n<li>taint effects can be -\n<ul>\n<li><code class=\"language-text\">NoSchedule</code> - do not schedule any new pods without the right toleration</li>\n<li><code class=\"language-text\">PreferNoSchedule</code> - prefer not scheduling</li>\n<li><code class=\"language-text\">NoExecute</code> - like <code class=\"language-text\">NoSchedule</code> but also evicts the existing pods on the node without the correct toleration</li>\n</ul>\n</li>\n<li>to apply toleration on pods, use -\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-yaml line-numbers\"><code class=\"language-yaml\"><span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">tolerations</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">key</span><span class=\"token punctuation\">:</span> key\n      <span class=\"token key atrule\">operator</span><span class=\"token punctuation\">:</span> Equal\n      <span class=\"token key atrule\">value</span><span class=\"token punctuation\">:</span> value\n      <span class=\"token key atrule\">effect</span><span class=\"token punctuation\">:</span> NoSchedule</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n</li>\n</ul>\n<h1 id=\"node-selectors-and-node-affinity\" style=\"position:relative;\"><a href=\"#node-selectors-and-node-affinity\" aria-label=\"node selectors and node affinity permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Node Selectors and Node Affinity</h1>\n<ul>\n<li>we add labels to nodes and then add selectors for them to pod definitions</li>\n<li>this way, the pods with the node affinity can only be run only on specific nodes</li>\n<li>however, pods without the node affinity can still be spun up on the nodes with labels</li>\n<li>to label a node, we use - <code class=\"language-text\">kubectl label node node_name key=value</code></li>\n<li>we can use <code class=\"language-text\">kubectl get nodes --show-labels</code> to verify</li>\n<li>to apply selectors for labels on nodes, use -\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-yaml line-numbers\"><code class=\"language-yaml\"><span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">nodeSelector</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">size</span><span class=\"token punctuation\">:</span> large</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span></span></pre></div>\n</li>\n<li>but, using node selectors we cannot specify complex conditions</li>\n<li>so, we use node affinity\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-yaml line-numbers\"><code class=\"language-yaml\"><span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">affinity</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">nodeAffinity</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">requiredDuringSchedulingIgnoredDuringExecution</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">nodeSelectorTerms</span><span class=\"token punctuation\">:</span>\n          <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">matchExpressions</span><span class=\"token punctuation\">:</span>\n              <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">key</span><span class=\"token punctuation\">:</span> size\n                <span class=\"token key atrule\">operator</span><span class=\"token punctuation\">:</span> In\n                <span class=\"token key atrule\">values</span><span class=\"token punctuation\">:</span>\n                  <span class=\"token punctuation\">-</span> large\n                  <span class=\"token punctuation\">-</span> medium</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n</li>\n<li>what if the node labels are changed after the pod was already scheduled? what if there are no nodes found with the conditions matching the node affinity value? for these, the value can be <code class=\"language-text\">requiredDuringSchedulingIgnoredDuringExecution</code> or <code class=\"language-text\">preferredDuringSchedulingIgnoredDuringExecution</code></li>\n<li>my understanding - <code class=\"language-text\">requiredDuringSchedulingRequiredDuringExecution</code> is not available by default</li>\n<li>some operators to use - <code class=\"language-text\">Equal</code>, <code class=\"language-text\">In</code>, <code class=\"language-text\">Exists</code></li>\n<li>so, overall, to ensure pods of a particular type and only this type end up on a particular node, we need to use node selectors / node affinity and taints and tolerations in conjunction</li>\n</ul>\n<h1 id=\"resource-management\" style=\"position:relative;\"><a href=\"#resource-management\" aria-label=\"resource management permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Resource Management</h1>\n<ul>\n<li>we can give an indication and set limits for the resources that can be used by kubernetes components</li>\n<li>specified at the container level</li>\n<li>this helps kubernetes in scheduling</li>\n<li>to enable metrics server, use - <code class=\"language-text\">minikube addons enable metrics-server</code></li>\n<li>can be written as for e.g. <code class=\"language-text\">0.5</code> or <code class=\"language-text\">500m</code> (500 milli cpu). 1 milli cpu is equivalent to 1 hyperthread / 1 vcpu</li>\n<li>memory can be written as <code class=\"language-text\">K</code> or <code class=\"language-text\">Ki</code> for kilobyte, <code class=\"language-text\">M</code> or <code class=\"language-text\">Mi</code> for megabyte and so on. we can only specify the numerical value as well, its value is in bytes e.g. <code class=\"language-text\">256Mi</code> or <code class=\"language-text\">268435456</code></li>\n<li>syntax -\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-yaml line-numbers\"><code class=\"language-yaml\"><span class=\"token key atrule\">containers</span><span class=\"token punctuation\">:</span>\n  <span class=\"token punctuation\">-</span> <span class=\"token comment\">#... </span>\n    <span class=\"token key atrule\">resources</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">limits</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">memory</span><span class=\"token punctuation\">:</span> 100Mi\n        <span class=\"token key atrule\">cpu</span><span class=\"token punctuation\">:</span> 200m\n      <span class=\"token key atrule\">requests</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">memory</span><span class=\"token punctuation\">:</span> 50Mi\n        <span class=\"token key atrule\">cpu</span><span class=\"token punctuation\">:</span> 100m</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n</li>\n<li>limits - amount of resources that containers should not cross</li>\n<li>if the container crosses the memory limit, it will be terminated / restarted. the pod has status <code class=\"language-text\">OOMKilled</code> (out of memory killed). the pod remains the same, the container changes</li>\n<li>containers are not allowed to use more than the cpu limit for an extended period, so there are no restarts / termination of the containers for crossing the cpu limits as cpu usage gets throttled automatically</li>\n<li>requests - amount of resources that containers are expected to use</li>\n<li>only when the node runs out of memory, the pod that the container exceeding the requests is a part of is evicted from the node, and it gets rescheduled</li>\n<li>if a containerâ€™s memory request exceeds the available memory on any node (technically sum of the memory requests of all the containers of a pod), the pod stays in <code class=\"language-text\">Pending</code> state indefinitely</li>\n<li>if the memory usage exceeds only the requested amount (and not the limit), the pod can be evicted if another pod enters with a higher qos and needs that memory</li>\n<li>so, memory limit cannot be exceeded while memory request can be exceeded if the node has enough memory</li>\n<li><code class=\"language-text\">kubectl describe nodes</code> gives details of the available and in use resources</li>\n<li><code class=\"language-text\">kubectl top pods</code> gives cpu and memory usage details because of the metrics-server addon. to get information related to the containers in the pod as well, we can use <code class=\"language-text\">kubectl top pods --containers</code></li>\n<li>similarly, we can use <code class=\"language-text\">kubectl top nodes</code></li>\n<li>prometheus is a better solution than metrics-server for real world use cases</li>\n<li>qos - quality of service determines the priority - guaranteed > burstable > best effort</li>\n<li>guaranteed - resource limit = resource request. note: remember that if only limits are defined, request = limit</li>\n<li>burstable - at least one container has limit / request defined, unequal limits and requests, etc</li>\n<li>best effort - no resources are defined at all</li>\n<li>we can view the qos assigned by kubernetes using <code class=\"language-text\">kubectl describe pod pod_name | grep QoS</code></li>\n<li>additional concept - priority classes are useful for e.g. when two pods have the same <code class=\"language-text\">qosClass</code>. we can run <code class=\"language-text\">k get priorityClasses</code> and then assign one of the values using <code class=\"language-text\">priorityClassName</code> under <code class=\"language-text\">spec</code> of the pod</li>\n</ul>\n<h3 id=\"limit-ranges\" style=\"position:relative;\"><a href=\"#limit-ranges\" aria-label=\"limit ranges permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Limit Ranges</h3>\n<ul>\n<li>limit ranges help us specify the following at a namespace level (<a href=\"https://gist.github.com/shameekagarwal/75ae269c7c98c48c57ec215c9dbba20e\">a yaml example</a>) -</li>\n<li><code class=\"language-text\">default</code> - default resources limit</li>\n<li><code class=\"language-text\">defaultRequest</code> - the default resources request</li>\n<li><code class=\"language-text\">max</code> and <code class=\"language-text\">min</code> the maximum and minimum permitted values for the requests and limits</li>\n<li><code class=\"language-text\">maxLimitRequestRatio</code> - the maximum limit to request ratio (limit should ideally be higher than request?)</li>\n</ul>\n<h3 id=\"resource-quotas\" style=\"position:relative;\"><a href=\"#resource-quotas\" aria-label=\"resource quotas permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Resource Quotas</h3>\n<ul>\n<li>limits the resources that can be consumed by a namespace. so, if we have multiple namespaces to support environments like dev and prod in our clusters, we can distribute resources equally so that there is no starvation for any of the environments. <a href=\"https://gist.github.com/shameekagarwal/8343d4bb2e0029ee00f4d57f8f4b9306\">a yaml example</a></li>\n<li>using resource quotas, we can limit compute (e.g. cpu and memory requests and limits), storage (e.g. persistent volume claims) and object count (e.g. number of pods, number of node ports, etc.)</li>\n</ul>\n<h1 id=\"daemon-sets\" style=\"position:relative;\"><a href=\"#daemon-sets\" aria-label=\"daemon sets permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Daemon Sets</h1>\n<ul>\n<li>ensures one pod runs on each node</li>\n<li>e.g. logging and monitoring pods which need to be run on every node can be created using daemon sets</li>\n<li>even kube-proxy is run this way. to verify, use - <code class=\"language-text\">kubectl get daemonsets --all-namespaces</code></li>\n<li>it used <a href=\"#nodename\"><code class=\"language-text\">nodeName</code></a> but in newer versions <a href=\"#node-selectors-and-node-affinity\">node affinity</a> underneath</li>\n</ul>\n<h1 id=\"init-containers\" style=\"position:relative;\"><a href=\"#init-containers\" aria-label=\"init containers permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Init Containers</h1>\n<ul>\n<li>defined inside a pod</li>\n<li>before the long-running containers start, we might want to install binaries, etc</li>\n<li>init containers are run to completion one at a time sequentially before the normal containers start running</li>\n<li>their syntax in yaml is the same as normal containers</li>\n</ul>\n<h1 id=\"static-pods\" style=\"position:relative;\"><a href=\"#static-pods\" aria-label=\"static pods permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Static Pods</h1>\n<ul>\n<li>a pod created by kubelet itself on the node without involving the api server / etcd is called a static pod</li>\n<li>the kubelet continuously monitors a directory for changes</li>\n<li>so, when we can create a file in it, it gets picked up by the kubelet</li>\n<li>if we edit the file / remove the file, the kubelet automatically changes / terminates the pod accordingly</li>\n<li>this does not work for deployments etc. since they require controllers</li>\n<li>if the node is a part of a cluster, it will notify the api server about the static pod. so, <code class=\"language-text\">kubectl get pods</code> will show the pod, since the etcd cluster etc. know about these pods. however, unlike a normal pod, the only way to modify this pod is to modify the file</li>\n<li>use case - since static pods does not depend on control plane components like controllers, scheduler, etc., they are used to deploy the control plane components themselves</li>\n<li>unless configured otherwise, the directory is <code class=\"language-text\">/etc/kubernetes/manifests/</code></li>\n<li>we can verify this in minikube after running <code class=\"language-text\">minikube ssh</code> by running <code class=\"language-text\">sudo ls /etc/kubernetes/manifests</code> that it has files for etcd, scheduler, api server and controller manager</li>\n<li>static pods will be suffixed by <code class=\"language-text\">-nodename</code> - <code class=\"language-text\">kubectl get pods --all-namespaces | grep minikube</code></li>\n<li>if we run <code class=\"language-text\">kubectl get pod pod_name --output=yaml</code>, we can confirm that the owner is a node by going to <code class=\"language-text\">ownerReferences.kind</code> which should have the value <code class=\"language-text\">Node</code></li>\n<li>to get the static pod path, use - <code class=\"language-text\">cat /var/lib/kubelet/config.yaml | grep staticPodPath</code></li>\n</ul>\n<h1 id=\"persistent-volumes\" style=\"position:relative;\"><a href=\"#persistent-volumes\" aria-label=\"persistent volumes permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Persistent Volumes</h1>\n<ul>\n<li>persistence of state should be decoupled from pods since they can be added / removed easily</li>\n<li>nfs is the way to go for disk storage in cloud. here, aws ebs has been shown</li>\n<li>note: ebs volumes should only be spun up in azs where worker nodes exist, since ebs is scoped to an az</li>\n<li>the <code class=\"language-text\">spec.capacity.storage</code> in the persistent volume defn. should be &#x3C;= the capacity of ebs</li>\n<li>access modes can be <code class=\"language-text\">ReadWriteOnce</code>, <code class=\"language-text\">ReadOnlyMany</code>, <code class=\"language-text\">ReadWriteMany</code></li>\n<li>we can run <code class=\"language-text\">kubectl get storageclasses</code> to get the available storage classes</li>\n<li>e.g. if we were using kops with aws, it would automatically add the storage class of gp2 for us</li>\n<li>default storage class admission controller observe requests for persistent volume claims and when a claim does not specify the storage class, it gets assigned the default storage class. when we run <code class=\"language-text\">kubectl get storageclasses</code>, we see that gp2 is marked as default</li>\n<li><a href=\"https://gist.github.com/shameekagarwal/03e5e9dd6c43439d654792bb8822806d\">yaml example</a> of persistent volume</li>\n<li>persistent volumes are used through persistent volume claims. the idea is that admins create a set of persistent volumes, and developers use them via persistent volume claims</li>\n<li>there is a one to one mapping i.e. one persistent volume can only be used by one persistent volume claim</li>\n<li><code class=\"language-text\">spec.storageClassName</code> and <code class=\"language-text\">spec.accessModes</code> should have the same value as that of persistent volume while the value of <code class=\"language-text\">spec.resources.requests.storage</code> should be &#x3C;= the value of <code class=\"language-text\">spec.capacity.storage</code> so that the persistent volume claim can get a segment of the persistent volume</li>\n<li>because of this, if the persistent volume has more storage than what the persistent volume claim asks for, the claim gets the extra storage as well</li>\n<li>if no matching persistent volume is found, the persistent volume claim remains unbound indefinitely</li>\n<li><a href=\"https://gist.github.com/shameekagarwal/a2afa15e76ee80c75a2dc19bfd234a54\">yaml example</a> of persistent volume claim</li>\n<li>usage -\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-yaml line-numbers\"><code class=\"language-yaml\"><span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">containers</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token comment\"># ...</span>\n      <span class=\"token key atrule\">volumeMounts</span><span class=\"token punctuation\">:</span>\n      <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> jenkins<span class=\"token punctuation\">-</span>home\n        <span class=\"token key atrule\">mountPath</span><span class=\"token punctuation\">:</span> /var/jenkins_home\n\n    <span class=\"token key atrule\">volumes</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> jenkins<span class=\"token punctuation\">-</span>home\n      <span class=\"token key atrule\">persistentVolumeClaim</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">claimName</span><span class=\"token punctuation\">:</span> jenkins<span class=\"token punctuation\">-</span>storage</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n</li>\n<li>the status of a persistent volume can be -\n<ul>\n<li><code class=\"language-text\">Available</code> when no persistent volume claim is bound to it</li>\n<li><code class=\"language-text\">Bound</code> when a persistent volume claim is bound to it</li>\n<li><code class=\"language-text\">Released</code> when the persistent volume claim is deleted</li>\n</ul>\n</li>\n<li>the default reclaim policy of a persistent volume is <code class=\"language-text\">Retain</code>. first, the pod / deployment is deleted, and then the persistent volume claim is deleted. now, the persistent volume has status of released. but it is not available to be bound because it already has existing data from previous pods which need to be deleted first</li>\n<li>so, we delete the persistent volume manually, try to clean up / delete the aws ebs manually and then can create new persistent volumes for the persistent volume claims</li>\n<li>till now, we used the manual method of provisioning volumes, i.e. static persistent volumes</li>\n<li>the dynamic method requires lesser intervention</li>\n<li>however, in case of a conflict, kubernetes will choose the static one</li>\n<li>the persistent volume is created automatically in case of dynamic persistent volumes</li>\n<li>when we delete the deployment and then the persistent volume claim now, the persistent volume as well as the actual nfs ebs volume is deleted automatically. this is because when using dynamic persistent volumes, the reclaim policy of the persistent volume is <code class=\"language-text\">Delete</code></li>\n<li><a href=\"https://gist.github.com/shameekagarwal/b5013b4645d62d287aeb2868ae37e5c3\">yaml example</a> for persistent volume claim for dynamic persistent volume</li>\n<li>the storage classes have a field called volume binding mode. this can be set to <code class=\"language-text\">WaitForFirstConsumer</code> i.e. persistent volume will not be bound to the persistent volume claim till there is a pod for the persistent volume claim. the other value that the binding mode can take is <code class=\"language-text\">Immediate</code></li>\n</ul>\n<h1 id=\"commands-and-arguments\" style=\"position:relative;\"><a href=\"#commands-and-arguments\" aria-label=\"commands and arguments permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Commands and Arguments</h1>\n<ul>\n<li>difference between command and entrypoint in docker is described <a href=\"/docker/#cmd-and-entrypoint\">here</a></li>\n<li>for e.g. in the pod definition -\n<ul>\n<li><code class=\"language-text\">spec.containers[*].command</code> is used for replacing <code class=\"language-text\">ENTRYPOINT</code> of docker file</li>\n<li><code class=\"language-text\">spec.containers[*].args</code> is used for replacing <code class=\"language-text\">CMD</code> of docker file</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"security-context\" style=\"position:relative;\"><a href=\"#security-context\" aria-label=\"security context permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Security Context</h1>\n<ul>\n<li>when we run a container, we can specify the id of the user used to run the container, capabilities, etc</li>\n<li>we can specify this at the pod level so that all the containers inherit it or at the container level as well</li>\n<li>note: capabilities can only be defined at the container level</li>\n<li>syntax -\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-yaml line-numbers\"><code class=\"language-yaml\"><span class=\"token key atrule\">securityContext</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">runAsUser</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1000</span>\n  <span class=\"token key atrule\">capabilities</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">add</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"MAC_ADMIN\"</span><span class=\"token punctuation\">]</span></code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span></span></pre></div>\n</li>\n<li>this is more of a docker functionality which can be specified via kubernetes. basically, since containers run using namespaces, and we do not want root users inside namespaces to perform any critical operations on the host itself. so, docker adds only some capabilities, which we extend using the add capabilities mechanism</li>\n<li>to verify, we can use <code class=\"language-text\">kubectl exec -- whoami</code>. it should display the username or the user id</li>\n</ul>\n<h1 id=\"patching-nodes\" style=\"position:relative;\"><a href=\"#patching-nodes\" aria-label=\"patching nodes permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Patching Nodes</h1>\n<ul>\n<li>if we remove a node suddenly, the pods scheduled on it are lost</li>\n<li>if it was a part of a replica set, it would be rescheduled, but not if it was a normal pod</li>\n<li>to stop any further scheduling on the current node, run <code class=\"language-text\">kubectl cordon node_name</code></li>\n<li>to stop any further scheduling and also evict existing pods, use <code class=\"language-text\">kubectl drain node_name</code></li>\n<li>if the node included a pod not spun as a part of a controller, we have to add <code class=\"language-text\">--force</code>. this is because that pod would be lost forever</li>\n<li>so, i think drain already does what cordon does</li>\n<li>pods part of replica sets etc. will be rescheduled on other nodes, because that is the job of controllers</li>\n<li>after running the <code class=\"language-text\">drain</code>, we can start the upgrade</li>\n<li>to enable scheduling pods on the node again, run <code class=\"language-text\">kubectl uncordon node_name</code></li>\n<li>my understanding - suppose a node goes down. so do the pods running on it. the time a controller waits to reconsider rescheduling the pod on another node is defined via <code class=\"language-text\">podEvictionTimeout</code>. this is why draining nodes is important, we donâ€™t rely on the timeout, and instead, rescheduling of pods happens gracefully</li>\n<li>to verify, use <code class=\"language-text\">kubectl describe node &lt;&lt;node-name>> | grep Unschedulable</code></li>\n</ul>\n<h1 id=\"upgrading-cluster\" style=\"position:relative;\"><a href=\"#upgrading-cluster\" aria-label=\"upgrading cluster permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Upgrading Cluster</h1>\n<ul>\n<li>we can run <code class=\"language-text\">kubectl get nodes</code> to see the version of kubernetes running on the nodes</li>\n<li>kubernetes follows semver with major, minor, patch and pre-release of alpha and beta</li>\n<li>this should be relatively easier in gke, eks, etc., the process described is for tools like kubeadm</li>\n<li>ideally, the kubernetes version of the components like controller, kube proxy, etc. should be &#x3C;= the version on api server, since all the components communicate with it</li>\n<li>so, we first upgrade the master, this means kubectl etc. wonâ€™t work since core components like api server are down, but ideally the applications should function normally since they are hosted on the worker nodes</li>\n<li>we can upgrade worker nodes all at once which results in downtime, one at a time or add nodes with the newer version to the cluster and delete the older nodes</li>\n<li>note: kubeadm does not manage kubelet, so we need to upgrade its version manually</li>\n<li>ideally, we should upgrade one minor version only at one go</li>\n<li>the cni will have its own upgrade instructions</li>\n<li>the <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/\">docs</a> have a nice step-by-step procedure</li>\n<li>some useful commands to verify versions include <code class=\"language-text\">kubectl version --output=json</code>, <code class=\"language-text\">kubeadm version</code> and <code class=\"language-text\">kubeadm upgrade plan</code>. <code class=\"language-text\">kubectl get nodes</code> also shows the version of kubernetes the nodes are running</li>\n<li>the steps for both control plane and worker nodes for kubeadm could be broken down into -\n<ul>\n<li>upgrade kubeadm</li>\n<li>drain the node</li>\n<li>upgrade kubelet (and kubectl)</li>\n<li>uncordon node</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"etcd-backup-and-restore\" style=\"position:relative;\"><a href=\"#etcd-backup-and-restore\" aria-label=\"etcd backup and restore permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Etcd Backup and Restore</h1>\n<ul>\n<li>to back up resources, we can use <code class=\"language-text\">kubectl get all --all-namespaces --output=yaml > resources.yaml</code> but it need not cover all resources. tools like velero help in back up as they query kubernetes api to get all resources</li>\n<li>another method is to back up and restore the etcd cluster itself</li>\n<li>etcd data directory and etcd certificates use <a href=\"/kubernetes-part-ii#volumes\">host path volumes</a></li>\n<li>volumes section in the output of <code class=\"language-text\">kubectl get pod etcd-minikube --namespace=kube-system --output=yaml</code>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-yaml line-numbers\"><code class=\"language-yaml\"><span class=\"token key atrule\">volumes</span><span class=\"token punctuation\">:</span>\n  <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">hostPath</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">path</span><span class=\"token punctuation\">:</span> /var/lib/minikube/certs/etcd\n      <span class=\"token key atrule\">type</span><span class=\"token punctuation\">:</span> DirectoryOrCreate\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> etcd<span class=\"token punctuation\">-</span>certs\n  <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">hostPath</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">path</span><span class=\"token punctuation\">:</span> /var/lib/minikube/etcd\n      <span class=\"token key atrule\">type</span><span class=\"token punctuation\">:</span> DirectoryOrCreate\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> etcd<span class=\"token punctuation\">-</span>data</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n</li>\n<li>run <code class=\"language-text\">export ETCDCTL_API=3</code> before running etcd commands</li>\n<li>to create a snapshot (<a href=\"https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#snapshot-using-etcdctl-options\">docs</a>) -\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-shell line-numbers\"><code class=\"language-shell\">etcdctl snapshot save <span class=\"token punctuation\">\\</span>\n  <span class=\"token parameter variable\">--cacert</span><span class=\"token operator\">=</span><span class=\"token punctuation\">..</span> <span class=\"token punctuation\">\\</span>\n  <span class=\"token parameter variable\">--cert</span><span class=\"token operator\">=</span><span class=\"token punctuation\">..</span> <span class=\"token punctuation\">\\</span>\n  <span class=\"token parameter variable\">--key</span><span class=\"token operator\">=</span><span class=\"token punctuation\">..</span> <span class=\"token punctuation\">\\</span>\n  <span class=\"token operator\">&lt;</span>location-of-snapshot<span class=\"token operator\">></span></code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span></span></pre></div>\nget the certificate locations using <code class=\"language-text\">kubectl get pod etcd-minikube --namespace=kube-system --output=yaml</code>. also, we need to be inside the controlplane node since we use the localhost ip</li>\n<li>to restore the data from a snapshot to a new directory (<a href=\"https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#restoring-an-etcd-cluster\">docs</a>) -\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-shell line-numbers\"><code class=\"language-shell\">etcdctl snapshot restore <span class=\"token punctuation\">\\</span>\n  --data-dir <span class=\"token operator\">&lt;</span>new-location-of-data<span class=\"token operator\">></span> <span class=\"token operator\">&lt;</span>location-of-snapshot<span class=\"token operator\">></span></code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span></span></pre></div>\n</li>\n<li>update <code class=\"language-text\">volumeMounts</code> in the static pod definition -\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-yaml line-numbers\"><code class=\"language-yaml\"><span class=\"token key atrule\">volumeMounts</span><span class=\"token punctuation\">:</span>\n  <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">mountPath</span><span class=\"token punctuation\">:</span> &lt;new<span class=\"token punctuation\">-</span>location<span class=\"token punctuation\">-</span>of<span class=\"token punctuation\">-</span>data<span class=\"token punctuation\">></span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> etcd<span class=\"token punctuation\">-</span>data</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span></span></pre></div>\n</li>\n<li>changes should get picked up and restarted automatically. however, sometimes we might have to run <code class=\"language-text\">kubectl delete pod etcd-minikube --namespace=kube-system</code> explicitly for static pods to update</li>\n<li>my understanding - the method above would work only when we have one master. also, it assumes that etcd we are using is run as a static pod, and not externally managed</li>\n<li>a file we can look at if we are inside the etcd server when it is externally managed is <code class=\"language-text\">/etc/systemd/system/etcd.service</code>. now, since it is not a static pod, we have to use commands like <code class=\"language-text\">systemctl daemon-reload</code> and <code class=\"language-text\">systemctl restart etcd.service</code></li>\n</ul>\n<h1 id=\"api-groups\" style=\"position:relative;\"><a href=\"#api-groups\" aria-label=\"api groups permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Api Groups</h1>\n<ul>\n<li>there are api groups like /api, /apis, /logs, /metrics, etc</li>\n<li>core group - grouped under /api, older standard</li>\n<li>named groups - grouped under /apis, newer standard, in which there is a logical grouping of resources</li>\n<li>e.g. to view all available endpoints, use -\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-shell line-numbers\"><code class=\"language-shell\"><span class=\"token function\">curl</span> <span class=\"token parameter variable\">-v</span> https://192.168.49.2:8443 <span class=\"token punctuation\">\\</span>\n  <span class=\"token parameter variable\">--cacert</span> /home/shameek/.minikube/ca.crt <span class=\"token punctuation\">\\</span>\n  <span class=\"token parameter variable\">--key</span> /home/shameek/.minikube/profiles/minikube/client.key <span class=\"token punctuation\">\\</span>\n  <span class=\"token parameter variable\">--cert</span> /home/shameek/.minikube/profiles/minikube/client.crt</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span></span></pre></div>\n</li>\n<li>an alternative is launching <code class=\"language-text\">kubectl proxy</code>. it starts a server on port 8001, and proxies our requests to the api server along with the certificates, so we can just run <code class=\"language-text\">curl localhost:8001</code></li>\n<li>we can also use <code class=\"language-text\">kubectl api-resources</code> to get the list of resources and their api groups. in the output of this command, the part before v1 is the api group</li>\n</ul>\n<h1 id=\"network-policy\" style=\"position:relative;\"><a href=\"#network-policy\" aria-label=\"network policy permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Network Policy</h1>\n<ul>\n<li>ingress - incoming traffic, egress - outgoing traffic</li>\n<li>the response is not considered when classifying traffic is ingress and egress (like security groups in aws)</li>\n<li>kubernetes has an all allow rule by default</li>\n<li>we use labels to link a pod to a network policy</li>\n<li>whether network policies are supported also depends on the networking solution our cluster depends on</li>\n<li>if we donâ€™t specify the <code class=\"language-text\">podSelector</code>, traffic from all pods are allowed, same for <code class=\"language-text\">namespaceSelector</code></li>\n<li>for traffic from outside the cluster, we can also use the <code class=\"language-text\">ipBlock</code> section</li>\n<li><a href=\"https://gist.github.com/shameekagarwal/4113cd73c42acab5ddfb3a882eee0391\">a yaml example</a> for a database</li>\n<li>notice how there are multiple rules. so, it works like <code class=\"language-text\">ipBlock</code> or (<code class=\"language-text\">namespaceSelector</code> and <code class=\"language-text\">podSelector</code>)</li>\n<li>if you want to apply it to <strong>all pods</strong>, use <code class=\"language-text\">podSelector: {}</code></li>\n<li>my understanding - if we want to restrict traffic between namespaces, e.g. ns-1 can only make requests to ns-2, we can use network policies. so, i created one with egress rule using key <code class=\"language-text\">kubernetes.io/metadata.name</code> under <code class=\"language-text\">namespaceSelector.matchLabels</code>. however, when i tried making requests to the cluster ip service in ns-2 from ns-1, the request was failing. i guess this was happening because kube dns is needed for the dns name resolution. so, i added <a href=\"https://stackoverflow.com/a/71127697/11885333\">this</a> policy as well to get it to work</li>\n</ul>\n<h1 id=\"interface\" style=\"position:relative;\"><a href=\"#interface\" aria-label=\"interface permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Interface</h1>\n<ul>\n<li>for implementations to work with kubernetes, they should be compatible with the interface</li>\n<li>this allows for kubernetes to be extended with multiple implementations seamlessly</li>\n<li>e.g. cri for container runtime interface, cni for network, csi for storage, etc</li>\n<li>e.g. csi lays a set of rpc calls that the cri will make. the csi implementations should implement these rpcs</li>\n</ul>\n<h1 id=\"networking\" style=\"position:relative;\"><a href=\"#networking\" aria-label=\"networking permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Networking</h1>\n<ul>\n<li>coredns - a binary which we can extract and run to turn a host into a dns server</li>\n<li>cni - container network interface defines a standard, so that different container runtimes like docker, rkt, mesos, etc. can use these standards</li>\n<li>cni also has various plugins, which the container runtimes can invoke</li>\n<li>e.g. we know that when a container runs, it should be enclosed in a network namespace. we can have a plugin called <code class=\"language-text\">bridge</code>, so that the different container runtimes can simply invoke this functionality</li>\n<li>usually, we can view the plugins using <code class=\"language-text\">ls /opt/cni/bin/</code></li>\n<li>with kubernetes, we have an internal network inside each node using network namespaces</li>\n<li>we also have ip address assigned to each node so that they can communicate with each other\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 281px; margin: 5px 0 5px 0 !important;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/e065cbbf1bda5a5ade92e645c6e2e4a9/6b1e2/networks.drawio.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 71.51898734177216%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsTAAALEwEAmpwYAAABqklEQVQ4y4WT2YoCQQxF6/8/ywdFcEFxRdvtwbXdtdVuNcO5EHHEYQKhq1PJTW6SCpPJxG63mx2PR1ssFrZarex0Okkvl4vN53PLskzKPbYkSSyOY93t93s7n8+y8Q3r9doQLnBGAd9ut/Z4PGRP01SAu91OvgTiQyEkxocvEsg6m81sOBzacrnUudfr2XQ6FYADov7f7/d1T2wURTYej3UmaYCiUyVgs9non0DOMMARQGwoYDDAH198UGgHeuE9gQaB1+tVDtAA1HsIyP1+ly9gUMZ+OBxeiVUh8nw+BcylAyOAOCDgLiSlWoCJc3kB0ovBYGDdbtc6nY75sDzzOyDJPwUbqimPRiMrFotWqVSs1WpJC4WCwKjYp/wO+KluV4U4M9lmsynQWq2maePwjfK3Cl+UaTCNhma9XhdgtVrV9P6izH5+q/IX5Xw+r8pyuZy+jUZDAD4gn/J/Enh6votUixKIjTNfGKD+LP1FsTZ+xk7ywDOi8dClj6VSSa8G9eD3VSGo3W5rcOVyWWc2g1emxfYmY2CneAUEkeRzJVzwgwWsOONLS6D8A2lVLPrNZaB2AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"networks\"\n        title=\"networks\"\n        src=\"/static/e065cbbf1bda5a5ade92e645c6e2e4a9/6b1e2/networks.drawio.png\"\n        srcset=\"/static/e065cbbf1bda5a5ade92e645c6e2e4a9/c26ae/networks.drawio.png 158w,\n/static/e065cbbf1bda5a5ade92e645c6e2e4a9/6b1e2/networks.drawio.png 281w\"\n        sizes=\"(max-width: 281px) 100vw, 281px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\nso, if we had a gateway, it would have routes like reach 10.1.1.0/24 via 192.168.1.11 and so on</li>\n<li>to view ip addresses of nodes, use <code class=\"language-text\">kubectl get nodes --output=wide</code></li>\n<li>to view interface related information, use <code class=\"language-text\">ip addr</code>. we need to ssh on the right node to view its interface related information. we can use <code class=\"language-text\">grep</code> using the ip address in the output of <code class=\"language-text\">kubectl get nodes --output=wide</code></li>\n<li>running containers / pods is the job of kubelet. so, <strong>in its configuration</strong> we define the path to cni scripts to create namespaces, assign ip addresses to the pods, etc. it is usually on the path <code class=\"language-text\">/etc/cni</code></li>\n<li>how plugins like weave work - an agent of weave runs on each node and so it knows about all the pods running on a node. this helps forward traffic from one pod to another running on different nodes easily. without weave, we would have to configure a gateway which forwards the traffic according to the ip addresses of the nodes. weave also manages assigning of ip addresses to pods</li>\n<li>to view the ip range of pods, view the logs of the networking plugin (e.g. weave) configured on the cluster. for the ip range of services, view the <a href=\"/kubernetes-part-i#kube-proxy\">api server manifest</a></li>\n</ul>\n<h1 id=\"json-path\" style=\"position:relative;\"><a href=\"#json-path\" aria-label=\"json path permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Json Path</h1>\n<ul>\n<li>using jsonpath, we get a subset of json data</li>\n<li>jsonpath always returns the results in the form of an array</li>\n<li>the root element can be represented as <code class=\"language-text\">$</code></li>\n<li>we can use <code class=\"language-text\">[]</code> for indexing arrays</li>\n<li>we can have conditions using <code class=\"language-text\">?()</code>, e.g. to get elements greater than 40 in an array, use <code class=\"language-text\">$[?(@ > 40)]</code></li>\n<li>we can use <code class=\"language-text\">in</code> and <code class=\"language-text\">nin</code> to represent in and not in, e.g. <code class=\"language-text\">$[?(@ nin [1, 2])]</code></li>\n<li>use <code class=\"language-text\">*</code> to retrieve all elements in an object (<code class=\"language-text\">$.*</code>) or in an array (<code class=\"language-text\">$[*]</code>)</li>\n<li>a convoluted e.g. - we want to extract the laureate whose first and last name we know -\n<div class=\"gatsby-highlight\" data-language=\"json\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-json line-numbers\"><code class=\"language-json\"><span class=\"token punctuation\">{</span>\n  <span class=\"token property\">\"prizes\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span>\n    <span class=\"token punctuation\">{</span>\n      <span class=\"token property\">\"category\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"physics\"</span><span class=\"token punctuation\">,</span>\n      <span class=\"token property\">\"laureates\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span>\n        <span class=\"token punctuation\">{</span> <span class=\"token property\">\"firstname\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Arthur\"</span><span class=\"token punctuation\">,</span> <span class=\"token property\">\"surname\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Ashkin\"</span> <span class=\"token punctuation\">}</span>\n      <span class=\"token punctuation\">]</span>\n    <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">{</span>\n      <span class=\"token property\">\"category\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"chemistry\"</span><span class=\"token punctuation\">,</span>\n      <span class=\"token property\">\"laureates\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span>\n        <span class=\"token punctuation\">{</span> <span class=\"token property\">\"firstname\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Frances H.\"</span><span class=\"token punctuation\">,</span> <span class=\"token property\">\"surname\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Arnold\"</span> <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n        <span class=\"token punctuation\">{</span> <span class=\"token property\">\"firstname\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"George P.\"</span><span class=\"token punctuation\">,</span> <span class=\"token property\">\"surname\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Smith\"</span> <span class=\"token punctuation\">}</span>\n      <span class=\"token punctuation\">]</span>\n    <span class=\"token punctuation\">}</span>\n  <span class=\"token punctuation\">]</span>\n<span class=\"token punctuation\">}</span></code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n<code class=\"language-text\">$.prizes[*].laureates[?(@.firstname == 'Malala' &amp;&amp; @.surname == 'Yousafzai')]</code></li>\n<li>to retrieve multiple elements in a list, we can use <code class=\"language-text\">$[0,3]</code></li>\n<li>we can also use the syntax <code class=\"language-text\">$[start:end:step]</code> e.g. <code class=\"language-text\">$[1:5:2]</code></li>\n<li>we can use negative indices to retrieve the last elements, e.g. <code class=\"language-text\">$[-3:]</code> for the last three elements</li>\n<li>element present at index end is not included</li>\n<li>step defaults to 1, start and end default to 0</li>\n<li>while <code class=\"language-text\">$[positive]</code> works, <code class=\"language-text\">$[negative]</code> fails in some cases, so specify an end when using negative indices</li>\n</ul>\n<h3 id=\"usage-in-kubectl\" style=\"position:relative;\"><a href=\"#usage-in-kubectl\" aria-label=\"usage in kubectl permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Usage in Kubectl</h3>\n<ul>\n<li>we mention the json path using <code class=\"language-text\">--output=jsonpath=\"{.items}\"</code> i.e. enclose within <code class=\"language-text\">{}</code> and remove <code class=\"language-text\">$</code></li>\n<li>we can get merge multiple queries - <code class=\"language-text\">jsonpath=\"{.name}{.cpu}\"</code></li>\n<li>we can print the results in a tabular format. e.g. to get the node name and its cpu usage in a table, use - <code class=\"language-text\">kubectl get nodes --output=custom-columns=\"NODE:.metadata.name,CPU:.status.capacity.cpu</code></li>\n<li>we can also specify a sort order using <code class=\"language-text\">--sort-by</code>, e.g. - <code class=\"language-text\">kubectl get pods --all-namespaces --output=custom-columns=\"NAME:.metadata.name\" --sort-by=\".metadata.name\"</code></li>\n<li>to reverse the order of sort by, <code class=\"language-text\">k get ... --sort-by=... | tac</code></li>\n</ul>\n<h1 id=\"miscellaneous-debugging-tips\" style=\"position:relative;\"><a href=\"#miscellaneous-debugging-tips\" aria-label=\"miscellaneous debugging tips permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Miscellaneous Debugging Tips</h1>\n<ul>\n<li>e.g. api server goes down, which makes using kubectl pointless. in this case, we can view logs in /var/log/pods if we cannot somehow use the container runtime</li>\n<li>sometimes, logs can be seen in /var/log/syslog as well</li>\n<li>we can also check /var/log/containers</li>\n<li>cri-o - <code class=\"language-text\">crictl ps</code>, <code class=\"language-text\">crictl logs &lt;container-id></code>, <code class=\"language-text\">crictl inspect &lt;container-id></code></li>\n<li><code class=\"language-text\">ps aux | cat</code> can be useful to view commands used to run for e.g. kubelet, since it is not run as a static pod</li>\n<li><code class=\"language-text\">find /etc | grep kubelet</code> to find path of kubelet, <code class=\"language-text\">find /var | grep log</code> to find path of logs, etc</li>\n<li><code class=\"language-text\">journalctl -u kubelet.service</code> to get proper logs related to kubelet. we can also use it for other pods, e.g. <code class=\"language-text\">journalctl | grep apiserver</code></li>\n<li>kubelet configuration for my minikube cluster is at <code class=\"language-text\">/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code>. it will point to the kubelet executable, e.g. at <code class=\"language-text\">/usr/bin/kubelet</code> for kubeadm</li>\n</ul>","frontmatter":{"title":"Kubernetes - Part III"}}},"pageContext":{"id":"de14570f-d795-55ed-b17b-2ef3cf90fb53"}},"staticQueryHashes":["1037383464","1617985380"]}