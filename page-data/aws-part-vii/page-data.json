{"componentChunkName":"component---src-templates-blog-js","path":"/aws-part-vii/","result":{"data":{"markdownRemark":{"fields":{"slug":"/aws-part-vii/"},"id":"e6588362-854c-5bd2-9baf-1462ba79a751","html":"<h1 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Introduction</h1>\n<ul>\n<li>cap theorem - consistency (all clients see the same data at the same time), availability (clients always get back a response, even if one or more nodes are down), and partition tolerance (even if nodes in the system breakdown, the system should continue to work). only two of the three can be guaranteed at a time</li>\n<li>acid - focuses on consistency of cap\n<ul>\n<li>atomic - all operations of a transaction go through or none of them go through</li>\n<li>consistency - move the database from one valid state to another, no intermediate states</li>\n<li>isolated - transactions do not interfere with each other and execute independently</li>\n<li>durable - once committed, transactions remain stored across outages and restarts</li>\n</ul>\n</li>\n<li>base - focuses on availability of cap\n<ul>\n<li>basically available - read and writes can or cannot be consistent, no guarantees</li>\n<li>soft state - consistency can be an opt-in feature in some dbs, and handled by the application</li>\n<li>eventually consistent - reads do get consistent after some time has passed, after the replication is over</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"rds\" style=\"position:relative;\"><a href=\"#rds\" aria-label=\"rds permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>RDS</h1>\n<ul>\n<li>rds is a database server as a service</li>\n<li>we can choose from mysql, mariadb, postgresql, oracle and microsoft sql server</li>\n<li>remember there are licensing concerns to be handled if using oracle or microsoft sql server</li>\n<li>we might want to run databases ourselves on ec2 instead of using rds in cases where we want to run something not supported by aws, some os level access that rds does not give, etc. disadvantages are management overheads around backups, dr, scalability, etc</li>\n<li>so, because of the above problem, one solution by aws is <u>rds custom</u>. currently only ms sql and oracle is supported. it allows us to ssh / rdp into the instance. with rds custom, we will gain visibility into the underlying ec2, ebs, s3 snapshots, etc</li>\n<li>rds runs inside a vpc, and not a public service like s3 or dynamodb</li>\n<li><u>rds subnet group</u> - a list of subnets that rds uses</li>\n<li>rds will pick different subnets in different azs for primary and standby</li>\n<li>an rds instance can have multiple databases</li>\n<li>each rds instance has its own dedicated ebs (note how this is different from amazon aurora)</li>\n<li>an <u>enhanced monitoring</u> option is present in rds for greater visibility into metrics</li>\n<li><u>storage autoscaling</u> - automatically scales the capacity up. we just have to specify the maximum</li>\n</ul>\n<h1 id=\"multi-az-instances\" style=\"position:relative;\"><a href=\"#multi-az-instances\" aria-label=\"multi az instances permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Multi AZ Instances</h1>\n<ul>\n<li>primary rds instance, with a standby to which data is replicated synchronously</li>\n<li>this replication happens at the storage level, which is less efficient</li>\n<li>the dns points to the primary instance. we always access rds via this dns, and so all our operations go the primary instance unless there is a failover to the standby</li>\n<li>backups use standby instances so that performance of our application is not impacted</li>\n<li>basically a cname record points to the primary and then starts pointing to the standby in case of a failover. so during failures, we would be briefly directed to the primary instance due to ttl</li>\n<li><u>different azs</u> in the same region are used</li>\n</ul>\n<h1 id=\"multi-az-cluster-instances\" style=\"position:relative;\"><a href=\"#multi-az-cluster-instances\" aria-label=\"multi az cluster instances permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Multi AZ Cluster Instances</h1>\n<ul>\n<li>one writer can replicate (synchronously as well) to two readers in <u>different azs</u></li>\n<li>we can only have two different readers unlike aurora which can have more</li>\n<li>reader instances can be used for read operations unlike in multi az instance mode where the standby cannot be used by us in general unless there is a failover</li>\n<li>this helps us scale our reads</li>\n<li>this means our application needs to understand when to use reader vs cluster (writer) endpoint</li>\n<li>the fact that this architecture uses ebs which is local to an instance makes it different from aurora</li>\n<li>writes are first written to local storage which is then flushed to ebs thus optimizing performance</li>\n<li>replication happens via transaction logs thus making it faster</li>\n<li>even failover can take advantage of these transaction logs</li>\n<li>writes are committed once one of the replicas acknowledge the write</li>\n<li>my understanding - so, in multi az instance, one standby is used and that cannot function as read replicas. in multi az cluster instances, two standbys are used and they can function as read replicas</li>\n</ul>\n<h1 id=\"backups\" style=\"position:relative;\"><a href=\"#backups\" aria-label=\"backups permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Backups</h1>\n<ul>\n<li>two types - automated backups and snapshots</li>\n<li>use s3 bts but hidden away from us</li>\n<li>this makes the backups regionally resilient since s3 is used</li>\n<li><u>backups cause an io pause on the instance it is done from</u></li>\n<li>backups are taken from the standby instances if available thus not impacting our application much</li>\n<li>snapshots are incremental</li>\n<li>automated backups - happen every day. however, every 5 minutes, transaction logs (the actual ddl dml statements that lead to a state) are also stored in s3</li>\n<li>this means while the rpo is 5 minutes, rto can be more since we need all the transaction logs to be replayed since the closest snapshot bts (recall in automated backup, snapshots are taken once a day)</li>\n<li>retention of snapshots in automated backups can be 0 (disabled) to 35 days</li>\n<li>we can explicitly enable replicating snapshots across regions</li>\n<li>remember when restoring, db endpoint changes because essentially new server instance(s) are created - so config of application needs to be updated. this behavior is also different from aurora</li>\n</ul>\n<h1 id=\"read-replicas\" style=\"position:relative;\"><a href=\"#read-replicas\" aria-label=\"read replicas permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Read Replicas</h1>\n<ul>\n<li>read only replicas</li>\n<li>there is asynchronous replication (remember multi az is synchronous replication)</li>\n<li>can be across regions (recall this does not happen in multi az)</li>\n<li>they have a different endpoint altogether from the main instance</li>\n<li>we can have upto 5 read replicas</li>\n<li>we can have read replicas of read replicas, but then eventual consistency issues start becoming noticeable</li>\n<li>read replicas can be promoted, and this can have much lower rto than backups - since for e.g. remaining transactions need not be replayed in case of automated backups?</li>\n<li>use pitr (point in time recovery) feature of automated backups if there is data corruption, since read replicas will have the corrupted data as well due to replication, albeit asynchronous</li>\n<li>so, use read replicas for dr (disaster recovery)</li>\n</ul>\n<h1 id=\"rds-security\" style=\"position:relative;\"><a href=\"#rds-security\" aria-label=\"rds security permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>RDS Security</h1>\n<ul>\n<li>ssl / tls for encryption in transit, and can be made mandatory</li>\n<li>encryption at rest - we can use kms. remember in this case, the encryption is done by the ec2 host, and the database engine is unaware of the encryption</li>\n<li>snapshots are also encrypted using the same kms keys</li>\n<li>oracle and ms sql server support tde (transparent data encryption). so, the encryption is handled by the engine itself. oracle supports tde using keys stored in aws cloud hsm which has even better security</li>\n<li>iam authentication - normally, logins to database are handled via users managed by the database itself. so, to use iam, the idea is we create a user inside the database configured to allow an authentication token to be used. iam users / roles have a policy that maps the identity to a database user. this allows them to make calls to the generate-db-auth-token api to generate an authentication token. this can then be used as the password for the user the policy mapped to. this has a validity of 15 mins. my understanding - so iam is only being used for authentication. for authorization, we need to grant appropriate permissions to the db user for which we generate this authentication token</li>\n</ul>\n<h1 id=\"aurora-architecture\" style=\"position:relative;\"><a href=\"#aurora-architecture\" aria-label=\"aurora architecture permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Aurora Architecture</h1>\n<ul>\n<li>part of rds but is very different from the normal rds discussed above</li>\n<li>it has a master with replicas, and replicas can be used for <u>both read and failover</u></li>\n<li>by default, primary for write and primary / secondary for reads</li>\n<li>there is no local storage like in normal rds which uses ebs - all the computes have access to a shared storage</li>\n<li>the maximum size is <u>128 tib</u> and there are <u>6</u> storage nodes</li>\n<li>data is replicated <u>synchronously</u> across these 6 storage nodes which are spread <u>across az</u> - so, it is much more resilient by design</li>\n<li>the replication happens at the storage level</li>\n<li>upto 15 replicas are allowed unlike max 2 replicas max (multi az cluster mode) for standby</li>\n<li>we do not specify storage beforehand - we are billed for what we use. we are billed for high watermark i.e. if we consume 50 gb, we are billed for 50gb but if we down scale to 40gb of data, we are still billed for 50gb. so, if we have a use case of reducing our data by a lot, we have to replicate data into a new cluster to reduce cost</li>\n<li>note - apparently, this high watermark was old, and aws have moved to newer solution where we are not billed for this high watermark but just what we are consuming at that point of time</li>\n<li>replicas can be easily added or removed since storage is detached from compute</li>\n<li>cluster endpoint - <u>directs to current primary instance for read and writes</u></li>\n<li>reader endpoint - load balance across reader replicas for scaling read easily. as we add or remove replicas, the reader endpoint is automatically updated for us bts</li>\n<li>instance endpoint - to connect to specific db instances (each instance has its own endpoint as well)</li>\n<li>custom endpoint - we can choose db instances to add to an endpoint we create manually. e.g. we can direct our internal users to an endpoint which load balances between lower capacity instances to handle ad hoc reporting and direct production endpoint to an endpoint which load balances between performant instances</li>\n<li>inplace backtracking is supported with aurora i.e. instead of us doing a snapshot and spinning up a new instance, we can do it on the same cluster. this means our application does not have to be updated again</li>\n<li>fast clone - a new database can be created from the original database without copying over the entire existing data, and just appending new data. this is called copy on write</li>\n<li>parallel query - push down query processing to storage layer, which helps reduce network transfers. this way the same database instance can be used for both transactional and analytical use cases, where the parallel query feature is used for analytics and the instance for transactional workloads</li>\n</ul>\n<h1 id=\"aurora-serverless\" style=\"position:relative;\"><a href=\"#aurora-serverless\" aria-label=\"aurora serverless permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Aurora Serverless</h1>\n<ul>\n<li>we do not have to provision db instances like before</li>\n<li>it uses acu - aurora capacity units</li>\n<li>we can set a minimum and maximum acu for an aurora serverless cluster, and aurora scales between them automatically for us based on the load it receives</li>\n<li>so, now instead of provisioned instances where we pay for all the instances, we only pay for the acu we consume here (which can even be 0 when not in use)</li>\n<li>aws has a pool of acus shared between customers, which has access to the underlying shared storage</li>\n<li>the underlying storage has the same resilience of 6 copies across different azs</li>\n<li>since the acus are very dynamic, underneath there is a proxy fleet which brokers our db connection to the acus. the proxy fleet is basically a collection of instances</li>\n<li>use case - for newer applications to understand the load, for unpredictable workloads. also great for test and development processes, since they are not billed when not in use</li>\n<li>aws prep question - cannot change provisioned to serverless directly, use aws dms</li>\n</ul>\n<h1 id=\"aurora-global-database\" style=\"position:relative;\"><a href=\"#aurora-global-database\" aria-label=\"aurora global database permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Aurora Global Database</h1>\n<ul>\n<li>upto 5 secondary regions</li>\n<li>the cluster in the primary region continues to have the same architecture (upto 15 read replicas)</li>\n<li>the secondary regions can have 16 read replicas each</li>\n<li>these secondary regions can be promoted in case of region wide failure</li>\n<li>around ~1s latency for replication to happen</li>\n<li>again since replication happens at the decoupled storage level, db performance is not impacted</li>\n</ul>\n<h1 id=\"aurora-multi-master\" style=\"position:relative;\"><a href=\"#aurora-multi-master\" aria-label=\"aurora multi master permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Aurora Multi Master</h1>\n<ul>\n<li>default is single master i.e. one instance that can perform read and write (used via the cluster endpoint), while other replicas can perform reads (used via the reader endpoint)</li>\n<li><u>there are no load balanced endpoints in multi master</u> - we directly connect to the instances</li>\n<li>there is no failover needed here - when using single master, the cluster endpoint needs to fail over to and promote one of the replicas which can take time, unlike multi master where no failover is needed</li>\n</ul>\n<h1 id=\"rds-proxy\" style=\"position:relative;\"><a href=\"#rds-proxy\" aria-label=\"rds proxy permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>RDS Proxy</h1>\n<ul>\n<li>opening and closing connections to database has a decent overhead</li>\n<li>this can be bad for e.g. for serverless which has to do this on every invocation</li>\n<li>applications connect to a database proxy, which maintains a pool of connections</li>\n<li>allows for multiplexing i.e. number of connections between db and proxy is smaller when compared to the number between application and proxy</li>\n<li>only accessible from inside a vpc</li>\n<li>in the case of a failover, the application does not see the failure, it just stays connected to the proxy. the proxy itself changes its connection to the failed over secondary from the primary</li>\n</ul>\n<h1 id=\"invoking-lambda-from-rds\" style=\"position:relative;\"><a href=\"#invoking-lambda-from-rds\" aria-label=\"invoking lambda from rds permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Invoking Lambda from RDS</h1>\n<ul>\n<li>rds events - capture db level events like snapshot events, security group events, etc</li>\n<li>however, rds events do not work for insert, update delete i.e. data modifying operations</li>\n<li>apparently, postgres / mysql can invoke lambda functions from their native stored procedures</li>\n</ul>\n<h1 id=\"dms\" style=\"position:relative;\"><a href=\"#dms\" aria-label=\"dms permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>DMS</h1>\n<ul>\n<li>database migration service</li>\n<li>one of the source or target (or both) need to be aws</li>\n<li>there are ec2s (called replication instances) which run replication tasks</li>\n<li>we need to specify the source and destination endpoints</li>\n<li><u>we can use full load only, cdc only, or full load + cdc</u></li>\n<li>so basically, any changes to the source are replicated. this helps achieve no downtime. so, for e.g. we need to change from a small sized aurora db instance to aurora serverless, we can use this</li>\n<li>so, whenever we want ongoing replication (or onetime), think dms</li>\n<li><u>sct - schema conversion toolset</u> - another aws tool of help migrate schemas from one db engine to another</li>\n<li>for very large data migrations, we can use snowball with dms and sct - idea is sct is used to extract and put data into snowball. snowball then puts data into s3 and finally, dms migrates data from s3 to for e.g. rds</li>\n</ul>","frontmatter":{"title":"AWS - Part VII"}}},"pageContext":{"id":"e6588362-854c-5bd2-9baf-1462ba79a751"}},"staticQueryHashes":["1037383464","1617985380"]}